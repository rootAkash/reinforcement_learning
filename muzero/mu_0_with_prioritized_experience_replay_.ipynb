{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mu_0 with prioritized experience replay .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyObJXkDKd1MKlGElo3UAqlG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rootAkash/reinforcement_learning/blob/master/muzero/mu_0_with_prioritized_experience_replay_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIEXeLFudTXo"
      },
      "source": [
        "#why priority?\r\n",
        "#since the mcts root value is a better estimate of value of a state. The difference between the n step return value and the mcts value tells somehow how much the value fuction has \r\n",
        "#coverged to the actual value (wrt to the mcts policy that we are following while generating trajectory).\r\n",
        "#policy network just apporximates the mcts to boot strap the value function.\r\n",
        "#so the difference between search value(mcts root) and the n step return value should be higher for sates which we have not trained properly or seen yet so we need to sample more of those\r\n",
        "#in order to get a better value function and lead it to quicker convergence in enviroments with intermediate rewards (eg ATARI)\r\n",
        "#so we sample from replay using the difference between root search value and  step value return as priority,it introduces a sampling bias. \r\n",
        "#sample bias: sampling bias is a bias in which a sample is collected in such a way that some members of the intended population have a lower or higher sampling probability than others.\r\n",
        "#intituively the samples having higher probablity will be sampled more leading to the model getting baised into optimising for these samples more.Equivalent of having bigger loss for higher\r\n",
        "#probablity samples since they are sampled more.\r\n",
        "#to correct for this we use importance sampling (depends on probality value of sample) and scale down the loss for sample using it.  "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwI18Z6p1oGP",
        "outputId": "456e385e-28d2-49d7-ec8d-9dff07fd2074"
      },
      "source": [
        "!pip install gym[all]\r\n",
        "!pip install box2d-py\r\n",
        "!apt-get install python-opengl -y\r\n",
        "!apt install xvfb -y"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym[all] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[all]) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (1.19.5)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"all\" in /usr/local/lib/python3.7/dist-packages (from gym[all]) (0.2.6)\n",
            "Collecting box2d-py~=2.3.5; extra == \"all\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/34/da5393985c3ff9a76351df6127c275dcb5749ae0abbe8d5210f06d97405d/box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 19.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python; extra == \"all\" in /usr/local/lib/python3.7/dist-packages (from gym[all]) (4.1.2.30)\n",
            "Requirement already satisfied: Pillow; extra == \"all\" in /usr/local/lib/python3.7/dist-packages (from gym[all]) (7.1.2)\n",
            "Collecting mujoco-py<2.0,>=1.50; extra == \"all\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/8c/64e0630b3d450244feef0688d90eab2448631e40ba6bdbd90a70b84898e7/mujoco-py-1.50.1.68.tar.gz (120kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 58.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: imageio; extra == \"all\" in /usr/local/lib/python3.7/dist-packages (from gym[all]) (2.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[all]) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0; extra == \"all\"->gym[all]) (1.15.0)\n",
            "Collecting glfw>=1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/d7/79c091c877493de7f8286ed62c77bf0f2c51105656073846b2326021b524/glfw-2.1.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (205kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 49.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.7/dist-packages (from mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (0.29.22)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.7/dist-packages (from mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (1.14.5)\n",
            "Collecting lockfile>=0.12.2\n",
            "  Downloading https://files.pythonhosted.org/packages/c8/22/9460e311f340cb62d26a38c419b1381b8593b0bb6b5d1f056938b086d362/lockfile-0.12.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.10->mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (2.20)\n",
            "Building wheels for collected packages: mujoco-py\n",
            "  Building wheel for mujoco-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for mujoco-py\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for mujoco-py\n",
            "Failed to build mujoco-py\n",
            "Installing collected packages: box2d-py, glfw, lockfile, mujoco-py\n",
            "    Running setup.py install for mujoco-py ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-jq7smy2z/mujoco-py/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-jq7smy2z/mujoco-py/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-oix5q0ha/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n",
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.7/dist-packages (2.3.8)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  libgle3\n",
            "The following NEW packages will be installed:\n",
            "  python-opengl\n",
            "0 upgraded, 1 newly installed, 0 to remove and 30 not upgraded.\n",
            "Need to get 496 kB of archives.\n",
            "After this operation, 5,416 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Fetched 496 kB in 1s (844 kB/s)\n",
            "Selecting previously unselected package python-opengl.\n",
            "(Reading database ... 160980 files and directories currently installed.)\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 30 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,270 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8 [784 kB]\n",
            "Fetched 784 kB in 1s (1,269 kB/s)\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 163335 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.8_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQRen3PlNkiB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0a5a2a7-67bb-44a0-e12f-0f4bf7aff065"
      },
      "source": [
        "import numpy as np\r\n",
        "def stcat(x,support=5):\r\n",
        "  x = np.sign(x) * ((abs(x) + 1)**0.5 - 1) + 0.001 * x\r\n",
        "  x = np.clip(x, -support, support)\r\n",
        "  floor = np.floor(x)\r\n",
        "  prob = x - floor\r\n",
        "  logits = np.zeros( 2 * support + 1)\r\n",
        "  first_index = int(floor + support)\r\n",
        "  second_index = int(floor + support+1)\r\n",
        "  logits[first_index] = 1-prob\r\n",
        "  if prob>0:\r\n",
        "    logits[second_index] = prob\r\n",
        "  return logits\r\n",
        "def catts(x,support=5):\r\n",
        "  support = np.arange(-support, support+1, 1)\r\n",
        "  x = np.sum(support*x)\r\n",
        "  x = np.sign(x) * ((((1 + 4 * 0.001 * (abs(x) + 1 + 0.001))**0.5 - 1) / (2 * 0.001))** 2- 1)\r\n",
        "  return x  \r\n",
        "\r\n",
        "#cat = stcat(58705)\r\n",
        "#print(cat)\r\n",
        "#scalar = catts(cat)\r\n",
        "#print(scalar)\r\n",
        "print(\"done\")        \r\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhf3dyDa2GYq",
        "outputId": "3fa74af5-79eb-4c50-8e9a-e95f176cd307"
      },
      "source": [
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class MuZeroNet(nn.Module):\r\n",
        "    def __init__(self, input_size, action_space_n, reward_support_size, value_support_size):\r\n",
        "        super().__init__()\r\n",
        "        self.hx_size = 32\r\n",
        "        self._representation = nn.Sequential(nn.Linear(input_size, self.hx_size),\r\n",
        "                                             nn.Tanh())\r\n",
        "        self._dynamics_state = nn.Sequential(nn.Linear(self.hx_size + action_space_n, 64),\r\n",
        "                                             nn.Tanh(),\r\n",
        "                                             nn.Linear(64, self.hx_size),\r\n",
        "                                             nn.Tanh())\r\n",
        "        self._dynamics_reward = nn.Sequential(nn.Linear(self.hx_size + action_space_n, 64),\r\n",
        "                                              nn.LeakyReLU(),\r\n",
        "                                              nn.Linear(64, 2*reward_support_size+1))\r\n",
        "        self._prediction_actor = nn.Sequential(nn.Linear(self.hx_size, 64),\r\n",
        "                                               nn.LeakyReLU(),\r\n",
        "                                               nn.Linear(64, action_space_n))\r\n",
        "        self._prediction_value = nn.Sequential(nn.Linear(self.hx_size, 64),\r\n",
        "                                               nn.LeakyReLU(),\r\n",
        "                                               nn.Linear(64, 2*value_support_size+1))\r\n",
        "        self.action_space_n = action_space_n\r\n",
        "\r\n",
        "        self._prediction_value[-1].weight.data.fill_(0)\r\n",
        "        self._prediction_value[-1].bias.data.fill_(0)\r\n",
        "        self._dynamics_reward[-1].weight.data.fill_(0)\r\n",
        "        self._dynamics_reward[-1].bias.data.fill_(0)\r\n",
        "\r\n",
        "    def p(self, state):\r\n",
        "        actor = torch.softmax(self._prediction_actor(state),dim=1)\r\n",
        "        value = torch.softmax(self._prediction_value(state),dim=1)\r\n",
        "        return actor, value\r\n",
        "\r\n",
        "    def h(self, obs_history):\r\n",
        "        return self._representation(obs_history)\r\n",
        "\r\n",
        "    def g(self, state, action):\r\n",
        "        x = torch.cat((state, action), dim=1)\r\n",
        "        next_state = self._dynamics_state(x)\r\n",
        "        reward = torch.softmax(self._dynamics_reward(x),dim=1)\r\n",
        "        return next_state, reward     \r\n",
        "\r\n",
        "    def initial_state(self, x):\r\n",
        "        hout = self.h(x)\r\n",
        "        prob,v= self.p(hout)\r\n",
        "        return hout,prob,v\r\n",
        "    def next_state(self,hin,a):\r\n",
        "        hout,r = self.g(hin,a)\r\n",
        "        prob,v= self.p(hout)\r\n",
        "        return hout,r,prob,v\r\n",
        "    def inference_initial_state(self, x):\r\n",
        "        with torch.no_grad():\r\n",
        "          hout = self.h(x)\r\n",
        "          prob,v=self.p(hout)\r\n",
        "\r\n",
        "          return hout,prob,v\r\n",
        "    def inference_next_state(self,hin,a):\r\n",
        "        with torch.no_grad():\r\n",
        "          hout,r = self.g(hin,a)\r\n",
        "          prob,v=self.p(hout)\r\n",
        "          return hout,r,prob,v     \r\n",
        "\r\n",
        "\r\n",
        "print(\"done\")                                      "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkKMTKa6wYtR"
      },
      "source": [
        "\r\n",
        "#MTCS    MUzero modified for intermeditate rewards settings and using predicted rewards\r\n",
        "#accepts policy as a list\r\n",
        "import torch\r\n",
        "import math\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import random\r\n",
        "def dynamics(net,state,action):\r\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
        "\r\n",
        "    #print(state,action) \r\n",
        "    next_state,reward,prob,value = net.inference_next_state(state.to(device),torch.tensor([action]).float().to(device))\r\n",
        "    reward = catts(reward.cpu().numpy().ravel())\r\n",
        "    value = catts(value.cpu().numpy().ravel())\r\n",
        "    prob = prob.cpu().tolist()[0]\r\n",
        "    #print(\"dynamics\",prob)\r\n",
        "    return next_state.cpu(),reward,prob,value\r\n",
        "\r\n",
        "\r\n",
        "class MinMaxStats:\r\n",
        "    \"\"\"A class that holds the min-max values of the tree.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        self.MAXIMUM_FLOAT_VALUE = float('inf')       \r\n",
        "        self.maximum =  -self.MAXIMUM_FLOAT_VALUE\r\n",
        "        self.minimum =  self.MAXIMUM_FLOAT_VALUE\r\n",
        "\r\n",
        "    def update(self, value: float):\r\n",
        "        if value is None:\r\n",
        "            raise ValueError\r\n",
        "\r\n",
        "        self.maximum = max(self.maximum, value)\r\n",
        "        self.minimum = min(self.minimum, value)\r\n",
        "\r\n",
        "    def normalize(self, value: float) -> float:\r\n",
        "        # If the value is unknow, by default we set it to the minimum possible value\r\n",
        "        if value is None:\r\n",
        "            return 0.0\r\n",
        "\r\n",
        "        if self.maximum > self.minimum:\r\n",
        "            # We normalize only when we have set the maximum and minimum values.\r\n",
        "            return (value - self.minimum) / (self.maximum - self.minimum)\r\n",
        "        return value\r\n",
        "\r\n",
        "\r\n",
        "class Node:\r\n",
        "    \"\"\"A class that represent nodes inside the MCTS tree\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, prior: float):\r\n",
        "        self.visit_count = 0\r\n",
        "        self.to_play = -1\r\n",
        "        self.prior = prior\r\n",
        "        self.value_sum = 0\r\n",
        "        self.children = {}\r\n",
        "        self.hidden_state = None\r\n",
        "        self.reward = 0\r\n",
        "\r\n",
        "    def expanded(self):\r\n",
        "        return len(self.children) > 0\r\n",
        "\r\n",
        "    def value(self):\r\n",
        "        if self.visit_count == 0:\r\n",
        "            return None\r\n",
        "        return self.value_sum / self.visit_count\r\n",
        "\r\n",
        "\r\n",
        "def softmax_sample(visit_counts, actions, t):\r\n",
        "    counts_exp = np.exp(visit_counts) * (1 / t)\r\n",
        "    probs = counts_exp / np.sum(counts_exp, axis=0)\r\n",
        "    action_idx = np.random.choice(len(actions), p=probs)\r\n",
        "    return actions[action_idx]\r\n",
        "\r\n",
        "\r\n",
        "\"\"\"MCTS module: where MuZero thinks inside the tree.\"\"\"\r\n",
        "\r\n",
        "\r\n",
        "def add_exploration_noise( node):\r\n",
        "    \"\"\"\r\n",
        "    At the start of each search, we add dirichlet noise to the prior of the root\r\n",
        "    to encourage the search to explore new actions.\r\n",
        "    \"\"\"\r\n",
        "    actions = list(node.children.keys())\r\n",
        "    noise = np.random.dirichlet([0.25] * len(actions)) # config.root_dirichlet_alpha\r\n",
        "    frac = 0.25#config.root_exploration_fraction\r\n",
        "    for a, n in zip(actions, noise):\r\n",
        "        node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def ucb_score(parent, child,min_max_stats):\r\n",
        "    \"\"\"\r\n",
        "    The score for a node is based on its value, plus an exploration bonus based on\r\n",
        "    the prior.\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "    pb_c_base = 19652\r\n",
        "    pb_c_init = 1.25\r\n",
        "    pb_c = math.log((parent.visit_count + pb_c_base + 1) / pb_c_base) + pb_c_init\r\n",
        "    pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\r\n",
        "\r\n",
        "    prior_score = pb_c * child.prior\r\n",
        "    value_score = min_max_stats.normalize(child.value())\r\n",
        "    return  value_score + prior_score \r\n",
        "\r\n",
        "def select_child(node, min_max_stats):\r\n",
        "    \"\"\"\r\n",
        "    Select the child with the highest UCB score.\r\n",
        "    \"\"\"\r\n",
        "    # When the parent visit count is zero, all ucb scores are zeros, therefore we return a random child\r\n",
        "    if node.visit_count == 0:\r\n",
        "        return random.sample(node.children.items(), 1)[0]\r\n",
        "\r\n",
        "    _, action, child = max(\r\n",
        "        (ucb_score(node, child, min_max_stats), action,\r\n",
        "         child) for action, child in node.children.items())\r\n",
        "    return action, child\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def expand_node(node, to_play, actions_space,hidden_state,reward,policy):\r\n",
        "    \"\"\"\r\n",
        "    We expand a node using the value, reward and policy prediction obtained from\r\n",
        "    the neural networks.\r\n",
        "    \"\"\"\r\n",
        "    node.to_play = to_play\r\n",
        "    node.hidden_state = hidden_state\r\n",
        "    node.reward = reward\r\n",
        "    policy = {a:policy[a] for a in actions_space}\r\n",
        "    policy_sum = sum(policy.values())\r\n",
        "    for action, p in policy.items():\r\n",
        "        node.children[action] = Node(p / policy_sum) # not needed since mine are already softmax but its fine \r\n",
        "\r\n",
        "\r\n",
        "def backpropagate(search_path, value,to_play,discount, min_max_stats):\r\n",
        "    \"\"\"\r\n",
        "    At the end of a simulation, we propagate the evaluation all the way up the\r\n",
        "    tree to the root.\r\n",
        "    \"\"\"\r\n",
        "    for node in search_path[::-1]: #[::-1] means reversed\r\n",
        "        node.value_sum += value \r\n",
        "        node.visit_count += 1\r\n",
        "        min_max_stats.update(node.value())\r\n",
        "\r\n",
        "        value = node.reward + discount * value\r\n",
        "\r\n",
        "\r\n",
        "def select_action(node, mode ='softmax'):\r\n",
        "    \"\"\"\r\n",
        "    After running simulations inside in MCTS, we select an action based on the root's children visit counts.\r\n",
        "    During training we use a softmax sample for exploration.\r\n",
        "    During evaluation we select the most visited child.\r\n",
        "    \"\"\"\r\n",
        "    visit_counts = [child.visit_count for child in node.children.values()]\r\n",
        "    actions = [action for action in node.children.keys()]\r\n",
        "    action = None\r\n",
        "    if mode == 'softmax':\r\n",
        "        t = 1.0\r\n",
        "        action = softmax_sample(visit_counts, actions, t)\r\n",
        "    elif mode == 'max':\r\n",
        "        action, _ = max(node.children.items(), key=lambda item: item[1].visit_count)\r\n",
        "    counts_exp = np.exp(visit_counts)\r\n",
        "    probs = counts_exp / np.sum(counts_exp, axis=0)    \r\n",
        "    #return action ,probs,node.value()\r\n",
        "    return action ,np.array(visit_counts)/sum(visit_counts),node.value()\r\n",
        "\r\n",
        "def run_mcts(net, state,prob,root_value,num_simulations,discount = 0.9):\r\n",
        "    \"\"\"\r\n",
        "    Core Monte Carlo Tree Search algorithm.\r\n",
        "    To decide on an action, we run N simulations, always starting at the root of\r\n",
        "    the search tree and traversing the tree according to the UCB formula until we\r\n",
        "    reach a leaf node.\r\n",
        "    \"\"\"\r\n",
        "    prob, root_value = prob.tolist()[0] ,catts(root_value.numpy().ravel())\r\n",
        "    to_play = True\r\n",
        "    action_space=[ i for i in range(len(prob))]#history.action_space()\r\n",
        "    #print(\"action space\",action_space)\r\n",
        "    root = Node(0)\r\n",
        "    expand_node(root, to_play,action_space,state,0.0,prob)#node, to_play, actions_space ,hidden_state,reward,policy\r\n",
        "    add_exploration_noise( root)\r\n",
        "\r\n",
        "\r\n",
        "    min_max_stats = MinMaxStats()\r\n",
        "\r\n",
        "    for _ in range(num_simulations): \r\n",
        "        node = root\r\n",
        "        search_path = [node]\r\n",
        "\r\n",
        "        while node.expanded():\r\n",
        "            action, node = select_child( node, min_max_stats)\r\n",
        "            search_path.append(node)\r\n",
        "\r\n",
        "        # Inside the search tree we use the dynamics function to obtain the next\r\n",
        "        # hidden state given an action and the previous hidden state.\r\n",
        "        parent = search_path[-2]\r\n",
        "        \r\n",
        "        #network_output = network.recurrent_inference(parent.hidden_state, action)\r\n",
        "        next_state,r,action_probs, value = dynamics(net,parent.hidden_state,onehot(action,len(action_space))) \r\n",
        "        expand_node(node, to_play, action_space,next_state,r,action_probs)#node, to_play, actions_space ,hidden_state,reward,policy\r\n",
        "\r\n",
        "        backpropagate(search_path, value, to_play, discount, min_max_stats)#search_path, value,,discount, min_max_stats\r\n",
        "    return root    \r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-NpN4lU12kW"
      },
      "source": [
        "import gym\r\n",
        "class ScalingObservationWrapper(gym.ObservationWrapper):\r\n",
        "    \"\"\"\r\n",
        "    Wrapper that apply a min-max scaling of observations.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, env, low=None, high=None):\r\n",
        "        super().__init__(env)\r\n",
        "        assert isinstance(env.observation_space, gym.spaces.Box)\r\n",
        "\r\n",
        "        low = np.array(self.observation_space.low if low is None else low)\r\n",
        "        high = np.array(self.observation_space.high if high is None else high)\r\n",
        "\r\n",
        "        self.mean = (high + low) / 2\r\n",
        "        self.max = high - self.mean\r\n",
        "\r\n",
        "    def observation(self, observation):\r\n",
        "        return (observation - self.mean) / self.max"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waeVGfWytBB1"
      },
      "source": [
        "\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "from tqdm import tqdm\r\n",
        "def onehot(a,n=2):\r\n",
        "  return np.eye(n)[a]\r\n",
        "def play_game(env,net,n_sim,discount,render,device,n_act,max_steps):\r\n",
        "    trajectory=[]\r\n",
        "    state = env.reset() \r\n",
        "    done = False\r\n",
        "    r =0 \r\n",
        "    stp=0\r\n",
        "    while not done:\r\n",
        "        if render:\r\n",
        "          env.render()\r\n",
        "        stp+=1  \r\n",
        "        h ,prob,pred_value= net.inference_initial_state(torch.tensor([state]).float().to(device)) \r\n",
        "        root  = run_mcts(net,h.cpu(),prob.cpu(),pred_value.cpu(),num_simulations=n_sim,discount=discount)\r\n",
        "        action,action_prob,mcts_val = select_action(root) \r\n",
        "        next_state, reward, done, info = env.step(action)\r\n",
        "        r+=reward\r\n",
        "        if stp>max_steps:\r\n",
        "          done = True\r\n",
        "        data = (state,onehot(action,n_act),action_prob,mcts_val,reward,pred_value.cpu())\r\n",
        "        trajectory.append(data)\r\n",
        "        state = next_state\r\n",
        "    print(\"DATA collection:played for \",len(trajectory),\" steps , rewards\",r)   \r\n",
        "    return trajectory    \r\n",
        "def eval_game(env,net,n_sim,render,device,max_steps):\r\n",
        "    state = env.reset() \r\n",
        "    done = False\r\n",
        "    r = 0\r\n",
        "    stp=0\r\n",
        "    while not done:\r\n",
        "        if render:\r\n",
        "          env.render()\r\n",
        "        stp+=1  \r\n",
        "        h ,prob,value= net.inference_initial_state(torch.tensor([state]).float().to(device)) \r\n",
        "        root  = run_mcts(net,h.cpu(),prob.cpu(),value.cpu(),num_simulations=n_sim,discount=discount)\r\n",
        "        action,action_prob,mcts_val = select_action(root,\"max\")\r\n",
        "        next_state, reward, done, info = env.step(action)\r\n",
        "        if stp>max_steps:\r\n",
        "          done = True\r\n",
        "        r+=reward\r\n",
        "        state = next_state\r\n",
        "    print(\"Eval:played for \",r ,\" rewards\")   \r\n",
        "    \r\n",
        "def sample_games(buffer,batch_size):\r\n",
        "    # Sample game from buffer either uniformly or according to some priority\r\n",
        "    #print(\"samplig from .\",len(buffer))\r\n",
        "    return random.choices(buffer, k=batch_size)\r\n",
        "\r\n",
        "def sample_position(trajectory,priority=None):\r\n",
        "    # Sample position from game either uniformly or according to some priority.\r\n",
        "    if priority == None:\r\n",
        "      return np.random.choice(len(trajectory),1)[0]\r\n",
        "    return np.random.choice(len(trajectory),1,p = priority)[0]\r\n",
        "    #return np.random.choice(list(range(0, len(trajectory))),1,p = priority)[0]\r\n",
        "def get_priorities(root_values,rewards,discount=0.99, td_steps=10):\r\n",
        "    z_values = []\r\n",
        "    alpha = 1\r\n",
        "    beta = 1 \r\n",
        "    for current_index in range(len(root_values)):\r\n",
        "        bootstrap_index = current_index + td_steps\r\n",
        "        if bootstrap_index < len(root_values):\r\n",
        "            value = root_values[bootstrap_index] * discount ** td_steps\r\n",
        "        else:\r\n",
        "            value = 0\r\n",
        "\r\n",
        "        for i, reward in enumerate(rewards[current_index:bootstrap_index]):\r\n",
        "            value += reward * discount ** i\r\n",
        "\r\n",
        "        if current_index < len(root_values):\r\n",
        "            z_values.append(value)\r\n",
        "    p = np.abs(np.array(root_values)-np.array(z_values))**alpha  \r\n",
        "    priority = p /np.sum(p)\r\n",
        "    N= len(root_values) #????????????????????????????????????????????????????????????????????????????????????????????????????????\r\n",
        "    weights = (1/(N*priority))**beta\r\n",
        "    return list(priority),list(weights)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def sample_batch(action_space_size,buffer,discount,batch_size,num_unroll_steps, td_steps,per):\r\n",
        "    obs_batch, action_batch, reward_batch, value_batch, policy_batch,weights_batch = [], [], [], [], [],[]\r\n",
        "    games = sample_games(buffer,batch_size)\r\n",
        "    for g in games:\r\n",
        "      state,action,action_prob,root_val,reward,pred_val = zip(*g)\r\n",
        "      state,action,action_prob,root_val,reward,pred_val  =list(state),list(action),list(action_prob),list(root_val),list(reward),list(pred_val)\r\n",
        "      if per:\r\n",
        "        #make priority for sampling from root_value and n_step value\r\n",
        "        priority,weights = get_priorities(root_val,reward,discount=discount, td_steps=td_steps)\r\n",
        "        \r\n",
        "        game_pos = sample_position(g,priority)#state index sampled using priority\r\n",
        "      else:  \r\n",
        "        weights = [1.0]*len(root_val)\r\n",
        "        game_pos = sample_position(g)#state index sampled using priority\r\n",
        "      _actions = action[game_pos:game_pos + num_unroll_steps]\r\n",
        "      # random action selection to complete num_unroll_steps\r\n",
        "      _actions += [onehot(np.random.randint(0, action_space_size),action_space_size)for _ in range(num_unroll_steps - len(_actions))]\r\n",
        "\r\n",
        "      obs_batch.append(state[game_pos])\r\n",
        "      action_batch.append(_actions)\r\n",
        "      value, reward, policy = make_target(child_visits=action_prob ,root_values=root_val,rewards=reward,state_index=game_pos,discount=discount, num_unroll_steps=num_unroll_steps, td_steps=td_steps)\r\n",
        "      reward_batch.append(reward)\r\n",
        "      value_batch.append(value)\r\n",
        "      policy_batch.append(policy)\r\n",
        "      weights_batch.append(weights[game_pos])\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    obs_batch = torch.tensor(obs_batch).float()\r\n",
        "    action_batch = torch.tensor(action_batch).long()\r\n",
        "    reward_batch = torch.tensor(reward_batch).float()\r\n",
        "    value_batch = torch.tensor(value_batch).float()\r\n",
        "    policy_batch = torch.tensor(policy_batch).float()\r\n",
        "    weights_batch = torch.tensor(weights_batch).float()\r\n",
        "    return obs_batch, action_batch, reward_batch, value_batch, policy_batch,weights_batch\r\n",
        "\r\n",
        "\r\n",
        "def make_target(child_visits,root_values,rewards,state_index,discount=0.99, num_unroll_steps=5, td_steps=10):\r\n",
        "        # The value target is the discounted root value of the search tree N steps into the future, plus\r\n",
        "        # the discounted sum of all rewards until then.\r\n",
        "        target_values, target_rewards, target_policies = [], [], []\r\n",
        "        for current_index in range(state_index, state_index + num_unroll_steps + 1):\r\n",
        "            bootstrap_index = current_index + td_steps\r\n",
        "            if bootstrap_index < len(root_values):\r\n",
        "                value = root_values[bootstrap_index] * discount ** td_steps\r\n",
        "            else:\r\n",
        "                value = 0\r\n",
        "\r\n",
        "            for i, reward in enumerate(rewards[current_index:bootstrap_index]):\r\n",
        "                value += reward * discount ** i\r\n",
        "\r\n",
        "            if current_index < len(root_values):\r\n",
        "                target_values.append(stcat(value))\r\n",
        "                target_rewards.append(stcat(rewards[current_index]))\r\n",
        "                target_policies.append(child_visits[current_index])\r\n",
        "\r\n",
        "            else:\r\n",
        "                # States past the end of games are treated as absorbing states.\r\n",
        "                target_values.append(stcat(0))\r\n",
        "                target_rewards.append(stcat(0))\r\n",
        "                # Note: Target policy is  set to 0 so that no policy loss is calculated for them\r\n",
        "                #target_policies.append([0 for _ in range(len(child_visits[0]))])\r\n",
        "                target_policies.append(child_visits[0]*0.0)\r\n",
        "\r\n",
        "        return target_values, target_rewards, target_policies\r\n",
        "\r\n",
        "\r\n",
        "def scalar_reward_loss( prediction, target):\r\n",
        "        return -(torch.log(prediction) * target).sum(1)\r\n",
        "\r\n",
        "def scalar_value_loss( prediction, target):\r\n",
        "        return -(torch.log(prediction) * target).sum(1)\r\n",
        "def update_weights(model, action_space_size, optimizer, replay_buffer,discount,batch_size,num_unroll_steps, td_steps,per ):\r\n",
        "    batch = sample_batch(action_space_size,replay_buffer,discount,batch_size,num_unroll_steps, td_steps,per)\r\n",
        "    obs_batch, action_batch, target_reward, target_value, target_policy,target_weights = batch\r\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
        "    obs_batch = obs_batch.to(device)\r\n",
        "    action_batch = action_batch.to(device)#.unsqueeze(-1) # its not onehot yet \r\n",
        "    target_reward = target_reward.to(device)\r\n",
        "    target_value = target_value.to(device)\r\n",
        "    target_policy = target_policy.to(device)\r\n",
        "    target_weights = target_weights.to(device)\r\n",
        "\r\n",
        "    # transform targets to categorical representation # its already done\r\n",
        "    # Reference:  Appendix F\r\n",
        "    #transformed_target_reward = config.scalar_transform(target_reward)\r\n",
        "    target_reward_phi =target_reward #config.reward_phi(transformed_target_reward)\r\n",
        "    #transformed_target_value = config.scalar_transform(target_value)\r\n",
        "    target_value_phi = target_value#config.value_phi(transformed_target_value)\r\n",
        "\r\n",
        "    hidden_state, policy_prob,value  = model.initial_state(obs_batch) # initial model_call ###################################### make changes\r\n",
        "    #h,init_pred_p,init_pred_v = net.initial_state(in_s)\r\n",
        "\r\n",
        "    value_loss = scalar_value_loss(value, target_value_phi[:, 0])\r\n",
        "    policy_loss = -(torch.log(policy_prob) * target_policy[:, 0]).sum(1)\r\n",
        "    reward_loss = torch.zeros(batch_size, device=device)\r\n",
        "\r\n",
        "    gradient_scale = 1 / num_unroll_steps\r\n",
        "    for step_i in range(num_unroll_steps):\r\n",
        "        hidden_state, reward,policy_prob,value  = model.next_state(hidden_state, action_batch[:, step_i]) ######################### make changes\r\n",
        "        #h,pred_reward,pred_policy,pred_value= net.next_state(h,act)\r\n",
        "        policy_loss += -(torch.log(policy_prob) * target_policy[:, step_i + 1]).sum(1)\r\n",
        "        value_loss += scalar_value_loss(value, target_value_phi[:, step_i + 1])\r\n",
        "        reward_loss += scalar_reward_loss(reward, target_reward_phi[:, step_i])\r\n",
        "        hidden_state.register_hook(lambda grad: grad * 0.5)\r\n",
        "\r\n",
        "    # optimize\r\n",
        "    value_loss_coeff = 1\r\n",
        "    loss = (policy_loss + value_loss_coeff * value_loss + reward_loss) # find value loss coefficiet = 1?\r\n",
        "    weights = target_weights#/target_weights.max()\r\n",
        "    weighted_loss = (weights * loss).mean()#1?\r\n",
        "    weighted_loss.register_hook(lambda grad: grad * gradient_scale)\r\n",
        "    loss = loss.mean()\r\n",
        "\r\n",
        "    optimizer.zero_grad()\r\n",
        "    weighted_loss.backward()\r\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 5)#5?\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "def adjust_lr(optimizer, step_count):\r\n",
        "\r\n",
        "    lr_init=0.05\r\n",
        "    lr_decay_rate=0.01\r\n",
        "    lr_decay_steps=10000\r\n",
        "    lr = lr_init * lr_decay_rate ** (step_count / lr_decay_steps)\r\n",
        "    lr = max(lr, 0.001)\r\n",
        "    for param_group in optimizer.param_groups:\r\n",
        "        param_group['lr'] = lr\r\n",
        "    return lr\r\n",
        "\r\n",
        "\r\n",
        "learning_rate = [0.05]   \r\n",
        "def net_train(net,  action_space_size, replay_buffer,discount,batch_size,num_unroll_steps, td_steps,training_steps=1000,per = False):\r\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
        "    model =net\r\n",
        "    #MuZeroNet(input_size=4, action_space_n=2, reward_support_size=5, value_support_size=5).to(device) #training fresh net\r\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate[0], momentum=0.9,weight_decay=1e-4)\r\n",
        "    #training_steps=training_steps=500#20000\r\n",
        "    # wait for replay buffer to be non-empty\r\n",
        "    while len(replay_buffer) == 0:\r\n",
        "        pass\r\n",
        "\r\n",
        "    for step_count in tqdm(range(training_steps)):\r\n",
        "        learning_rate[0] = adjust_lr( optimizer, step_count)\r\n",
        "        update_weights(model, action_space_size, optimizer, replay_buffer,discount,batch_size,num_unroll_steps, td_steps,per)\r\n",
        "    return model\r\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZWra51wFVvb",
        "outputId": "d7cac4fa-51d9-42cd-f8f3-b37dae0a1dfc"
      },
      "source": [
        "import gym\r\n",
        "import numpy as np\r\n",
        "from collections import deque\r\n",
        "\r\n",
        "render = False\r\n",
        "episodes_per_train=30\r\n",
        "episodes_per_eval =5\r\n",
        "#buffer =[]\r\n",
        "buffer = deque(maxlen = episodes_per_train)\r\n",
        "training_steps=50\r\n",
        "max_steps=5000\r\n",
        "n_sim= 50\r\n",
        "discount = 0.99\r\n",
        "batch_size = 512\r\n",
        "envs = ['CartPole-v1','MountainCar-v0','LunarLander-v2']\r\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
        "print(\"training for \",envs[2])\r\n",
        "env=gym.make(envs[2])\r\n",
        "#env=env.unwrapped\r\n",
        "#env = ScalingObservationWrapper(env, low=[-2.4, -2.0, -0.42, -3.5], high=[2.4, 2.0, 0.42, 3.5])\r\n",
        "\r\n",
        "s_dim =env.observation_space.shape[0]\r\n",
        "print(\"s_dim: \",s_dim)\r\n",
        "a_dim =env.action_space.n\r\n",
        "print(\"a_dim: \",a_dim)\r\n",
        "a_bound =1 #env.action_space.high[0]\r\n",
        "print(\"a_bound: \",a_bound)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "net = MuZeroNet(input_size=s_dim, action_space_n=a_dim, reward_support_size=5, value_support_size=5).to(device)\r\n",
        "\r\n",
        "for t in range(training_steps):\r\n",
        "  for _ in range(episodes_per_train):\r\n",
        "    buffer.append(play_game(env,net,n_sim,discount,render,device,a_dim,max_steps))\r\n",
        "  print(\"training from \",len(buffer),\" games\")  \r\n",
        "  if t<20:\r\n",
        "    priority = True \r\n",
        "    tr_stp=500\r\n",
        "  else :\r\n",
        "    tr_stp=2000\r\n",
        "    priority =False\r\n",
        "  net = net_train(net,  action_space_size=a_dim, replay_buffer=buffer,discount=discount,batch_size=batch_size,num_unroll_steps=5, td_steps=10,training_steps=tr_stp,per = priority)\r\n",
        "  for _ in range(episodes_per_eval):\r\n",
        "    eval_game(env,net,n_sim,render,device,max_steps)\r\n",
        "  \r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training for  LunarLander-v2\n",
            "s_dim:  8\n",
            "a_dim:  4\n",
            "a_bound:  1\n",
            "DATA collection:played for  88  steps , rewards -101.85668461706629\n",
            "DATA collection:played for  104  steps , rewards -117.76086883613767\n",
            "DATA collection:played for  99  steps , rewards -504.50921462555596\n",
            "DATA collection:played for  103  steps , rewards -120.95003351036684\n",
            "DATA collection:played for  95  steps , rewards -408.40362463389835\n",
            "DATA collection:played for  76  steps , rewards -97.81801424871111\n",
            "DATA collection:played for  105  steps , rewards -464.0141749483592\n",
            "DATA collection:played for  74  steps , rewards -303.38397742425855\n",
            "DATA collection:played for  75  steps , rewards -232.2964755769686\n",
            "DATA collection:played for  82  steps , rewards -92.08698841593089\n",
            "DATA collection:played for  127  steps , rewards -395.25222689318514\n",
            "DATA collection:played for  109  steps , rewards -194.6926962643017\n",
            "DATA collection:played for  91  steps , rewards -233.71634342907694\n",
            "DATA collection:played for  110  steps , rewards -165.30159527500166\n",
            "DATA collection:played for  60  steps , rewards -234.62169379825002\n",
            "DATA collection:played for  100  steps , rewards -271.4157228305191\n",
            "DATA collection:played for  79  steps , rewards -187.91524801114008\n",
            "DATA collection:played for  86  steps , rewards -315.84348678278786\n",
            "DATA collection:played for  121  steps , rewards -175.339508864621\n",
            "DATA collection:played for  66  steps , rewards -101.43821477145559\n",
            "DATA collection:played for  88  steps , rewards -165.50534647184026\n",
            "DATA collection:played for  76  steps , rewards -108.58423801660682\n",
            "DATA collection:played for  77  steps , rewards -162.0121468241143\n",
            "DATA collection:played for  66  steps , rewards -96.268838881996\n",
            "DATA collection:played for  125  steps , rewards -314.62506569897\n",
            "DATA collection:played for  104  steps , rewards -71.06846005834703\n",
            "DATA collection:played for  84  steps , rewards -143.50026670994043\n",
            "DATA collection:played for  77  steps , rewards -173.69750548907766\n",
            "DATA collection:played for  80  steps , rewards -84.29521590782228\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  86  steps , rewards -168.3994302592149\n",
            "training from  30  games\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [03:55<00:00,  2.12it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  -369.18820124380767  rewards\n",
            "Eval:played for  -104.46200592694606  rewards\n",
            "Eval:played for  -189.8690013828629  rewards\n",
            "Eval:played for  -117.02598889453671  rewards\n",
            "Eval:played for  -522.5154526453116  rewards\n",
            "DATA collection:played for  92  steps , rewards -28.066749886392046\n",
            "DATA collection:played for  72  steps , rewards -150.40891211377502\n",
            "DATA collection:played for  89  steps , rewards -241.1991161257918\n",
            "DATA collection:played for  71  steps , rewards -136.47378143620304\n",
            "DATA collection:played for  105  steps , rewards -301.26114194733054\n",
            "DATA collection:played for  83  steps , rewards -328.08624423505455\n",
            "DATA collection:played for  82  steps , rewards -278.38412454805996\n",
            "DATA collection:played for  94  steps , rewards -514.1207208279138\n",
            "DATA collection:played for  76  steps , rewards -256.0368771162459\n",
            "DATA collection:played for  83  steps , rewards -263.47190839491043\n",
            "DATA collection:played for  85  steps , rewards -221.06833835610166\n",
            "DATA collection:played for  65  steps , rewards -202.71556944614903\n",
            "DATA collection:played for  72  steps , rewards -164.48359924725548\n",
            "DATA collection:played for  71  steps , rewards 21.034511061021092\n",
            "DATA collection:played for  85  steps , rewards -236.83676208729946\n",
            "DATA collection:played for  91  steps , rewards -441.24643052976114\n",
            "DATA collection:played for  70  steps , rewards -86.88465223732314\n",
            "DATA collection:played for  84  steps , rewards -120.32553870128461\n",
            "DATA collection:played for  69  steps , rewards -136.29785638610792\n",
            "DATA collection:played for  104  steps , rewards -169.15809843384127\n",
            "DATA collection:played for  98  steps , rewards -378.49473957545416\n",
            "DATA collection:played for  97  steps , rewards -66.06813458958584\n",
            "DATA collection:played for  62  steps , rewards -34.27627191815871\n",
            "DATA collection:played for  83  steps , rewards -465.4832684467951\n",
            "DATA collection:played for  91  steps , rewards -259.9296823288766\n",
            "DATA collection:played for  60  steps , rewards -116.8860570019593\n",
            "DATA collection:played for  68  steps , rewards -119.51150640737119\n",
            "DATA collection:played for  54  steps , rewards -83.87308656620971\n",
            "DATA collection:played for  69  steps , rewards -64.90916425697966\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  93  steps , rewards -448.42437885680044\n",
            "training from  30  games\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [03:44<00:00,  2.23it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  -231.34640141883276  rewards\n",
            "Eval:played for  9.229893036200224  rewards\n",
            "Eval:played for  -208.63988720240894  rewards\n",
            "Eval:played for  -271.6539387968308  rewards\n",
            "Eval:played for  -299.85026892178706  rewards\n",
            "DATA collection:played for  99  steps , rewards -237.22378647222234\n",
            "DATA collection:played for  56  steps , rewards -113.16520204148878\n",
            "DATA collection:played for  66  steps , rewards -184.809342521921\n",
            "DATA collection:played for  99  steps , rewards -334.45159157348905\n",
            "DATA collection:played for  77  steps , rewards -370.7532199053163\n",
            "DATA collection:played for  81  steps , rewards -365.3790383559971\n",
            "DATA collection:played for  62  steps , rewards -135.56536607687528\n",
            "DATA collection:played for  69  steps , rewards -301.6412036095619\n",
            "DATA collection:played for  56  steps , rewards -214.05972241013495\n",
            "DATA collection:played for  106  steps , rewards -208.7046497145085\n",
            "DATA collection:played for  73  steps , rewards -39.61481569224097\n",
            "DATA collection:played for  54  steps , rewards -156.83960173620255\n",
            "DATA collection:played for  74  steps , rewards -185.50528433121104\n",
            "DATA collection:played for  69  steps , rewards -233.58690311771812\n",
            "DATA collection:played for  97  steps , rewards -260.75213745374987\n",
            "DATA collection:played for  86  steps , rewards -307.144729067585\n",
            "DATA collection:played for  88  steps , rewards -206.35910113980196\n",
            "DATA collection:played for  74  steps , rewards -247.56089664065888\n",
            "DATA collection:played for  73  steps , rewards -258.4216481560293\n",
            "DATA collection:played for  62  steps , rewards -180.25856061675307\n",
            "DATA collection:played for  71  steps , rewards -219.74142366937974\n",
            "DATA collection:played for  98  steps , rewards -428.2645997252637\n",
            "DATA collection:played for  73  steps , rewards -183.6823567283384\n",
            "DATA collection:played for  89  steps , rewards -199.9839709057543\n",
            "DATA collection:played for  87  steps , rewards -339.7986286601107\n",
            "DATA collection:played for  77  steps , rewards -227.6157451624636\n",
            "DATA collection:played for  65  steps , rewards -218.49890457554255\n",
            "DATA collection:played for  60  steps , rewards -223.59630965969973\n",
            "DATA collection:played for  101  steps , rewards -222.94068115146246\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  59  steps , rewards -180.85467494771046\n",
            "training from  30  games\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [03:40<00:00,  2.27it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  -156.37000047530657  rewards\n",
            "Eval:played for  -129.600771095379  rewards\n",
            "Eval:played for  -243.14965506556672  rewards\n",
            "Eval:played for  -205.9169200635184  rewards\n",
            "Eval:played for  -146.77096774053422  rewards\n",
            "DATA collection:played for  59  steps , rewards 0.46725029711898003\n",
            "DATA collection:played for  85  steps , rewards -218.72563350437267\n",
            "DATA collection:played for  84  steps , rewards -270.6362673671854\n",
            "DATA collection:played for  81  steps , rewards -199.48262517663844\n",
            "DATA collection:played for  60  steps , rewards -154.91768394430048\n",
            "DATA collection:played for  64  steps , rewards 17.420894917305944\n",
            "DATA collection:played for  105  steps , rewards -216.9261013399867\n",
            "DATA collection:played for  68  steps , rewards -155.38347238019907\n",
            "DATA collection:played for  99  steps , rewards -240.20013248737885\n",
            "DATA collection:played for  101  steps , rewards -297.1714646225172\n",
            "DATA collection:played for  68  steps , rewards -23.554954898611967\n",
            "DATA collection:played for  85  steps , rewards -240.90004158469887\n",
            "DATA collection:played for  74  steps , rewards -198.90679320234906\n",
            "DATA collection:played for  92  steps , rewards -321.39992370415473\n",
            "DATA collection:played for  59  steps , rewards -132.9651826539809\n",
            "DATA collection:played for  68  steps , rewards -31.82447957166869\n",
            "DATA collection:played for  118  steps , rewards -270.64676396087225\n",
            "DATA collection:played for  99  steps , rewards -312.52170684167993\n",
            "DATA collection:played for  92  steps , rewards -200.88796637280984\n",
            "DATA collection:played for  57  steps , rewards -140.1112172731165\n",
            "DATA collection:played for  107  steps , rewards -219.89942334477064\n",
            "DATA collection:played for  65  steps , rewards -129.4761049849629\n",
            "DATA collection:played for  51  steps , rewards -87.6136133273155\n",
            "DATA collection:played for  62  steps , rewards -11.861630859936056\n",
            "DATA collection:played for  111  steps , rewards -337.6923473175797\n",
            "DATA collection:played for  69  steps , rewards -139.97503389317535\n",
            "DATA collection:played for  96  steps , rewards -238.57968155092937\n",
            "DATA collection:played for  119  steps , rewards -291.68939744965144\n",
            "DATA collection:played for  60  steps , rewards -157.88594502803159\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  57  steps , rewards -125.16943223889896\n",
            "training from  30  games\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [03:46<00:00,  2.21it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  -227.1969331842232  rewards\n",
            "Eval:played for  -116.9191553130954  rewards\n",
            "Eval:played for  -156.53398669401176  rewards\n",
            "Eval:played for  -163.3744495797692  rewards\n",
            "Eval:played for  -274.172345068608  rewards\n",
            "DATA collection:played for  57  steps , rewards -129.39826555788562\n",
            "DATA collection:played for  70  steps , rewards -222.55023231903135\n",
            "DATA collection:played for  90  steps , rewards -319.3756617259369\n",
            "DATA collection:played for  74  steps , rewards -218.14281178587692\n",
            "DATA collection:played for  58  steps , rewards -148.67591977812887\n",
            "DATA collection:played for  52  steps , rewards -157.03842553415103\n",
            "DATA collection:played for  61  steps , rewards -129.09913436625916\n",
            "DATA collection:played for  63  steps , rewards -39.19527900815342\n",
            "DATA collection:played for  70  steps , rewards -110.22606093370221\n",
            "DATA collection:played for  87  steps , rewards -322.60402060531317\n",
            "DATA collection:played for  84  steps , rewards -160.77119634975867\n",
            "DATA collection:played for  54  steps , rewards -96.75003454037235\n",
            "DATA collection:played for  59  steps , rewards -101.33257189830478\n",
            "DATA collection:played for  68  steps , rewards -155.37059107318032\n",
            "DATA collection:played for  111  steps , rewards -361.0037481222355\n",
            "DATA collection:played for  82  steps , rewards -269.514794039334\n",
            "DATA collection:played for  79  steps , rewards -213.93518834792224\n",
            "DATA collection:played for  58  steps , rewards -81.78377676147319\n",
            "DATA collection:played for  52  steps , rewards -129.56327092942084\n",
            "DATA collection:played for  120  steps , rewards -301.94088015746934\n",
            "DATA collection:played for  111  steps , rewards -9.304451776836743\n",
            "DATA collection:played for  61  steps , rewards -101.63323990332603\n",
            "DATA collection:played for  97  steps , rewards -164.00668994380084\n",
            "DATA collection:played for  54  steps , rewards -133.77906975405722\n",
            "DATA collection:played for  52  steps , rewards -93.2735583421907\n",
            "DATA collection:played for  82  steps , rewards -191.44122911339565\n",
            "DATA collection:played for  85  steps , rewards -53.950104599016484\n",
            "DATA collection:played for  79  steps , rewards -40.03527950643054\n",
            "DATA collection:played for  61  steps , rewards -136.5087581075473\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  73  steps , rewards -162.36998047239223\n",
            "training from  30  games\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [03:37<00:00,  2.30it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  -235.15302562408866  rewards\n",
            "Eval:played for  -88.24096169024573  rewards\n",
            "Eval:played for  -176.12740092411747  rewards\n",
            "Eval:played for  -305.91034921208416  rewards\n",
            "Eval:played for  -116.47065699949933  rewards\n",
            "DATA collection:played for  159  steps , rewards -793.7208403917292\n",
            "DATA collection:played for  121  steps , rewards -172.8626440832067\n",
            "DATA collection:played for  81  steps , rewards -494.7437481925009\n",
            "DATA collection:played for  130  steps , rewards -393.45219144730083\n",
            "DATA collection:played for  58  steps , rewards -112.81136496550278\n",
            "DATA collection:played for  121  steps , rewards -247.20135935153309\n",
            "DATA collection:played for  116  steps , rewards -232.38712753518067\n",
            "DATA collection:played for  120  steps , rewards -207.56879537387994\n",
            "DATA collection:played for  140  steps , rewards -286.79238622223386\n",
            "DATA collection:played for  120  steps , rewards -263.1512801851987\n",
            "DATA collection:played for  184  steps , rewards -471.9672969727361\n",
            "DATA collection:played for  140  steps , rewards -256.4558317465048\n",
            "DATA collection:played for  112  steps , rewards -157.0893069334399\n",
            "DATA collection:played for  117  steps , rewards -243.38268765786705\n",
            "DATA collection:played for  83  steps , rewards -542.961885258405\n",
            "DATA collection:played for  123  steps , rewards -219.76954988304766\n",
            "DATA collection:played for  149  steps , rewards -921.4192961076436\n",
            "DATA collection:played for  120  steps , rewards -212.40386847485797\n",
            "DATA collection:played for  67  steps , rewards -120.91552463304782\n",
            "DATA collection:played for  117  steps , rewards -192.93487038678575\n",
            "DATA collection:played for  201  steps , rewards -464.7430938556431\n",
            "DATA collection:played for  175  steps , rewards -399.8915960524283\n",
            "DATA collection:played for  137  steps , rewards -375.2286508366278\n",
            "DATA collection:played for  53  steps , rewards -109.8311301778359\n",
            "DATA collection:played for  54  steps , rewards -78.80605120213238\n",
            "DATA collection:played for  119  steps , rewards -225.10995197497536\n",
            "DATA collection:played for  137  steps , rewards -296.49305773215843\n",
            "DATA collection:played for  177  steps , rewards -426.3631555108249\n",
            "DATA collection:played for  139  steps , rewards -411.5459259932012\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  122  steps , rewards -204.05857537220703\n",
            "training from  30  games\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [04:39<00:00,  1.79it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  -73.02227592782693  rewards\n",
            "Eval:played for  -69.10417801037228  rewards\n",
            "Eval:played for  -111.14255847283309  rewards\n",
            "Eval:played for  -117.93545355145733  rewards\n",
            "Eval:played for  -97.80495293433655  rewards\n",
            "DATA collection:played for  124  steps , rewards -143.65449797202933\n",
            "DATA collection:played for  84  steps , rewards -96.32055170017279\n",
            "DATA collection:played for  156  steps , rewards -101.3450920324785\n",
            "DATA collection:played for  59  steps , rewards -101.7489563636547\n",
            "DATA collection:played for  112  steps , rewards -141.3741267465536\n",
            "DATA collection:played for  146  steps , rewards -158.07955009791965\n",
            "DATA collection:played for  67  steps , rewards -104.75138379715114\n",
            "DATA collection:played for  125  steps , rewards -118.57883858699716\n",
            "DATA collection:played for  124  steps , rewards -107.9960839052333\n",
            "DATA collection:played for  69  steps , rewards -81.16183642584916\n",
            "DATA collection:played for  76  steps , rewards -121.223957262477\n",
            "DATA collection:played for  65  steps , rewards -108.48967054921478\n",
            "DATA collection:played for  53  steps , rewards -122.36261443463559\n",
            "DATA collection:played for  107  steps , rewards -111.09869371608936\n",
            "DATA collection:played for  142  steps , rewards -178.94500512821986\n",
            "DATA collection:played for  123  steps , rewards -73.4190841705925\n",
            "DATA collection:played for  76  steps , rewards -102.71332272354792\n",
            "DATA collection:played for  63  steps , rewards -110.7421333728587\n",
            "DATA collection:played for  117  steps , rewards -90.99982213935189\n",
            "DATA collection:played for  137  steps , rewards -95.99556986578074\n",
            "DATA collection:played for  54  steps , rewards -80.40853026739777\n",
            "DATA collection:played for  57  steps , rewards -91.08139275315729\n",
            "DATA collection:played for  102  steps , rewards -90.80498533303279\n",
            "DATA collection:played for  53  steps , rewards -82.11894650344516\n",
            "DATA collection:played for  55  steps , rewards -92.88766514903317\n",
            "DATA collection:played for  68  steps , rewards -136.37665800756346\n",
            "DATA collection:played for  79  steps , rewards -123.87843327065018\n",
            "DATA collection:played for  143  steps , rewards -130.23585791718818\n",
            "DATA collection:played for  59  steps , rewards -109.17641279286302\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  57  steps , rewards -108.6875723383821\n",
            "training from  30  games\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [04:01<00:00,  2.07it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  -85.76180263395794  rewards\n",
            "Eval:played for  -79.30968179149794  rewards\n",
            "Eval:played for  -220.75713966250535  rewards\n",
            "Eval:played for  -82.38907438101592  rewards\n",
            "Eval:played for  -95.19095359270511  rewards\n",
            "DATA collection:played for  149  steps , rewards -107.47821949125746\n",
            "DATA collection:played for  227  steps , rewards -202.5525976463028\n",
            "DATA collection:played for  63  steps , rewards -97.18887331094588\n",
            "DATA collection:played for  93  steps , rewards -52.40443270220027\n",
            "DATA collection:played for  201  steps , rewards -124.42095719590884\n",
            "DATA collection:played for  135  steps , rewards -84.38364677496921\n",
            "DATA collection:played for  187  steps , rewards -89.91408662983284\n",
            "DATA collection:played for  220  steps , rewards -119.58412512694821\n",
            "DATA collection:played for  62  steps , rewards -71.19903038815903\n",
            "DATA collection:played for  63  steps , rewards -110.64119778937729\n",
            "DATA collection:played for  97  steps , rewards -90.40517915600883\n",
            "DATA collection:played for  142  steps , rewards -64.99638925471368\n",
            "DATA collection:played for  74  steps , rewards -53.064318014618415\n",
            "DATA collection:played for  63  steps , rewards -119.83360869055844\n",
            "DATA collection:played for  255  steps , rewards -115.5314031269146\n",
            "DATA collection:played for  138  steps , rewards -85.09252341145194\n",
            "DATA collection:played for  725  steps , rewards -134.24634426954506\n",
            "DATA collection:played for  92  steps , rewards -108.28923276737892\n",
            "DATA collection:played for  342  steps , rewards -193.45997878807674\n",
            "DATA collection:played for  270  steps , rewards -132.1332732921319\n",
            "DATA collection:played for  140  steps , rewards -28.053569230298223\n",
            "DATA collection:played for  200  steps , rewards -94.29464828419265\n",
            "DATA collection:played for  59  steps , rewards -112.57618599157558\n",
            "DATA collection:played for  322  steps , rewards -122.71162135705714\n",
            "DATA collection:played for  388  steps , rewards -292.6491723146212\n",
            "DATA collection:played for  99  steps , rewards -161.23366847495973\n",
            "DATA collection:played for  121  steps , rewards -102.69288836311955\n",
            "DATA collection:played for  160  steps , rewards -82.60755787883373\n",
            "DATA collection:played for  143  steps , rewards -54.29825570671207\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  60  steps , rewards -68.80760362843088\n",
            "training from  30  games\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [05:38<00:00,  1.48it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  -107.18823920580786  rewards\n",
            "Eval:played for  -120.74106821452183  rewards\n",
            "Eval:played for  -121.46276144676179  rewards\n",
            "Eval:played for  -159.02937635729592  rewards\n",
            "Eval:played for  -130.58369878322677  rewards\n",
            "DATA collection:played for  116  steps , rewards -110.16140782675829\n",
            "DATA collection:played for  73  steps , rewards -87.94932741858953\n",
            "DATA collection:played for  97  steps , rewards -143.41084467474462\n",
            "DATA collection:played for  102  steps , rewards -21.901295469044356\n",
            "DATA collection:played for  109  steps , rewards -158.31014287311504\n",
            "DATA collection:played for  146  steps , rewards -210.57813272875413\n",
            "DATA collection:played for  77  steps , rewards -150.34313250233058\n",
            "DATA collection:played for  93  steps , rewards -87.98457644418522\n",
            "DATA collection:played for  87  steps , rewards -20.79924076669795\n",
            "DATA collection:played for  107  steps , rewards -176.06407548674684\n",
            "DATA collection:played for  147  steps , rewards -91.41871933170597\n",
            "DATA collection:played for  85  steps , rewards -104.86755547151621\n",
            "DATA collection:played for  95  steps , rewards -107.26487174107656\n",
            "DATA collection:played for  71  steps , rewards -7.930011243065948\n",
            "DATA collection:played for  212  steps , rewards -75.59226734358042\n",
            "DATA collection:played for  71  steps , rewards -107.30959649917783\n",
            "DATA collection:played for  85  steps , rewards -162.95139766009433\n",
            "DATA collection:played for  102  steps , rewards -120.84909724216102\n",
            "DATA collection:played for  104  steps , rewards -87.67504840454663\n",
            "DATA collection:played for  72  steps , rewards -112.2885309545411\n",
            "DATA collection:played for  78  steps , rewards -130.6292209671161\n",
            "DATA collection:played for  91  steps , rewards -74.88641373692906\n",
            "DATA collection:played for  121  steps , rewards -140.64686273537728\n",
            "DATA collection:played for  72  steps , rewards -25.435133249715392\n",
            "DATA collection:played for  77  steps , rewards -80.99862065525582\n",
            "DATA collection:played for  112  steps , rewards -273.7541799468323\n",
            "DATA collection:played for  133  steps , rewards -43.987517985037115\n",
            "DATA collection:played for  105  steps , rewards -126.75503263371195\n",
            "DATA collection:played for  103  steps , rewards -129.1464038597789\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  66  steps , rewards -130.83618329222531\n",
            "training from  30  games\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [04:11<00:00,  1.99it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  -91.81726692680022  rewards\n",
            "Eval:played for  -19.099774669449644  rewards\n",
            "Eval:played for  -101.22377651593169  rewards\n",
            "Eval:played for  -84.7628273118444  rewards\n",
            "Eval:played for  -143.01433261464757  rewards\n",
            "DATA collection:played for  148  steps , rewards -17.05724973098698\n",
            "DATA collection:played for  107  steps , rewards -66.53206054651754\n",
            "DATA collection:played for  62  steps , rewards -56.62176414615274\n",
            "DATA collection:played for  133  steps , rewards -26.58356706481115\n",
            "DATA collection:played for  167  steps , rewards -136.50141547470102\n",
            "DATA collection:played for  129  steps , rewards -34.17129145069646\n",
            "DATA collection:played for  79  steps , rewards -96.91541993937281\n",
            "DATA collection:played for  58  steps , rewards -119.88659253923747\n",
            "DATA collection:played for  61  steps , rewards -54.99350905369919\n",
            "DATA collection:played for  104  steps , rewards -161.52527696935664\n",
            "DATA collection:played for  148  steps , rewards -60.07321295321703\n",
            "DATA collection:played for  132  steps , rewards -61.26319228083027\n",
            "DATA collection:played for  69  steps , rewards -126.2449505339682\n",
            "DATA collection:played for  101  steps , rewards -85.02700150126068\n",
            "DATA collection:played for  89  steps , rewards -40.72783852373291\n",
            "DATA collection:played for  79  steps , rewards -88.51616874430152\n",
            "DATA collection:played for  122  steps , rewards -157.3116996735579\n",
            "DATA collection:played for  110  steps , rewards -199.2120493181694\n",
            "DATA collection:played for  89  steps , rewards -98.46880802582864\n",
            "DATA collection:played for  112  steps , rewards -45.64248844273107\n",
            "DATA collection:played for  92  steps , rewards -79.50376293306209\n",
            "DATA collection:played for  65  steps , rewards -87.05806792303062\n",
            "DATA collection:played for  84  steps , rewards -79.0355145998524\n",
            "DATA collection:played for  74  steps , rewards -113.88348873156065\n",
            "DATA collection:played for  72  steps , rewards -112.21175792864949\n",
            "DATA collection:played for  144  steps , rewards -42.470741169552035\n",
            "DATA collection:played for  140  steps , rewards -38.148040021374754\n",
            "DATA collection:played for  157  steps , rewards -62.17338493598682\n",
            "DATA collection:played for  86  steps , rewards -67.6909073663623\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  121  steps , rewards -19.068177301837594\n",
            "training from  30  games\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [04:17<00:00,  1.94it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  -54.779875565424085  rewards\n",
            "Eval:played for  -36.24185923281178  rewards\n",
            "Eval:played for  -67.95766475544451  rewards\n",
            "Eval:played for  -14.51381421841731  rewards\n",
            "Eval:played for  -95.93291479322562  rewards\n",
            "DATA collection:played for  128  steps , rewards 9.336022326054135\n",
            "DATA collection:played for  85  steps , rewards -135.70949317045688\n",
            "DATA collection:played for  99  steps , rewards -97.99108867734824\n",
            "DATA collection:played for  124  steps , rewards -56.16650814535568\n",
            "DATA collection:played for  82  steps , rewards -82.48407754368881\n",
            "DATA collection:played for  68  steps , rewards -54.717589354365856\n",
            "DATA collection:played for  81  steps , rewards -34.000714019594156\n",
            "DATA collection:played for  114  steps , rewards -84.06418293645076\n",
            "DATA collection:played for  54  steps , rewards -54.46752521534446\n",
            "DATA collection:played for  87  steps , rewards -68.36529047439159\n",
            "DATA collection:played for  58  steps , rewards -46.13351422868993\n",
            "DATA collection:played for  75  steps , rewards -49.40282126560007\n",
            "DATA collection:played for  96  steps , rewards -47.45910305034643\n",
            "DATA collection:played for  89  steps , rewards -50.267675342805816\n",
            "DATA collection:played for  84  steps , rewards -41.15414683673944\n",
            "DATA collection:played for  96  steps , rewards -68.05456945690142\n",
            "DATA collection:played for  70  steps , rewards -79.13814249626216\n",
            "DATA collection:played for  86  steps , rewards 40.891558120586666\n",
            "DATA collection:played for  71  steps , rewards -42.30523384597139\n",
            "DATA collection:played for  136  steps , rewards -27.427082767313763\n",
            "DATA collection:played for  77  steps , rewards -110.44099323984862\n",
            "DATA collection:played for  98  steps , rewards -50.800460017521914\n",
            "DATA collection:played for  68  steps , rewards -65.6636591190513\n",
            "DATA collection:played for  102  steps , rewards -38.213233264018925\n",
            "DATA collection:played for  75  steps , rewards -37.47079266129272\n",
            "DATA collection:played for  102  steps , rewards -50.41517189594511\n",
            "DATA collection:played for  65  steps , rewards 4.680870881791037\n",
            "DATA collection:played for  66  steps , rewards -86.9214161980403\n",
            "DATA collection:played for  106  steps , rewards -16.47655689702343\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  81  steps , rewards -123.87317970174487\n",
            "training from  30  games\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [03:56<00:00,  2.11it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  -75.87296310336494  rewards\n",
            "Eval:played for  -182.12812981217934  rewards\n",
            "Eval:played for  -119.09609210281937  rewards\n",
            "Eval:played for  -92.47230144210737  rewards\n",
            "Eval:played for  -73.2740967145973  rewards\n",
            "DATA collection:played for  82  steps , rewards -131.72807537915008\n",
            "DATA collection:played for  88  steps , rewards -43.11373219570882\n",
            "DATA collection:played for  62  steps , rewards -99.87302076147805\n",
            "DATA collection:played for  97  steps , rewards -43.323911366755794\n",
            "DATA collection:played for  98  steps , rewards -104.79335385325709\n",
            "DATA collection:played for  73  steps , rewards -159.31063748755236\n",
            "DATA collection:played for  94  steps , rewards -3.259587897648757\n",
            "DATA collection:played for  70  steps , rewards -130.49043541350903\n",
            "DATA collection:played for  83  steps , rewards -132.2103307652061\n",
            "DATA collection:played for  81  steps , rewards -100.9544741466117\n",
            "DATA collection:played for  93  steps , rewards -47.374002488600155\n",
            "DATA collection:played for  67  steps , rewards -73.06015770487183\n",
            "DATA collection:played for  92  steps , rewards -95.91155929930197\n",
            "DATA collection:played for  75  steps , rewards -88.86232150662065\n",
            "DATA collection:played for  76  steps , rewards -113.31001370502511\n",
            "DATA collection:played for  63  steps , rewards -76.69380601778022\n",
            "DATA collection:played for  87  steps , rewards -104.43298622944465\n",
            "DATA collection:played for  79  steps , rewards -116.75553937626424\n",
            "DATA collection:played for  210  steps , rewards -142.78694325014297\n",
            "DATA collection:played for  100  steps , rewards -192.3441672744454\n",
            "DATA collection:played for  65  steps , rewards -101.08837910675437\n",
            "DATA collection:played for  69  steps , rewards -82.62236126736592\n",
            "DATA collection:played for  64  steps , rewards -99.02692594298371\n",
            "DATA collection:played for  85  steps , rewards -61.80910473561387\n",
            "DATA collection:played for  60  steps , rewards -75.46843687618309\n",
            "DATA collection:played for  77  steps , rewards -110.3664075188748\n",
            "DATA collection:played for  97  steps , rewards -103.81226418743591\n",
            "DATA collection:played for  74  steps , rewards -94.00097578774717\n",
            "DATA collection:played for  57  steps , rewards -99.63662455937875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  63  steps , rewards -81.44128128875144\n",
            "training from  30  games\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [03:51<00:00,  2.16it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  -186.64885111145128  rewards\n",
            "Eval:played for  -29.72091545312246  rewards\n",
            "Eval:played for  -65.78387830633909  rewards\n",
            "Eval:played for  -100.004158925516  rewards\n",
            "Eval:played for  15.589562416033274  rewards\n",
            "DATA collection:played for  111  steps , rewards -38.56502980728096\n",
            "DATA collection:played for  102  steps , rewards 0.34701420136235583\n",
            "DATA collection:played for  102  steps , rewards -33.869665427259605\n",
            "DATA collection:played for  105  steps , rewards -10.376877595525968\n",
            "DATA collection:played for  55  steps , rewards -67.93717335367472\n",
            "DATA collection:played for  93  steps , rewards -69.82536266680508\n",
            "DATA collection:played for  126  steps , rewards -127.71369649126024\n",
            "DATA collection:played for  102  steps , rewards -44.28624679058265\n",
            "DATA collection:played for  65  steps , rewards -37.14624182249463\n",
            "DATA collection:played for  116  steps , rewards -29.06017197396433\n",
            "DATA collection:played for  124  steps , rewards -68.48804489944555\n",
            "DATA collection:played for  95  steps , rewards 6.433069517100094\n",
            "DATA collection:played for  133  steps , rewards -41.77913771825847\n",
            "DATA collection:played for  97  steps , rewards -316.68398557868875\n",
            "DATA collection:played for  112  steps , rewards -88.83630408221968\n",
            "DATA collection:played for  125  steps , rewards -40.585701569327355\n",
            "DATA collection:played for  151  steps , rewards -23.70363919780408\n",
            "DATA collection:played for  133  steps , rewards 18.526254104173304\n",
            "DATA collection:played for  144  steps , rewards -1.6505430709994613\n",
            "DATA collection:played for  138  steps , rewards -22.490502916422216\n",
            "DATA collection:played for  109  steps , rewards 4.653294799929128\n",
            "DATA collection:played for  76  steps , rewards -77.4743286857345\n",
            "DATA collection:played for  90  steps , rewards -154.20530540533446\n",
            "DATA collection:played for  164  steps , rewards -109.20769980990119\n",
            "DATA collection:played for  95  steps , rewards 36.57995697300427\n",
            "DATA collection:played for  188  steps , rewards -25.41864400179945\n",
            "DATA collection:played for  81  steps , rewards -16.965470567641873\n",
            "DATA collection:played for  102  steps , rewards 23.0910326077405\n",
            "DATA collection:played for  85  steps , rewards -37.78700006567418\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  71  steps , rewards 3.8758071490167083\n",
            "training from  30  games\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [04:23<00:00,  1.90it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  -105.73009797580217  rewards\n",
            "Eval:played for  31.75694167100525  rewards\n",
            "Eval:played for  -23.736807230084523  rewards\n",
            "Eval:played for  18.91956337585262  rewards\n",
            "Eval:played for  -70.99821527386499  rewards\n",
            "DATA collection:played for  139  steps , rewards -133.75221149091908\n",
            "DATA collection:played for  121  steps , rewards -26.62548958368886\n",
            "DATA collection:played for  84  steps , rewards -94.97840507979609\n",
            "DATA collection:played for  89  steps , rewards -134.03061854866021\n",
            "DATA collection:played for  94  steps , rewards -77.42796833064078\n",
            "DATA collection:played for  179  steps , rewards -198.18087024692312\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45wHFq7ATuGY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}