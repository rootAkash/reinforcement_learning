{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reanalyze:lunarlander.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP4D/SCF5/efgjv8KboMcoe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rootAkash/reinforcement_learning/blob/master/muzero/Reanalyze_lunarlander.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBD-aKyF5fUr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwI18Z6p1oGP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "592b969b-538b-401f-8783-f35bff0e7405"
      },
      "source": [
        "!pip install gym[all]\n",
        "!pip install box2d-py\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym[all] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[all]) (1.4.1)\n",
            "Requirement already satisfied: imageio; extra == \"all\" in /usr/local/lib/python3.7/dist-packages (from gym[all]) (2.4.1)\n",
            "Requirement already satisfied: opencv-python; extra == \"all\" in /usr/local/lib/python3.7/dist-packages (from gym[all]) (4.1.2.30)\n",
            "Requirement already satisfied: Pillow; extra == \"all\" in /usr/local/lib/python3.7/dist-packages (from gym[all]) (7.1.2)\n",
            "Collecting box2d-py~=2.3.5; extra == \"all\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/34/da5393985c3ff9a76351df6127c275dcb5749ae0abbe8d5210f06d97405d/box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 5.4MB/s \n",
            "\u001b[?25hCollecting mujoco-py<2.0,>=1.50; extra == \"all\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/8c/64e0630b3d450244feef0688d90eab2448631e40ba6bdbd90a70b84898e7/mujoco-py-1.50.1.68.tar.gz (120kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: atari-py~=0.2.0; extra == \"all\" in /usr/local/lib/python3.7/dist-packages (from gym[all]) (0.2.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[all]) (0.16.0)\n",
            "Collecting glfw>=1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/d7/79c091c877493de7f8286ed62c77bf0f2c51105656073846b2326021b524/glfw-2.1.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (205kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.7/dist-packages (from mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (0.29.22)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.7/dist-packages (from mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (1.14.5)\n",
            "Collecting lockfile>=0.12.2\n",
            "  Downloading https://files.pythonhosted.org/packages/c8/22/9460e311f340cb62d26a38c419b1381b8593b0bb6b5d1f056938b086d362/lockfile-0.12.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0; extra == \"all\"->gym[all]) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.10->mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (2.20)\n",
            "Building wheels for collected packages: mujoco-py\n",
            "  Building wheel for mujoco-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for mujoco-py\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for mujoco-py\n",
            "Failed to build mujoco-py\n",
            "Installing collected packages: box2d-py, glfw, lockfile, mujoco-py\n",
            "    Running setup.py install for mujoco-py ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-6ca2re0r/mujoco-py/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-6ca2re0r/mujoco-py/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-bcc18lr6/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n",
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.7/dist-packages (2.3.8)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "Suggested packages:\n",
            "  libgle3\n",
            "The following NEW packages will be installed:\n",
            "  python-opengl\n",
            "0 upgraded, 1 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 496 kB of archives.\n",
            "After this operation, 5,416 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Fetched 496 kB in 2s (290 kB/s)\n",
            "Selecting previously unselected package python-opengl.\n",
            "(Reading database ... 160690 files and directories currently installed.)\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,270 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.9 [784 kB]\n",
            "Fetched 784 kB in 2s (422 kB/s)\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 163045 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.9_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQRen3PlNkiB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bda6ebc-fbfd-40c1-dea0-55d8d8a689dd"
      },
      "source": [
        "import numpy as np\n",
        "def stcat(x,support=15):\n",
        "  x = np.sign(x) * ((abs(x) + 1)**0.5 - 1) + 0.001 * x\n",
        "  x = np.clip(x, -support, support)\n",
        "  floor = np.floor(x)\n",
        "  prob = x - floor\n",
        "  logits = np.zeros( 2 * support + 1)\n",
        "  first_index = int(floor + support)\n",
        "  second_index = int(floor + support+1)\n",
        "  logits[first_index] = 1-prob\n",
        "  if prob>0:\n",
        "    logits[second_index] = prob\n",
        "  return logits\n",
        "#allow for batch processing  \n",
        "def catts(x,support=15):\n",
        "  support = np.arange(-support, support+1, 1)\n",
        "  if len(x.shape)==2:\n",
        "    #for  batch of x\\\n",
        "    x = np.sum(support*x,axis=1)\n",
        "  elif len(x.shape)==1:\n",
        "    #for single x\n",
        "    x = np.sum(support*x)  \n",
        "  else:\n",
        "    print(\"wrong input for conversion to  scalar\")  \n",
        "  x = np.sign(x) * ((((1 + 4 * 0.001 * (np.abs(x) + 1 + 0.001))**0.5 - 1) / (2 * 0.001))** 2- 1)\n",
        "  return x  \n",
        "\n",
        "#cat = stcat(5)#test 1 example\n",
        "cat = np.array([stcat(15),stcat(-15)]) # test batch example\n",
        "print(cat,cat.shape)\n",
        "scalar = catts(cat)\n",
        "print(scalar)\n",
        "print(\"done\")        \n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            "  0.    0.    0.    0.    0.    0.    0.985 0.015 0.    0.    0.    0.\n",
            "  0.    0.    0.    0.    0.    0.    0.   ]\n",
            " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.015\n",
            "  0.985 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            "  0.    0.    0.    0.    0.    0.    0.   ]] (2, 31)\n",
            "[ 15. -15.]\n",
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhf3dyDa2GYq",
        "outputId": "f199bb27-c5fa-47f4-97c3-9da3b1f673bc"
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MuZeroNet(nn.Module):\n",
        "    def __init__(self, input_size, action_space_n, reward_support_size, value_support_size):\n",
        "        super().__init__()\n",
        "        self.hx_size = 32\n",
        "        self._representation = nn.Sequential(nn.Linear(input_size, self.hx_size),\n",
        "                                             nn.Tanh())\n",
        "        self._dynamics_state = nn.Sequential(nn.Linear(self.hx_size + action_space_n, 64),\n",
        "                                             nn.Tanh(),\n",
        "                                             nn.Linear(64, self.hx_size),\n",
        "                                             nn.Tanh())\n",
        "        self._dynamics_reward = nn.Sequential(nn.Linear(self.hx_size + action_space_n, 64),\n",
        "                                              nn.LeakyReLU(),\n",
        "                                              nn.Linear(64, 2*reward_support_size+1))\n",
        "        self._prediction_actor = nn.Sequential(nn.Linear(self.hx_size, 64),\n",
        "                                               nn.LeakyReLU(),\n",
        "                                               nn.Linear(64, action_space_n))\n",
        "        self._prediction_value = nn.Sequential(nn.Linear(self.hx_size, 64),\n",
        "                                               nn.LeakyReLU(),\n",
        "                                               nn.Linear(64, 2*value_support_size+1))\n",
        "        self.action_space_n = action_space_n\n",
        "\n",
        "        self._prediction_value[-1].weight.data.fill_(0)\n",
        "        self._prediction_value[-1].bias.data.fill_(0)\n",
        "        self._dynamics_reward[-1].weight.data.fill_(0)\n",
        "        self._dynamics_reward[-1].bias.data.fill_(0)\n",
        "\n",
        "    def p(self, state):\n",
        "        actor = torch.softmax(self._prediction_actor(state),dim=1)\n",
        "        value = torch.softmax(self._prediction_value(state),dim=1)\n",
        "        return actor, value\n",
        "\n",
        "    def h(self, obs_history):\n",
        "        return self._representation(obs_history)\n",
        "\n",
        "    def g(self, state, action):\n",
        "        x = torch.cat((state, action), dim=1)\n",
        "        next_state = self._dynamics_state(x)\n",
        "        reward = torch.softmax(self._dynamics_reward(x),dim=1)\n",
        "        return next_state, reward     \n",
        "\n",
        "    def initial_state(self, x):\n",
        "        hout = self.h(x)\n",
        "        prob,v= self.p(hout)\n",
        "        return hout,prob,v\n",
        "    def next_state(self,hin,a):\n",
        "        hout,r = self.g(hin,a)\n",
        "        prob,v= self.p(hout)\n",
        "        return hout,r,prob,v\n",
        "    def inference_initial_state(self, x):\n",
        "        with torch.no_grad():\n",
        "          hout = self.h(x)\n",
        "          prob,v=self.p(hout)\n",
        "\n",
        "          return hout,prob,v\n",
        "    def inference_next_state(self,hin,a):\n",
        "        with torch.no_grad():\n",
        "          hout,r = self.g(hin,a)\n",
        "          prob,v=self.p(hout)\n",
        "          return hout,r,prob,v     \n",
        "\n",
        "\n",
        "print(\"done\")                                      "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkKMTKa6wYtR"
      },
      "source": [
        "\n",
        "#MTCS    MUzero modified for intermeditate rewards settings and using predicted rewards\n",
        "#accepts policy as a list\n",
        "import torch\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "def dynamics(net,state,action):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    #print(state,action) \n",
        "    next_state,reward,prob,value = net.inference_next_state(state.to(device),torch.tensor([action]).float().to(device))\n",
        "    reward = catts(reward.cpu().numpy().ravel())\n",
        "    value = catts(value.cpu().numpy().ravel())\n",
        "    prob = prob.cpu().tolist()[0]\n",
        "    #print(\"dynamics\",prob)\n",
        "    return next_state.cpu(),reward,prob,value\n",
        "\n",
        "\n",
        "class MinMaxStats:\n",
        "    \"\"\"A class that holds the min-max values of the tree.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.MAXIMUM_FLOAT_VALUE = float('inf')       \n",
        "        self.maximum =  -self.MAXIMUM_FLOAT_VALUE\n",
        "        self.minimum =  self.MAXIMUM_FLOAT_VALUE\n",
        "\n",
        "    def update(self, value: float):\n",
        "        if value is None:\n",
        "            raise ValueError\n",
        "\n",
        "        self.maximum = max(self.maximum, value)\n",
        "        self.minimum = min(self.minimum, value)\n",
        "\n",
        "    def normalize(self, value: float) -> float:\n",
        "        # If the value is unknow, by default we set it to the minimum possible value\n",
        "        if value is None:\n",
        "            return 0.0\n",
        "\n",
        "        if self.maximum > self.minimum:\n",
        "            # We normalize only when we have set the maximum and minimum values.\n",
        "            return (value - self.minimum) / (self.maximum - self.minimum)\n",
        "        return value\n",
        "\n",
        "\n",
        "class Node:\n",
        "    \"\"\"A class that represent nodes inside the MCTS tree\"\"\"\n",
        "\n",
        "    def __init__(self, prior: float):\n",
        "        self.visit_count = 0\n",
        "        self.to_play = -1\n",
        "        self.prior = prior\n",
        "        self.value_sum = 0\n",
        "        self.children = {}\n",
        "        self.hidden_state = None\n",
        "        self.reward = 0\n",
        "\n",
        "    def expanded(self):\n",
        "        return len(self.children) > 0\n",
        "\n",
        "    def value(self):\n",
        "        if self.visit_count == 0:\n",
        "            return None\n",
        "        return self.value_sum / self.visit_count\n",
        "\n",
        "\n",
        "def softmax_sample(visit_counts, actions, t):\n",
        "    counts_exp = np.exp(visit_counts) * (1 / t)\n",
        "    probs = counts_exp / np.sum(counts_exp, axis=0)\n",
        "    action_idx = np.random.choice(len(actions), p=probs)\n",
        "    return actions[action_idx]\n",
        "\n",
        "\n",
        "\"\"\"MCTS module: where MuZero thinks inside the tree.\"\"\"\n",
        "\n",
        "\n",
        "def add_exploration_noise( node):\n",
        "    \"\"\"\n",
        "    At the start of each search, we add dirichlet noise to the prior of the root\n",
        "    to encourage the search to explore new actions.\n",
        "    \"\"\"\n",
        "    actions = list(node.children.keys())\n",
        "    noise = np.random.dirichlet([0.25] * len(actions)) # config.root_dirichlet_alpha\n",
        "    frac = 0.25#config.root_exploration_fraction\n",
        "    for a, n in zip(actions, noise):\n",
        "        node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac\n",
        "\n",
        "\n",
        "\n",
        "def ucb_score(parent, child,min_max_stats):\n",
        "    \"\"\"\n",
        "    The score for a node is based on its value, plus an exploration bonus based on\n",
        "    the prior.\n",
        "\n",
        "    \"\"\"\n",
        "    pb_c_base = 19652\n",
        "    pb_c_init = 1.25\n",
        "    pb_c = math.log((parent.visit_count + pb_c_base + 1) / pb_c_base) + pb_c_init\n",
        "    pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
        "\n",
        "    prior_score = pb_c * child.prior\n",
        "    value_score = min_max_stats.normalize(child.value())\n",
        "    return  value_score + prior_score \n",
        "\n",
        "def select_child(node, min_max_stats):\n",
        "    \"\"\"\n",
        "    Select the child with the highest UCB score.\n",
        "    \"\"\"\n",
        "    # When the parent visit count is zero, all ucb scores are zeros, therefore we return a random child\n",
        "    if node.visit_count == 0:\n",
        "        return random.sample(node.children.items(), 1)[0]\n",
        "\n",
        "    _, action, child = max(\n",
        "        (ucb_score(node, child, min_max_stats), action,\n",
        "         child) for action, child in node.children.items())\n",
        "    return action, child\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def expand_node(node, to_play, actions_space,hidden_state,reward,policy):\n",
        "    \"\"\"\n",
        "    We expand a node using the value, reward and policy prediction obtained from\n",
        "    the neural networks.\n",
        "    \"\"\"\n",
        "    node.to_play = to_play\n",
        "    node.hidden_state = hidden_state\n",
        "    node.reward = reward\n",
        "    policy = {a:policy[a] for a in actions_space}\n",
        "    policy_sum = sum(policy.values())\n",
        "    for action, p in policy.items():\n",
        "        node.children[action] = Node(p / policy_sum) # not needed since mine are already softmax but its fine \n",
        "\n",
        "\n",
        "def backpropagate(search_path, value,to_play,discount, min_max_stats):\n",
        "    \"\"\"\n",
        "    At the end of a simulation, we propagate the evaluation all the way up the\n",
        "    tree to the root.\n",
        "    \"\"\"\n",
        "    for node in search_path[::-1]: #[::-1] means reversed\n",
        "        node.value_sum += value \n",
        "        node.visit_count += 1\n",
        "        min_max_stats.update(node.value())\n",
        "\n",
        "        value = node.reward + discount * value\n",
        "\n",
        "\n",
        "def select_action(node, mode ='softmax'):\n",
        "    \"\"\"\n",
        "    After running simulations inside in MCTS, we select an action based on the root's children visit counts.\n",
        "    During training we use a softmax sample for exploration.\n",
        "    During evaluation we select the most visited child.\n",
        "    \"\"\"\n",
        "    visit_counts = [child.visit_count for child in node.children.values()]\n",
        "    actions = [action for action in node.children.keys()]\n",
        "    action = None\n",
        "    if mode == 'softmax':\n",
        "        t = 1.0\n",
        "        action = softmax_sample(visit_counts, actions, t)\n",
        "    elif mode == 'max':\n",
        "        action, _ = max(node.children.items(), key=lambda item: item[1].visit_count)\n",
        "    counts_exp = np.exp(visit_counts)\n",
        "    probs = counts_exp / np.sum(counts_exp, axis=0)    \n",
        "    #return action ,probs,node.value()\n",
        "    return action ,np.array(visit_counts)/sum(visit_counts),node.value()\n",
        "\n",
        "def run_mcts(net, state,prob,root_value,num_simulations,discount = 0.9):\n",
        "    \"\"\"\n",
        "    Core Monte Carlo Tree Search algorithm.\n",
        "    To decide on an action, we run N simulations, always starting at the root of\n",
        "    the search tree and traversing the tree according to the UCB formula until we\n",
        "    reach a leaf node.\n",
        "    \"\"\"\n",
        "    prob, root_value = prob.tolist()[0] ,catts(root_value.numpy().ravel())\n",
        "    to_play = True\n",
        "    action_space=[ i for i in range(len(prob))]#history.action_space()\n",
        "    #print(\"action space\",action_space)\n",
        "    root = Node(0)\n",
        "    expand_node(root, to_play,action_space,state,0.0,prob)#node, to_play, actions_space ,hidden_state,reward,policy\n",
        "    add_exploration_noise( root)\n",
        "\n",
        "\n",
        "    min_max_stats = MinMaxStats()\n",
        "\n",
        "    for _ in range(num_simulations): \n",
        "        node = root\n",
        "        search_path = [node]\n",
        "\n",
        "        while node.expanded():\n",
        "            action, node = select_child( node, min_max_stats)\n",
        "            search_path.append(node)\n",
        "\n",
        "        # Inside the search tree we use the dynamics function to obtain the next\n",
        "        # hidden state given an action and the previous hidden state.\n",
        "        parent = search_path[-2]\n",
        "        \n",
        "        #network_output = network.recurrent_inference(parent.hidden_state, action)\n",
        "        next_state,r,action_probs, value = dynamics(net,parent.hidden_state,onehot(action,len(action_space))) \n",
        "        expand_node(node, to_play, action_space,next_state,r,action_probs)#node, to_play, actions_space ,hidden_state,reward,policy\n",
        "\n",
        "        backpropagate(search_path, value, to_play, discount, min_max_stats)#search_path, value,,discount, min_max_stats\n",
        "    return root    \n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-NpN4lU12kW"
      },
      "source": [
        "import gym\n",
        "class ScalingObservationWrapper(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    Wrapper that apply a min-max scaling of observations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env, low=None, high=None):\n",
        "        super().__init__(env)\n",
        "        assert isinstance(env.observation_space, gym.spaces.Box)\n",
        "\n",
        "        low = np.array(self.observation_space.low if low is None else low)\n",
        "        high = np.array(self.observation_space.high if high is None else high)\n",
        "\n",
        "        self.mean = (high + low) / 2\n",
        "        self.max = high - self.mean\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return (observation - self.mean) / self.max"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waeVGfWytBB1"
      },
      "source": [
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "def onehot(a,n=2):\n",
        "  return np.eye(n)[a]\n",
        "def play_game(env,net,targetnet,n_sim,discount,render,device,n_act,max_steps,td_steps,per):\n",
        "    trajectory=[]\n",
        "    root_values,pred_values,rewards=[],[],[]\n",
        "    state = env.reset() \n",
        "    done = False\n",
        "    r =0 \n",
        "    stp=0\n",
        "    while not done:\n",
        "        if render:\n",
        "          env.render()\n",
        "        stp+=1  \n",
        "        h ,prob,pred_value= net.inference_initial_state(torch.tensor([state]).float().to(device)) \n",
        "        root  = run_mcts(net,h.cpu(),prob.cpu(),pred_value.cpu(),num_simulations=n_sim,discount=discount)\n",
        "        action,action_prob,mcts_val = select_action(root) \n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        r+=reward\n",
        "        if stp>max_steps:\n",
        "          done = True\n",
        "        data = [state,onehot(action,n_act),action_prob,mcts_val,reward,1]#state,onehotaction,action_prob,mcts_val,reward,priority\n",
        "        if targetnet is None:\n",
        "          root_values.append(mcts_val)\n",
        "        else:\n",
        "          ht ,tarprob,target_pred_value= targetnet.inference_initial_state(torch.tensor([state]).float().to(device))\n",
        "          root_values.append(catts(target_pred_value.cpu().numpy().ravel()))\n",
        "        pred_values.append(catts(pred_value.cpu().numpy().ravel()))\n",
        "        rewards.append(reward)\n",
        "        trajectory.append(data)\n",
        "        state = next_state\n",
        "    #calculating priority as z - pred value\n",
        "    if per:  \n",
        "      priorities =get_initial_priorities(root_values,pred_values,rewards,discount=discount, td_steps=td_steps)\n",
        "      #update trajectory priority\n",
        "      assert len(trajectory) == len(priorities)\n",
        "      for i in range(len(trajectory)):\n",
        "        trajectory[i][5]=priorities[i]\n",
        "    print(\"DATA collection:played for \",len(trajectory),\" steps , rewards\",r,\" last state \",state)   \n",
        "    return trajectory    \n",
        "def get_initial_priorities(root_values,pred_values,rewards,discount=0.99, td_steps=10):\n",
        "    z_values = []\n",
        "    alpha = 1\n",
        "    beta = 1 \n",
        "    for current_index in range(len(root_values)):\n",
        "        bootstrap_index = current_index + td_steps\n",
        "        if bootstrap_index < len(root_values):\n",
        "            value = root_values[bootstrap_index] * discount ** td_steps\n",
        "        else:\n",
        "            value = 0\n",
        "\n",
        "        for i, reward in enumerate(rewards[current_index:bootstrap_index]):\n",
        "            value += reward * discount ** i\n",
        "\n",
        "        if current_index < len(root_values):\n",
        "            z_values.append(value)\n",
        "    #print(\"get priorities\",pred_values,z_values)        \n",
        "    p = np.abs(np.array(pred_values)-np.array(z_values))**alpha  + 0.00001\n",
        "    #priority = p /np.sum(p)\n",
        "    #N= len(pred_values) \n",
        "    #weights = (1/(N*priority))**beta\n",
        "    return list(p)#,list(weights)\n",
        "def eval_game(env,net,n_sim,render,device,max_steps):\n",
        "    state = env.reset() \n",
        "    done = False\n",
        "    r = 0\n",
        "    stp=0\n",
        "    while not done:\n",
        "        if render:\n",
        "          env.render()\n",
        "        stp+=1  \n",
        "        h ,prob,value= net.inference_initial_state(torch.tensor([state]).float().to(device)) \n",
        "        root  = run_mcts(net,h.cpu(),prob.cpu(),value.cpu(),num_simulations=n_sim,discount=discount)\n",
        "        action,action_prob,mcts_val = select_action(root,\"max\")\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        if stp>max_steps:\n",
        "          done = True\n",
        "        r+=reward\n",
        "        state = next_state\n",
        "    print(\"Eval:played for \",r ,\" rewards\",\" last state \",state)   \n",
        "    \n",
        "def sample_games(buffer,batch_size):\n",
        "    # Sample game from buffer either uniformly or according to some priority\n",
        "    #print(\"samplig from .\",len(buffer))\n",
        "    return list(np.random.choice(len(buffer),batch_size))\n",
        "\n",
        "def sample_position(trajectory,priority=None):\n",
        "    # Sample position from game either uniformly or according to some priority.\n",
        "    if priority == None:\n",
        "      return np.random.choice(len(trajectory),1)[0]\n",
        "    return np.random.choice(len(trajectory),1,p = priority)[0]\n",
        "    #return np.random.choice(list(range(0, len(trajectory))),1,p = priority)[0]\n",
        "\n",
        "def update_priorites(buffer,indexes,new_priority):\n",
        "    #buffer is a list and is passed as refernce so changes made here will reflect in buffer\n",
        "    for i in range(len(indexes)):\n",
        "      x,y = indexes[i]\n",
        "      #old_state,old_onehot_action,old_action_prob,old_mcts_val,old_reward,old_pred_value = buffer[x][y]\n",
        "      #buffer[x][y]=(old_state,old_onehot_action,old_action_prob,old_mcts_val,old_reward,new_pred_values[i])\n",
        "      buffer[x][y][5]=new_priority[i]\n",
        "\n",
        "\n",
        "def sample_batch(model,targetmodel,action_space_size,buffer,discount,batch_size,num_unroll_steps, td_steps,n_sim,per):\n",
        "    obs_batch, action_batch, reward_batch, value_batch, policy_batch,weights_batch = [], [], [], [], [],[]\n",
        "    indexes=[]\n",
        "    game_idx = sample_games(buffer,batch_size)\n",
        "    for gi in game_idx:\n",
        "      g = buffer[gi]\n",
        "      state,action,action_prob,root_val,reward,priority = zip(*g)\n",
        "      state,action,action_prob,root_val,reward,priority  =list(state),list(action),list(action_prob),list(root_val),list(reward),list(priority)\n",
        "      #print(\"pred val sample batch\",priority)\n",
        "      if per:\n",
        "        #make priority for sampling from root_value and n_step value\n",
        "        ps  = np.array(priority)/np.sum(np.array(priority))\n",
        "        game_pos = sample_position(g,list(ps))#state index sampled using priority\n",
        "        beta =1 \n",
        "        N = len(g)\n",
        "        weight =(1/(N*ps[game_pos]))**beta\n",
        "        #N= len(pred_values) \n",
        "        #weights = (1/(N*priority))**beta\n",
        "      else:  \n",
        "        weight = 1.0\n",
        "        game_pos = sample_position(g)#state index sampled using priority\n",
        "      _actions = action[game_pos:game_pos + num_unroll_steps]\n",
        "      # random action selection to complete num_unroll_steps\n",
        "      _actions += [onehot(np.random.randint(0, action_space_size),action_space_size)for _ in range(num_unroll_steps - len(_actions))]\n",
        "\n",
        "      obs_batch.append(state[game_pos])\n",
        "      action_batch.append(_actions)\n",
        "      value, reward, policy = make_target(model=model,target_model=targetmodel,buffer=buffer,gameindex=gi,states_trajectory=state,child_visits=action_prob ,root_values=root_val,\n",
        "                                          rewards=reward,state_index=game_pos,discount=discount, num_unroll_steps=num_unroll_steps, td_steps=td_steps,n_sim=n_sim)\n",
        "      reward_batch.append(reward)\n",
        "      value_batch.append(value)\n",
        "      policy_batch.append(policy)\n",
        "      weights_batch.append(weight)\n",
        "      indexes.append((gi,game_pos))\n",
        "\n",
        "\n",
        "\n",
        "    obs_batch = torch.tensor(obs_batch).float()\n",
        "    action_batch = torch.tensor(action_batch).long()\n",
        "    reward_batch = torch.tensor(reward_batch).float()\n",
        "    value_batch = torch.tensor(value_batch).float()\n",
        "    policy_batch = torch.tensor(policy_batch).float()\n",
        "    weights_batch = torch.tensor(weights_batch).float()\n",
        "    return obs_batch, action_batch, reward_batch, value_batch, policy_batch,weights_batch,indexes\n",
        "\n",
        "\n",
        "def make_target(model,target_model,buffer,gameindex,states_trajectory,child_visits,root_values,rewards,state_index,discount=0.99, num_unroll_steps=5, td_steps=5,n_sim=50):\n",
        "        # The value target is the discounted root value of the search tree or value by target network N steps into the future, plus\n",
        "        # the discounted sum of all rewards until then.\n",
        "        target_values, target_rewards, target_policies = [], [], []\n",
        "        for current_index in range(state_index, state_index + num_unroll_steps + 1):\n",
        "            bootstrap_index = current_index + td_steps\n",
        "            if bootstrap_index < len(root_values):\n",
        "                if target_model is None:\n",
        "                    value = root_values[bootstrap_index] * discount ** td_steps\n",
        "                else:\n",
        "                    #  a target network  based on recent parameters is used to provide a fresher,\n",
        "                    # stable n-step bootstrapped target for the value function\n",
        "                    obs = states_trajectory[bootstrap_index]\n",
        "                    ht ,tarprob,target_pred_value= target_model.inference_initial_state(torch.tensor([obs]).float().to(device))\n",
        "                    #ht ,tarprob,target_pred_value= model.inference_initial_state(torch.tensor([obs]).float().to(device))#try recent model(same as target update freq as 1 and hard update ) ######################\n",
        "                    value=catts(target_pred_value.cpu().numpy().ravel()) * discount ** td_steps\n",
        "            else:\n",
        "                value = 0\n",
        "\n",
        "            for i, reward in enumerate(rewards[current_index:bootstrap_index]):\n",
        "                value += reward * discount ** i\n",
        "\n",
        "            if current_index < len(root_values):\n",
        "                target_values.append(stcat(value))\n",
        "                target_rewards.append(stcat(rewards[current_index]))\n",
        "                if target_model is not None and np.random.random() <= 0.8: \n",
        "                    #we recompute policy for current_index using latest params model  \n",
        "                    #and then change it in buffer also use it as target policy 80 percent of time to keep labels stable\n",
        "                    obs = states_trajectory[current_index]\n",
        "                    h ,prob,pred_value= model.inference_initial_state(torch.tensor([obs]).float().to(device)) ##############################################\n",
        "                    root  = run_mcts(model,h.cpu(),prob.cpu(),pred_value.cpu(),num_simulations=n_sim,discount=discount)\n",
        "                    action,action_prob,mcts_val = select_action(root) \n",
        "                    buffer[gameindex][current_index][2]=action_prob #only change this rest values depend on the original trajectory\n",
        "                    child_visits[current_index] =action_prob\n",
        "\n",
        "                target_policies.append(child_visits[current_index])\n",
        "\n",
        "            else:\n",
        "                # States past the end of games are treated as absorbing states.\n",
        "                target_values.append(stcat(0))\n",
        "                target_rewards.append(stcat(0))\n",
        "                # Note: Target policy is  set to 0 so that no policy loss is calculated for them\n",
        "                #target_policies.append([0 for _ in range(len(child_visits[0]))])\n",
        "                target_policies.append(child_visits[0]*0.0)\n",
        "\n",
        "        return target_values, target_rewards, target_policies\n",
        "\n",
        "\n",
        "def scalar_reward_loss( prediction, target):\n",
        "        return -(torch.log(prediction) * target).sum(1)\n",
        "\n",
        "def scalar_value_loss( prediction, target):\n",
        "        return -(torch.log(prediction) * target).sum(1)\n",
        "def update_weights(model,targetmodel,action_space_size, optimizer, replay_buffer,discount,batch_size,num_unroll_steps, td_steps,n_sim,per ):\n",
        "    batch = sample_batch(model,targetmodel,action_space_size,replay_buffer,discount,batch_size,num_unroll_steps, td_steps,n_sim,per)\n",
        "    obs_batch, action_batch, target_reward, target_value, target_policy,target_weights,indexes = batch\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    obs_batch = obs_batch.to(device)\n",
        "    action_batch = action_batch.to(device) \n",
        "    target_reward = target_reward.to(device)\n",
        "    target_value = target_value.to(device)\n",
        "    target_policy = target_policy.to(device)\n",
        "    target_weights = target_weights.to(device)\n",
        "    target_reward_phi =target_reward \n",
        "    target_value_phi = target_value\n",
        "\n",
        "    hidden_state, policy_prob,value  = model.initial_state(obs_batch) # initial model_call #\n",
        "    \n",
        "    value_loss = scalar_value_loss(value, target_value_phi[:, 0])\n",
        "    policy_loss = -(torch.log(policy_prob) * target_policy[:, 0]).sum(1)\n",
        "    reward_loss = torch.zeros(batch_size, device=device)\n",
        "    initial_state_values = value.detach()\n",
        "    gradient_scale = 1 / num_unroll_steps\n",
        "    for step_i in range(num_unroll_steps):\n",
        "        hidden_state, reward,policy_prob,value  = model.next_state(hidden_state, action_batch[:, step_i]) \n",
        "        #h,pred_reward,pred_policy,pred_value= net.next_state(h,act)\n",
        "        policy_loss += -(torch.log(policy_prob) * target_policy[:, step_i + 1]).sum(1)\n",
        "        value_loss += scalar_value_loss(value, target_value_phi[:, step_i + 1])\n",
        "        reward_loss += scalar_reward_loss(reward, target_reward_phi[:, step_i])\n",
        "        hidden_state.register_hook(lambda grad: grad * 0.5)\n",
        "\n",
        "    # optimize\n",
        "    if targetmodel is None:\n",
        "      value_loss_coeff = 1\n",
        "    else:  \n",
        "      value_loss_coeff = 1 #to reduce value overfiiting due to off policy \n",
        "    loss = (policy_loss + value_loss_coeff * value_loss + reward_loss) # find value loss coefficiet = 1?\n",
        "    weights = target_weights#/target_weights.max()#dividing by max doesnt work\n",
        "    weighted_loss = (weights * loss).mean()#1?\n",
        "    weighted_loss.register_hook(lambda grad: grad * gradient_scale)\n",
        "    loss = loss.mean()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    weighted_loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "    optimizer.step()\n",
        "    if per:\n",
        "      #remvoing 2nd forward pass can do it also should be chill???\n",
        "      #updated_h,updated_prob,updated_pred_value= model.inference_initial_state(obs_batch) \n",
        "      #return indexes,updated_pred_value.cpu().numpy()\n",
        "      return indexes,np.abs(catts(initial_state_values.cpu().numpy())-catts(target_value[:, 0].cpu().numpy())) +0.00001\n",
        "    return None,None  \n",
        "\n",
        "def adjust_lr(optimizer, step_count):\n",
        "\n",
        "    lr_init=0.05\n",
        "    lr_decay_rate=0.01\n",
        "    lr_decay_steps=10000\n",
        "    lr = lr_init * lr_decay_rate ** (step_count / lr_decay_steps)\n",
        "    lr = max(lr, 0.001)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    return lr\n",
        "def soft_update(target, source, tau):\n",
        "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
        "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
        "    return target\n",
        "def get_scalars(new_pred_values):\n",
        "    vals = []\n",
        "    for i in range(new_pred_values.shape[0]):\n",
        "      #print(new_pred_values[i,:].shape)\n",
        "      vals.append(catts(new_pred_values[i,:]))\n",
        "    return vals\n",
        "learning_rate = [0.05]   \n",
        "stp=[0]\n",
        "def net_train(net, targetnet, action_space_size, replay_buffer,discount,batch_size,num_unroll_steps, td_steps,training_steps=1000,target_update=50,tou=1,n_sim=50,per = False):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model =net\n",
        "    target_model = targetnet\n",
        "    #MuZeroNet(input_size=4, action_space_n=2, reward_support_size=5, value_support_size=5).to(device) #training fresh net\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate[0], momentum=0.9,weight_decay=1e-4)\n",
        "    #training_steps=training_steps=500#20000\n",
        "    # wait for replay buffer to be non-empty\n",
        "    while len(replay_buffer) == 0:\n",
        "        pass\n",
        "\n",
        "    for step_count in tqdm(range(training_steps)):\n",
        "        stp[0]+=1\n",
        "        learning_rate[0] = adjust_lr( optimizer, step_count)\n",
        "        indexes,new_priority = update_weights(model,target_model, action_space_size, optimizer, replay_buffer,discount,batch_size,num_unroll_steps, td_steps,n_sim,per)\n",
        "        if target_model is not None:\n",
        "          if stp[0] % target_update==0:\n",
        "            #print(\"softupdate \", tou)\n",
        "            soft_update(target=target_model, source=model, tau=tou)\n",
        "        if per:\n",
        "          #print(\"new pred val net train\",new_pred_values,new_pred_values.shape)\n",
        "          #new_pred_values = get_scalars(new_pred_values)\n",
        "          #print(\"new pred val net train\",new_pred_values)\n",
        "          update_priorites(replay_buffer,indexes,new_priority)\n",
        "\n",
        "    return model,target_model\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KZWra51wFVvb",
        "outputId": "f83fd88c-6e2d-469f-a4bf-e5ff63cdfb0e"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "render = False\n",
        "episodes_per_train=30\n",
        "episodes_per_eval =5\n",
        "buffer =[]\n",
        "#buffer = deque(maxlen = episodes_per_train)\n",
        "training_steps=50\n",
        "max_steps=5000\n",
        "n_sim= 25\n",
        "discount = 0.91\n",
        "target_update_frewq = 50 # C\n",
        "update_frac=1 # hard update tou\n",
        "batch_size = 126\n",
        "envs = ['CartPole-v1','MountainCar-v0','LunarLander-v2']\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"training for \",envs[2])\n",
        "env=gym.make(envs[2])\n",
        "#env=env.unwrapped\n",
        "#env = ScalingObservationWrapper(env, low=[-2.4, -2.0, -0.42, -3.5], high=[2.4, 2.0, 0.42, 3.5])\n",
        "\n",
        "s_dim =env.observation_space.shape[0]\n",
        "print(\"s_dim: \",s_dim)\n",
        "a_dim =env.action_space.n\n",
        "print(\"a_dim: \",a_dim)\n",
        "a_bound =1 #env.action_space.high[0]\n",
        "print(\"a_bound: \",a_bound)\n",
        "\n",
        "\n",
        "\n",
        "net = MuZeroNet(input_size=s_dim, action_space_n=a_dim, reward_support_size=15, value_support_size=15).to(device)\n",
        "targetnet = MuZeroNet(input_size=s_dim, action_space_n=a_dim, reward_support_size=15, value_support_size=15).to(device) #None for not using reanalyze\n",
        "targetnet = soft_update(target=targetnet, source=net, tau=1)#make them same\n",
        "for t in range(training_steps):\n",
        "  if t<0:\n",
        "    priority = True \n",
        "    tr_stp=20\n",
        "  else :\n",
        "    tr_stp=300\n",
        "    priority = True \n",
        "  if targetnet is None:\n",
        "    buffer =[] # onpolicy \n",
        "  for _ in range(episodes_per_train):\n",
        "    buffer.append(play_game(env,net,targetnet,n_sim,discount,render,device,a_dim,max_steps,td_steps=5,per=priority))\n",
        "  print(\"training from \",len(buffer),\" games\")  \n",
        "\n",
        "  print(\"training with \",\" priority \",priority,\" training_steps \",tr_stp,\" discount \",discount,\" batch_size \",batch_size)  \n",
        "  net,targetnet = net_train(net,targetnet,action_space_size=a_dim, replay_buffer=buffer,discount=discount,batch_size=batch_size,num_unroll_steps=5, \n",
        "                            td_steps=5,training_steps=tr_stp,target_update=target_update_frewq,tou=update_frac,n_sim=n_sim,per = priority)\n",
        "  if t>5:\n",
        "    for _ in range(episodes_per_eval):\n",
        "      eval_game(env,net,n_sim,render,device,max_steps)\n",
        "  \n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training for  LunarLander-v2\n",
            "s_dim:  8\n",
            "a_dim:  4\n",
            "a_bound:  1\n",
            "DATA collection:played for  69  steps , rewards -114.84143443540677  last state  [ 0.12896605 -0.0125572   0.11754847 -0.31585854  0.39918184 -3.7452137\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  82  steps , rewards -289.67487034712906  last state  [-0.4994634   0.11755386 -0.5022531  -0.33129975  2.7789237   3.718322\n",
            "  0.          0.        ]\n",
            "DATA collection:played for  107  steps , rewards -112.09486231747886  last state  [ 6.5890026e-01  1.1017886e-01 -1.0708249e-09 -5.1182113e-08\n",
            "  2.9482257e-01 -3.1085830e-07  1.0000000e+00  1.0000000e+00]\n",
            "DATA collection:played for  88  steps , rewards -351.736217241349  last state  [-1.0040182   0.27371943 -1.5600069  -1.4693817   1.4744068   0.24248576\n",
            "  0.          0.        ]\n",
            "DATA collection:played for  69  steps , rewards -160.88313828799986  last state  [ 0.2053998  -0.00943132  0.73269737 -0.3263235  -0.63152117  3.2012222\n",
            "  1.          0.        ]\n",
            "DATA collection:played for  78  steps , rewards -125.290077792277  last state  [-0.4539724   0.12623034 -0.35159475 -0.10269275  0.42147058 -2.2416523\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  72  steps , rewards -68.31841772081722  last state  [ 0.42155534 -0.1437987   0.4192377  -0.59568876 -0.05130456  4.456369\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  122  steps , rewards -353.86995539744123  last state  [-0.60153055  0.23430055 -0.84154016 -0.24699716  2.7554226   2.964317\n",
            "  0.          0.        ]\n",
            "DATA collection:played for  62  steps , rewards -89.44910066368985  last state  [-0.01714497 -0.02267707 -0.02846818 -0.63359135 -0.2481368   5.1884623\n",
            "  1.          0.        ]\n",
            "DATA collection:played for  139  steps , rewards -221.3005483973891  last state  [-0.30352807  0.0835496  -0.29648486 -0.1873653   2.364653    3.0726104\n",
            "  0.          0.        ]\n",
            "DATA collection:played for  126  steps , rewards -233.13810783129773  last state  [-0.26644117  0.01073485 -0.6648799   0.3716102   1.7395436   2.609262\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  105  steps , rewards -265.6961787132177  last state  [ 1.0116695   0.4603035   1.4513614  -1.1883415  -0.9656513  -0.42397556\n",
            "  0.          0.        ]\n",
            "DATA collection:played for  114  steps , rewards -95.49057308559456  last state  [-3.2586655e-01 -3.8852181e-02 -2.1725301e-01  3.0128893e-03\n",
            " -2.0569265e-02  6.8222243e-07  1.0000000e+00  1.0000000e+00]\n",
            "DATA collection:played for  92  steps , rewards -74.65648129375889  last state  [-0.3664627  -0.10382161 -0.09833644  0.05864628  0.1860326   0.36589292\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  133  steps , rewards -33.031766990031784  last state  [ 0.3710394  -0.10276014  0.6315768  -0.47106     0.0610096  -2.9175363\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  118  steps , rewards -120.12872389133575  last state  [ 0.04867916 -0.0041361  -0.03743295 -0.16674733  0.5805144  -2.7019439\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  90  steps , rewards -69.51115759923692  last state  [ 0.3871294  -0.12869628  0.16680798  0.05970733 -0.12816013 -0.15081522\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  100  steps , rewards -410.8726360389491  last state  [-0.7175782   0.15825127 -0.8292001   0.04001028  1.9028643  -0.4647976\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  103  steps , rewards -41.8471802191294  last state  [ 0.00494852 -0.04239406  0.04234621 -0.00404881  0.00201719 -0.2494367\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  71  steps , rewards -90.13654935658485  last state  [-0.5544545   0.08098605 -0.37874323 -0.4149758  -0.01460911 -4.309757\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  76  steps , rewards -120.1751677505341  last state  [-1.09872624e-01  2.27425102e-04 -7.93645233e-02 -5.82047738e-02\n",
            "  6.99730039e-01 -1.38084924e+00  0.00000000e+00  1.00000000e+00]\n",
            "DATA collection:played for  69  steps , rewards -130.652461102813  last state  [-0.44464478  0.04436346 -0.9535634  -0.77925473  0.3467932   4.833004\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  95  steps , rewards -88.19400538079091  last state  [ 0.28057203 -0.05665447  0.27512804 -0.79240644 -0.13395894  4.0924997\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  81  steps , rewards -381.72741012398006  last state  [-0.81033003  0.11593331 -0.99123794 -0.21767014  1.6663585  -4.2400465\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  63  steps , rewards -211.26004496992397  last state  [-0.5563545  -0.01338539 -1.2113836  -0.10348732  1.0970254   2.1507478\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  117  steps , rewards -275.61492771397  last state  [-0.48144192 -0.0742709  -0.21653597  0.12920846  1.6246296   0.8727423\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  86  steps , rewards -222.36506839324076  last state  [-0.65949637 -0.00351572 -1.3272465  -0.4562017   0.7952385  -2.930209\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  69  steps , rewards -71.00868937123127  last state  [ 7.5911328e-02 -4.2820279e-02  5.4495715e-02  9.4817437e-08\n",
            "  3.1429034e-04  3.1637256e-07  1.0000000e+00  1.0000000e+00]\n",
            "DATA collection:played for  82  steps , rewards -276.1386668956787  last state  [-0.51415026  0.12500039 -1.0706351   0.1867675   1.8653136   1.1273613\n",
            "  0.          1.        ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/300 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  65  steps , rewards -106.97069795660522  last state  [-0.16616717 -0.02477169 -0.496142   -0.9034894   0.2881449  -4.806237\n",
            "  0.          1.        ]\n",
            "training from  30  games\n",
            "training with   priority  True  training_steps  300  discount  0.91  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [30:27<00:00,  6.09s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  61  steps , rewards -519.8538513745793  last state  [ 0.23304233  0.01168135  0.15040538 -0.1860797  -3.4992073   2.6471608\n",
            "  0.          0.        ]\n",
            "DATA collection:played for  54  steps , rewards -360.39475172051783  last state  [-0.10859518  0.01455503  0.18269154 -0.17284465 -2.7384372  -4.0398717\n",
            "  0.          0.        ]\n",
            "DATA collection:played for  91  steps , rewards -357.88254188428584  last state  [-0.3368352   0.08121666  0.04313734 -0.11371399 -3.9241266   2.0346951\n",
            "  0.          0.        ]\n",
            "DATA collection:played for  56  steps , rewards -367.8254076087958  last state  [-0.25988704 -0.01955283 -0.62102175 -0.84436893 -2.8674262  -7.203303\n",
            "  0.          0.        ]\n",
            "DATA collection:played for  79  steps , rewards -775.9188867627892  last state  [ 0.7085072  -0.16792749  0.22747584 -0.89712125 -6.0858393   5.849751\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  75  steps , rewards -520.6103923014894  last state  [ 2.7401352e-02 -3.4342051e-04  3.8933095e-02 -2.6032457e-02\n",
            " -4.4051576e+00 -2.6161125e-01  0.0000000e+00  1.0000000e+00]\n",
            "DATA collection:played for  83  steps , rewards -708.8418935436893  last state  [-0.29866487 -0.08625912 -0.8816616  -1.0993152  -6.087674    5.1931725\n",
            "  1.          0.        ]\n",
            "DATA collection:played for  54  steps , rewards -358.5605417869602  last state  [-0.2511325   0.03326133  0.39699966 -0.06193006 -2.6038747  -1.22356\n",
            "  0.          0.        ]\n",
            "DATA collection:played for  60  steps , rewards -504.7727506169467  last state  [ 0.20697756  0.00618057  0.08169408 -0.33202282 -3.3467987   3.7898183\n",
            "  0.          0.        ]\n",
            "DATA collection:played for  66  steps , rewards -548.9796701018058  last state  [ 0.06404047  0.00965425  0.29365945 -0.27475637 -4.207893   -5.191574\n",
            "  0.          0.        ]\n",
            "DATA collection:played for  80  steps , rewards -706.5284782979065  last state  [ 0.20917082 -0.02118758 -0.39375746 -0.04805494 -6.0024905  -0.3458342\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  80  steps , rewards -793.0557110185734  last state  [ 0.66896325  0.20749477 -0.41199976 -0.20217557 -6.5607843   4.0467\n",
            "  1.          0.        ]\n",
            "DATA collection:played for  58  steps , rewards -424.73170248870963  last state  [-1.5148163e-01 -7.7694203e-03 -2.7740222e-01 -8.7557645e-08\n",
            " -3.1414344e+00  5.6992684e-07  0.0000000e+00  0.0000000e+00]\n",
            "DATA collection:played for  61  steps , rewards -515.1751333981124  last state  [ 0.5897032   0.23869695  0.5356685   0.06353261 -3.7793593  -1.632745\n",
            "  0.          0.        ]\n",
            "DATA collection:played for  85  steps , rewards -753.9805368883297  last state  [-0.4458189  -0.10880201 -0.48491257 -0.80156136 -6.5265      5.0772996\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  66  steps , rewards -529.6407589180537  last state  [-1.8915673e-01  2.8452969e-03 -8.2250822e-01 -7.7030152e-01\n",
            " -4.1798005e+00 -6.6598253e+00  0.0000000e+00  0.0000000e+00]\n",
            "DATA collection:played for  84  steps , rewards -775.5012360425114  last state  [-9.4228074e-02  9.2755316e-04 -2.3655002e-01 -3.0309681e-02\n",
            " -7.0052309e+00  6.6776961e-01  1.0000000e+00  0.0000000e+00]\n",
            "DATA collection:played for  56  steps , rewards -385.8982036454598  last state  [-0.18497714  0.01716121  0.8668954  -0.26830262 -2.8884902  -2.8682122\n",
            "  0.          0.        ]\n",
            "DATA collection:played for  68  steps , rewards -535.916722311255  last state  [-0.3052582  -0.02501542 -0.47925892  0.16849537 -4.4585066  -3.2209148\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  66  steps , rewards -551.8136751559416  last state  [ 0.22950229  0.01208018 -0.53367525  0.04741539 -4.2424593  -2.720323\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  52  steps , rewards -328.43288957573566  last state  [-1.2280722e-01  1.6519271e-02  6.6990860e-02  1.2167632e-03\n",
            " -2.3304720e+00 -2.0110926e-01  0.0000000e+00  0.0000000e+00]\n",
            "DATA collection:played for  71  steps , rewards -487.67539802535146  last state  [-0.35907584 -0.05436197 -1.3776597  -0.05857669 -4.024994    2.0555038\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  66  steps , rewards -555.0246941532027  last state  [ 0.08904447  0.01165396  0.35009095 -0.08065654 -4.172735   -2.9558449\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  84  steps , rewards -780.6595866361084  last state  [ 5.2529061e-01 -2.0759085e-01  1.5657526e+00 -1.5063532e-01\n",
            " -6.4261832e+00  5.9246986e-07  0.0000000e+00  1.0000000e+00]\n",
            "DATA collection:played for  49  steps , rewards -263.17799866055947  last state  [-0.28768063  0.06714328  0.5189649  -0.08230703 -2.3799653  -0.6706577\n",
            "  1.          0.        ]\n",
            "DATA collection:played for  81  steps , rewards -704.430121299697  last state  [-7.7268317e-02 -4.2852893e-02 -4.6814407e-07  2.0151460e-07\n",
            " -6.2826896e+00  2.1199364e-06  1.0000000e+00  1.0000000e+00]\n",
            "DATA collection:played for  93  steps , rewards -333.020391213066  last state  [ 0.04737587  0.00697354 -0.25578266 -0.19234857 -3.3926165   2.1191561\n",
            "  0.          0.        ]\n",
            "DATA collection:played for  81  steps , rewards -743.1415505124362  last state  [-0.11034336 -0.03030913 -0.27526146 -0.72073597 -6.430867    4.9482527\n",
            "  1.          0.        ]\n",
            "DATA collection:played for  59  steps , rewards -443.2752215255538  last state  [-0.21934143 -0.00986462 -0.541866   -0.70994174 -3.0897331   5.4886537\n",
            "  0.          0.        ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/300 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  74  steps , rewards -598.2174920484368  last state  [-8.9449979e-02  2.6555038e-03 -4.6797681e-01 -2.6977187e-01\n",
            " -4.9886079e+00  3.2110851e+00  0.0000000e+00  1.0000000e+00]\n",
            "training from  60  games\n",
            "training with   priority  True  training_steps  300  discount  0.91  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [34:46<00:00,  6.96s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  58  steps , rewards -97.12546754131458  last state  [-0.02961979 -0.03598908  0.0400053  -0.9650003   0.07961972 -5.9123445\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  66  steps , rewards -104.82444798115577  last state  [-0.41022807  0.10192138 -0.5647808  -0.07468276  0.12214163  2.2103734\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  86  steps , rewards -130.40652385977359  last state  [-0.19937763 -0.01306235  0.4310046  -0.70874906 -0.01577564 -6.7840614\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  101  steps , rewards -142.1575998747163  last state  [-0.5222703  -0.09634812  0.3645855  -0.70101935  0.03350504 -5.829814\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  85  steps , rewards -140.90462507256032  last state  [ 0.28088588 -0.00918273  0.097418   -0.7665646   0.2606732  -5.873688\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  116  steps , rewards -235.5111531612281  last state  [ 0.838299   -0.21269502  0.9135753   0.23235105  1.1729131  -3.149059\n",
            "  0.          0.        ]\n",
            "DATA collection:played for  86  steps , rewards -143.99422006361175  last state  [-0.09907951 -0.02007589  0.03451449 -0.7339804   0.29715925 -5.628409\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  99  steps , rewards -114.26942489600677  last state  [-0.37603545 -0.09193671 -0.11482696  0.00123672  0.11603715 -0.10397565\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  76  steps , rewards -119.055670657706  last state  [-0.509121   -0.06696281 -1.0546665  -1.0479065   0.27911893  5.2594004\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  74  steps , rewards -132.38582543339277  last state  [-0.12115078 -0.03267233 -0.06010785 -0.8462057   0.11571516 -5.723405\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  55  steps , rewards -97.84277445590668  last state  [-0.20111623 -0.00825011  0.44365495 -0.61675537  0.03751996 -5.1875353\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  56  steps , rewards -100.30430218182038  last state  [-1.7242393e-01 -4.2782512e-02 -2.3203111e-01  7.3917014e-05\n",
            " -6.9006527e-04 -3.3088759e-04  1.0000000e+00  1.0000000e+00]\n",
            "DATA collection:played for  81  steps , rewards -133.31238806130594  last state  [ 0.43849164  0.01364981  0.3353664  -0.5007281   0.45232007 -5.0074854\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  58  steps , rewards -107.3998290529052  last state  [-0.31105798  0.01999119 -0.02825857 -0.79706633  0.067421   -5.898576\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  73  steps , rewards -147.19938020808524  last state  [ 0.39339504  0.01792553  0.41958722 -0.53825927  0.36006352 -5.40468\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  75  steps , rewards -154.2759265422202  last state  [ 0.11331262 -0.02249525  0.32807112 -0.6294558   0.2369651  -5.709177\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  111  steps , rewards -116.28095434633451  last state  [-0.59020126 -0.12819378 -0.5120985   0.05766987  0.01705144 -0.34074098\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  74  steps , rewards -113.54654430179974  last state  [-0.02880831 -0.03147035 -0.11467856 -0.7762397   0.1321467  -5.3026853\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  62  steps , rewards -116.71017975159583  last state  [-0.07461624 -0.03506315  0.03484732 -0.9115295   0.08781016 -5.8498917\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  89  steps , rewards -110.70796630136368  last state  [-0.45756346  0.03188296 -0.3398366  -0.8523448  -0.01255926 -5.6571417\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  67  steps , rewards -136.6892095739151  last state  [ 0.12576361 -0.02913361  0.36525297 -0.68937004  0.12969613 -5.8340387\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  63  steps , rewards -117.76135969765824  last state  [-0.35225543 -0.10776686 -0.99915266 -1.1246754   0.14024113  5.447995\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  67  steps , rewards -114.841139890887  last state  [-0.24154115 -0.02922752 -0.08832672 -0.882678   -0.07481959 -6.0352106\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  72  steps , rewards -65.4608128259967  last state  [ 0.44180003 -0.08492935  0.6998031   0.07531492  0.41908833  0.7605028\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  76  steps , rewards -133.19311753502396  last state  [-0.18483058 -0.04151129 -0.5153181  -0.5571148   0.09919524 -4.984218\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  54  steps , rewards -102.5336359061279  last state  [-0.30213085 -0.00629657 -0.20778044 -0.8874141   0.06370813 -6.109794\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  68  steps , rewards -135.16099104767855  last state  [-0.08686809 -0.0280373   0.017694   -0.826061    0.1800615  -5.8119364\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  62  steps , rewards -119.85605734743277  last state  [-0.11979437 -0.03350443 -0.0387231  -0.92770183  0.108206   -6.0376863\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  103  steps , rewards -132.68142747881905  last state  [-3.2388324e-01 -7.6204412e-02 -5.3360522e-01 -6.5596804e-02\n",
            "  1.8265462e-01  1.6301519e-07  1.0000000e+00  1.0000000e+00]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/300 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  71  steps , rewards -148.01879591039062  last state  [-0.07205334 -0.02754574  0.01048988 -0.82805073  0.188374   -5.7579894\n",
            "  0.          1.        ]\n",
            "training from  90  games\n",
            "training with   priority  True  training_steps  300  discount  0.91  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [34:45<00:00,  6.95s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  80  steps , rewards -141.04504801549308  last state  [-0.40659952 -0.12070794 -1.2596848  -0.7929819  -0.02262457  4.869935\n",
            "  1.          0.        ]\n",
            "DATA collection:played for  93  steps , rewards -118.6107683001845  last state  [-3.9597878e-01 -5.0573722e-02  4.5730950e-08 -3.2427685e-08\n",
            " -1.3571738e-01 -1.4666310e-07  1.0000000e+00  1.0000000e+00]\n",
            "DATA collection:played for  53  steps , rewards -78.95685513119997  last state  [-3.3710860e-02 -4.2759802e-02 -6.2479934e-08 -3.1764642e-08\n",
            "  1.8693476e-04  2.8266555e-07  1.0000000e+00  1.0000000e+00]\n",
            "DATA collection:played for  88  steps , rewards -150.36607097623792  last state  [ 0.63547957  0.15834147  0.790199   -0.76220167  0.06787123 -6.2868276\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  67  steps , rewards -126.69626848399955  last state  [-1.36117935e-01 -4.27885205e-02 -1.41258895e-01 -1.18796805e-07\n",
            "  2.19658148e-04  6.51766072e-07  1.00000000e+00  1.00000000e+00]\n",
            "DATA collection:played for  62  steps , rewards -121.22337196514762  last state  [-0.27645993 -0.05081129 -1.1962969  -0.86251533 -0.07350092  4.8898783\n",
            "  1.          0.        ]\n",
            "DATA collection:played for  106  steps , rewards -148.86473197473907  last state  [-0.70080835 -0.24005416 -1.2899401  -0.8968573   0.09289972  5.43575\n",
            "  1.          0.        ]\n",
            "DATA collection:played for  90  steps , rewards -114.93744491197704  last state  [ 0.3592083   0.01758035 -0.52579045 -0.7320976   0.1710616   5.295045\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  87  steps , rewards -118.53665198278875  last state  [ 0.14740142 -0.04008524  0.1387743  -1.1188382  -0.03364774  6.0949087\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  81  steps , rewards -121.7914854226601  last state  [ 0.39262277 -0.03689442  0.35415202  0.06474321  0.02224767  0.31832945\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  62  steps , rewards -103.39730338606083  last state  [ 1.4298764e-01 -4.2779006e-02  3.3816185e-02  2.3106211e-06\n",
            "  3.0463876e-04  2.3976618e-05  1.0000000e+00  1.0000000e+00]\n",
            "DATA collection:played for  67  steps , rewards -109.59907134847809  last state  [-4.2278114e-01  9.9476561e-02 -1.8659135e-08 -5.4246820e-09\n",
            " -1.7209192e-01  7.6857603e-08  1.0000000e+00  1.0000000e+00]\n",
            "DATA collection:played for  72  steps , rewards -116.95747155902451  last state  [ 0.32560748 -0.0070508   0.04095547 -0.24390827  0.17920576 -1.2974877\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  72  steps , rewards -106.51392691993954  last state  [ 2.2444305e-01 -3.0325688e-02 -5.1233268e-08 -2.8383422e-08\n",
            "  1.5403120e-01  1.9471634e-07  1.0000000e+00  1.0000000e+00]\n",
            "DATA collection:played for  59  steps , rewards -105.34983886016299  last state  [ 1.7837200e-01 -4.2762019e-02  4.5474313e-02  6.4181886e-06\n",
            "  2.6180898e-04  2.0737149e-05  1.0000000e+00  1.0000000e+00]\n",
            "DATA collection:played for  57  steps , rewards -116.33566654735647  last state  [-0.11073007 -0.03356025 -0.30175784 -1.0542003  -0.1127736   6.338597\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  54  steps , rewards -98.3220140598401  last state  [-0.23661414 -0.03910062 -0.32359093  0.00557845 -0.04368462 -0.03057294\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  73  steps , rewards -134.55973782095336  last state  [-1.4532003e-01 -4.2690136e-02 -1.7289461e-01 -2.0652077e-08\n",
            "  7.9274719e-04 -1.6558916e-07  1.0000000e+00  1.0000000e+00]\n",
            "DATA collection:played for  85  steps , rewards -111.7819792470485  last state  [ 0.58770007  0.06739591  0.23110473 -0.06715791  0.15967432  2.08428\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  69  steps , rewards -105.82942269902438  last state  [-0.4921058   0.065456   -1.1734151  -0.7995294  -0.04620427  4.6545625\n",
            "  1.          0.        ]\n",
            "DATA collection:played for  98  steps , rewards -133.05127588247453  last state  [-0.3823692  -0.12430951 -1.0569503  -0.9206856   0.16022404  5.3545327\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  107  steps , rewards -138.8634100620691  last state  [-0.5582681  -0.12835985 -1.2028121  -1.0059141   0.1694237   6.688554\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  69  steps , rewards -132.35851386323208  last state  [-0.214576   -0.04084163 -0.43640384 -1.010076   -0.0153652   6.1373115\n",
            "  1.          0.        ]\n",
            "DATA collection:played for  92  steps , rewards -116.50977555226108  last state  [-2.6827854e-01 -3.4452427e-02 -9.7246712e-08 -5.7946643e-08\n",
            " -7.9579897e-02  4.0740036e-07  1.0000000e+00  1.0000000e+00]\n",
            "DATA collection:played for  91  steps , rewards -95.66320781749214  last state  [-8.0340005e-02 -4.2781439e-02 -5.4696131e-02  3.7178992e-08\n",
            "  1.8913635e-04 -4.8581580e-07  1.0000000e+00  1.0000000e+00]\n",
            "DATA collection:played for  96  steps , rewards -107.03874081873033  last state  [-1.2365065e-01 -4.2781580e-02 -4.6676394e-02 -3.7179223e-08\n",
            "  1.9264320e-04  1.4580971e-07  0.0000000e+00  1.0000000e+00]\n",
            "DATA collection:played for  92  steps , rewards -102.76769656206321  last state  [ 3.58515918e-01 -1.04923606e-01  4.89162773e-01  3.74831446e-02\n",
            " -2.78242528e-01 -7.03140511e-04  1.00000000e+00  0.00000000e+00]\n",
            "DATA collection:played for  86  steps , rewards -124.55701777531189  last state  [-0.20692316 -0.04380161 -0.5420739  -0.12210009  0.13937783  6.454009\n",
            "  1.          0.        ]\n",
            "DATA collection:played for  66  steps , rewards -104.18914783207657  last state  [ 0.2907707   0.01281639 -0.01614857 -0.87144     0.03839433  5.744025\n",
            "  1.          0.        ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/300 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  87  steps , rewards -96.65716546521811  last state  [ 1.31808564e-01 -4.27975319e-02  1.12522116e-07 -2.63992970e-08\n",
            "  2.59389344e-04 -5.09259621e-07  1.00000000e+00  1.00000000e+00]\n",
            "training from  120  games\n",
            "training with   priority  True  training_steps  300  discount  0.91  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [35:22<00:00,  7.07s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  81  steps , rewards -141.14787859665387  last state  [-0.26252228 -0.03837489 -0.26801798 -0.90241927  0.0567753  -6.093003\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  105  steps , rewards -142.05517785460657  last state  [ 0.34247446 -0.05614159  0.6938146  -0.7166797   0.0291006  -5.870151\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  71  steps , rewards -131.8029210912279  last state  [ 2.6427260e-01 -1.9181898e-02 -4.7868084e-02 -7.3626111e-03\n",
            "  2.2691157e-01 -4.9713998e-07  1.0000000e+00  1.0000000e+00]\n",
            "DATA collection:played for  89  steps , rewards -148.29347842008647  last state  [ 0.29579574 -0.01943882 -0.08283907 -0.24652845  0.1668322   1.2441609\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  93  steps , rewards -162.29175803802522  last state  [-0.623629   -0.27768266 -0.6298425  -0.07016357  0.03071163 -0.685039\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  57  steps , rewards -117.05310247106765  last state  [ 0.26524323 -0.03360986  0.46387786 -0.98735976  0.08246182 -6.246297\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  96  steps , rewards -132.86745652235032  last state  [-0.16290179 -0.03511794 -0.40747255 -1.1455767   0.14831212 -5.95791\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  93  steps , rewards -140.86068205046024  last state  [ 0.11675825 -0.02760539  0.25899914 -0.81548756  0.18077599 -6.053367\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  74  steps , rewards -135.1011318306695  last state  [-4.67464447e-01 -1.12744786e-01 -9.93215919e-01 -1.62624300e-01\n",
            "  2.41141319e-01 -7.07181300e-07  1.00000000e+00  1.00000000e+00]\n",
            "DATA collection:played for  55  steps , rewards -114.97087950431359  last state  [ 0.1029563  -0.02655758  0.309568   -0.9491232   0.20512058 -6.2688093\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  88  steps , rewards -139.76575201021998  last state  [ 0.6029722  -0.07824174  0.2960796  -0.02206469  0.1881916   0.5286075\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  85  steps , rewards -148.04858329225564  last state  [-0.46856362 -0.19565716 -0.562551   -0.937094    0.04077451 -6.1535993\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  70  steps , rewards -118.21960415505461  last state  [ 0.24597263 -0.0032912  -0.5597423  -0.43195668  0.4522166  -1.1745135\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  75  steps , rewards -153.93756261502187  last state  [ 0.10797672 -0.03104577  0.28153795 -0.82234     0.13046883 -5.952164\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  55  steps , rewards -90.30710589193944  last state  [-0.30241928  0.07425901  0.4954849  -0.507461    0.12751257 -4.1389346\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  95  steps , rewards -135.1416157088031  last state  [ 2.6576930e-01  1.8204427e-03 -7.4106640e-01 -2.0232950e-01\n",
            "  3.8893080e-01 -8.6130575e-07  1.0000000e+00  1.0000000e+00]\n",
            "DATA collection:played for  92  steps , rewards -123.17065914490524  last state  [ 2.1536827e-01 -2.7394786e-02 -8.2625480e-09  8.3901623e-09\n",
            "  1.9286521e-01  2.4627372e-08  1.0000000e+00  1.0000000e+00]\n",
            "DATA collection:played for  60  steps , rewards -127.58391017555687  last state  [-0.1559637  -0.04244713 -0.21388765 -0.18607874 -0.00464428  0.99487936\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  60  steps , rewards -98.41546169760238  last state  [ 0.1121376  -0.03845436  0.25628045 -0.86580807  0.02940884 -5.87827\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  88  steps , rewards -132.64350846011416  last state  [-0.32808885 -0.05261309 -0.31820083 -0.29966763  0.05535546  1.5325507\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  75  steps , rewards -148.14059291974576  last state  [ 0.3196582  -0.02866998  0.588237   -0.82768923  0.21022192 -6.043848\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  70  steps , rewards -115.58489999823878  last state  [ 2.9258490e-01  3.3708584e-02 -6.7401755e-01 -2.2671224e-01\n",
            "  4.6878019e-01 -8.0020911e-08  1.0000000e+00  1.0000000e+00]\n",
            "DATA collection:played for  74  steps , rewards -125.54207410185313  last state  [ 0.08819743 -0.03287751  0.21522887 -0.8537124   0.11286193 -5.7673454\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  60  steps , rewards -110.07994382499247  last state  [-0.0544425  -0.0272129   0.03930062 -0.82477003  0.19172758 -5.850074\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  90  steps , rewards -137.70889598047995  last state  [-0.5858673   0.03010155 -1.1834614  -0.18158981  0.30011424  0.51116705\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  64  steps , rewards -128.67922227001543  last state  [-0.05362044 -0.04236198 -0.07363252 -0.14859703 -0.00456054  0.8252301\n",
            "  1.          0.        ]\n",
            "DATA collection:played for  88  steps , rewards -140.8029543871786  last state  [ 0.3954975  -0.08888654  1.1380041  -0.4897493   0.3018048  -3.7569633\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  77  steps , rewards -165.46383389038886  last state  [ 0.57841825 -0.13826387  1.1110563  -0.66904473  0.28857633 -5.5132937\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  65  steps , rewards -120.52716579443788  last state  [ 0.07003937 -0.03928692  0.22007558 -0.77240735  0.01696799 -5.597238\n",
            "  1.          0.        ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/300 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  71  steps , rewards -148.46183945984717  last state  [ 0.15199213 -0.0229318   0.3500145  -0.7659994   0.25316224 -5.7560205\n",
            "  0.          1.        ]\n",
            "training from  150  games\n",
            "training with   priority  True  training_steps  300  discount  0.91  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [35:40<00:00,  7.13s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  76  steps , rewards -122.53007487743417  last state  [ 1.4762068e-01 -3.8847532e-02 -9.2241240e-01 -7.5516647e-01\n",
            "  1.0046959e-03  4.5210257e+00  1.0000000e+00  1.0000000e+00]\n",
            "DATA collection:played for  69  steps , rewards -98.17673450804895  last state  [-0.25022602 -0.06176418 -0.5076493  -0.9257948   0.2725305   5.804909\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  76  steps , rewards -141.00095485080442  last state  [ 0.44979686 -0.18448208  0.33951667 -0.7957265  -0.04361035  5.9060183\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  84  steps , rewards -117.8502710423439  last state  [-5.0429571e-01 -1.3631964e-03 -1.0604885e+00 -1.0994316e+00\n",
            "  1.1643453e-01  5.4462409e+00  0.0000000e+00  0.0000000e+00]\n",
            "DATA collection:played for  59  steps , rewards -66.24320104599039  last state  [ 0.37823248 -0.08611521 -0.06198249  0.02673771 -0.0181218   0.10807852\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  84  steps , rewards -110.13036431707478  last state  [-0.20987806 -0.0109796   0.29134083 -0.86062366 -0.09843656 -5.1693225\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  55  steps , rewards -79.40372203952512  last state  [-1.6122770e-01 -4.2769101e-02 -6.2403977e-02  6.8729255e-06\n",
            " -3.0291115e-04 -4.4120847e-05  1.0000000e+00  1.0000000e+00]\n",
            "DATA collection:played for  65  steps , rewards -110.59715424318115  last state  [ 0.23404351 -0.05503609  0.8568395  -0.48331842 -0.28545853 -4.5660276\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  70  steps , rewards -121.98226092844939  last state  [-4.7954846e-02 -4.1947380e-02 -2.0985635e-01 -9.6981746e-01\n",
            " -5.5968538e-03  6.0429940e+00  1.0000000e+00  1.0000000e+00]\n",
            "DATA collection:played for  58  steps , rewards -64.80948873364622  last state  [-4.4653319e-02 -4.2547408e-02 -5.3456318e-02  0.0000000e+00\n",
            "  2.6621816e-03  4.8312131e-08  1.0000000e+00  1.0000000e+00]\n",
            "DATA collection:played for  72  steps , rewards -139.85961077393702  last state  [ 7.0123576e-02 -4.2771388e-02 -8.8492044e-08  1.5619571e-08\n",
            "  1.5196750e-04  4.0039208e-07  1.0000000e+00  1.0000000e+00]\n",
            "DATA collection:played for  94  steps , rewards -121.43004927131079  last state  [-5.6957889e-01 -6.5733746e-02 -3.9534646e-01  2.4895322e-02\n",
            " -9.4329342e-02 -4.3754774e-04  1.0000000e+00  1.0000000e+00]\n",
            "DATA collection:played for  73  steps , rewards -138.57406713468214  last state  [ 0.3493905  -0.12779549  0.9269164  -1.1774096  -0.19067492 -5.5400977\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  65  steps , rewards -81.70051360928932  last state  [-9.8261639e-02 -4.2827893e-02  4.9207301e-08  5.2798576e-08\n",
            "  3.5973234e-04  3.2737483e-07  1.0000000e+00  1.0000000e+00]\n",
            "DATA collection:played for  73  steps , rewards -116.94891302737778  last state  [ 0.2585867  -0.05951474  0.6395515  -1.0162601  -0.20054817 -5.2933283\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  99  steps , rewards -82.43676694152583  last state  [ 9.05011147e-02 -4.27954234e-02 -1.04877110e-07 -1.10437014e-07\n",
            "  2.55319552e-04  4.74647237e-07  0.00000000e+00  0.00000000e+00]\n",
            "DATA collection:played for  97  steps , rewards -107.31793776089143  last state  [ 1.5395288e-01 -4.3000165e-02  1.5895645e-01 -1.0376647e+00\n",
            " -9.9298358e-04  6.1021290e+00  1.0000000e+00  1.0000000e+00]\n",
            "DATA collection:played for  97  steps , rewards -139.82993696647029  last state  [ 0.7630819  -0.06772631  1.0611134  -1.1014801  -0.41994932 -4.191926\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  75  steps , rewards -127.39847455335908  last state  [ 0.56909764  0.03691271  0.82562417 -0.7011788   0.04742011  6.3328233\n",
            "  0.          0.        ]\n",
            "DATA collection:played for  68  steps , rewards -109.67871296063471  last state  [-0.29199928 -0.07285948 -1.037525   -1.0082152   0.09382286  5.0922995\n",
            "  1.          0.        ]\n",
            "DATA collection:played for  81  steps , rewards -92.13644560941742  last state  [-0.03346329 -0.0420414  -0.07388161 -0.02371889  0.00946094 -0.12660235\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  94  steps , rewards -116.15629646366872  last state  [ 0.23567447 -0.05528547  0.76179224 -0.6182067  -0.26649424 -5.516763\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  91  steps , rewards -137.57745689453836  last state  [ 0.50565755 -0.07662276  0.01849831 -0.6559287  -0.14290299  5.710967\n",
            "  1.          0.        ]\n",
            "DATA collection:played for  68  steps , rewards -105.85070077129741  last state  [-0.34452146  0.0187018  -0.12671441 -0.87023085 -0.05841268 -5.7236795\n",
            "  0.          1.        ]\n",
            "DATA collection:played for  95  steps , rewards -79.0615074917051  last state  [-0.18984899 -0.03471983  0.04421037 -0.00327976 -0.09800547  0.01887665\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  91  steps , rewards -111.73927892695633  last state  [-0.29430962 -0.03172551 -0.15937397 -1.137611   -0.01888148 -6.318521\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  57  steps , rewards -107.15043151961888  last state  [ 9.3638137e-02 -4.2783406e-02  1.2991049e-07  8.4037737e-08\n",
            "  1.9780236e-04 -5.8786634e-07  1.0000000e+00  1.0000000e+00]\n",
            "DATA collection:played for  80  steps , rewards -112.06676964956355  last state  [-0.19614029 -0.02211269  0.25467286 -0.91485846 -0.05792679 -5.57404\n",
            "  1.          1.        ]\n",
            "DATA collection:played for  59  steps , rewards -96.16955411507266  last state  [-1.3177443e-01 -4.2797852e-02 -2.1413043e-01  1.3683633e-07\n",
            "  2.6215817e-04 -9.6209010e-07  1.0000000e+00  1.0000000e+00]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/300 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  63  steps , rewards -100.01155443221933  last state  [-0.04041958 -0.04332285 -0.12392225 -0.10762497 -0.0065088  -0.572359\n",
            "  1.          1.        ]\n",
            "training from  180  games\n",
            "training with   priority  True  training_steps  300  discount  0.91  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 64%|██████▎   | 191/300 [22:55<13:31,  7.44s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-8553bfd24e0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training with \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" priority \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpriority\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" training_steps \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtr_stp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" discount \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" batch_size \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m   net,targetnet = net_train(net,targetnet,action_space_size=a_dim, replay_buffer=buffer,discount=discount,batch_size=batch_size,num_unroll_steps=5, \n\u001b[0;32m---> 51\u001b[0;31m                             td_steps=5,training_steps=tr_stp,target_update=target_update_frewq,tou=update_frac,n_sim=n_sim,per = priority)\n\u001b[0m\u001b[1;32m     52\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes_per_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-6c232c1ff9e8>\u001b[0m in \u001b[0;36mnet_train\u001b[0;34m(net, targetnet, action_space_size, replay_buffer, discount, batch_size, num_unroll_steps, td_steps, training_steps, target_update, tou, n_sim, per)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0mstp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mlearning_rate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madjust_lr\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mindexes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_priority\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_space_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_unroll_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtd_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_sim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarget_model\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mstp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtarget_update\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-6c232c1ff9e8>\u001b[0m in \u001b[0;36mupdate_weights\u001b[0;34m(model, targetmodel, action_space_size, optimizer, replay_buffer, discount, batch_size, num_unroll_steps, td_steps, n_sim, per)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargetmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction_space_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_unroll_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtd_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_sim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mper\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargetmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction_space_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_unroll_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtd_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_sim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m     \u001b[0mobs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-6c232c1ff9e8>\u001b[0m in \u001b[0;36msample_batch\u001b[0;34m(model, targetmodel, action_space_size, buffer, discount, batch_size, num_unroll_steps, td_steps, n_sim, per)\u001b[0m\n\u001b[1;32m    133\u001b[0m       \u001b[0maction_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m       value, reward, policy = make_target(model=model,target_model=targetmodel,buffer=buffer,gameindex=gi,states_trajectory=state,child_visits=action_prob ,root_values=root_val,\n\u001b[0;32m--> 135\u001b[0;31m                                           rewards=reward,state_index=game_pos,discount=discount, num_unroll_steps=num_unroll_steps, td_steps=td_steps,n_sim=n_sim)\n\u001b[0m\u001b[1;32m    136\u001b[0m       \u001b[0mreward_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m       \u001b[0mvalue_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-6c232c1ff9e8>\u001b[0m in \u001b[0;36mmake_target\u001b[0;34m(model, target_model, buffer, gameindex, states_trajectory, child_visits, root_values, rewards, state_index, discount, num_unroll_steps, td_steps, n_sim)\u001b[0m\n\u001b[1;32m    181\u001b[0m                     \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstates_trajectory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                     \u001b[0mh\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_value\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_initial_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m##############################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                     \u001b[0mroot\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mrun_mcts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_simulations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_sim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m                     \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmcts_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                     \u001b[0mbuffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgameindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_prob\u001b[0m \u001b[0;31m#only change this rest values depend on the original trajectory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-41f7141ac352>\u001b[0m in \u001b[0;36mrun_mcts\u001b[0;34m(net, state, prob, root_value, num_simulations, discount)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0msearch_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0;32mwhile\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_child\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_max_stats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0msearch_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}