{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mu_0 with prioritized experience replay 1.0 .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMruV5zIn9mHIsz7aqFCmN7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rootAkash/reinforcement_learning/blob/master/muzero/mu_0_with_prioritized_experience_replay_1_0_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIEXeLFudTXo"
      },
      "source": [
        "#why priority?\n",
        "#since the mcts root value is a better estimate of value of a state. The difference between the n step return value and the mcts value(or even the network predicted value ) tells somehow how much the value fuction has \n",
        "#coverged to the actual value (wrt to the mcts policy that we are following while generating trajectory).\n",
        "#policy network just apporximates the mcts to boot strap the value function.\n",
        "#so the difference between search value(mcts root) and the n step return value should be higher for sates which we have not trained properly or seen yet so we need to sample more of those\n",
        "#in order to get a better value function and lead it to quicker convergence in enviroments with intermediate rewards (eg ATARI)\n",
        "#so we sample from replay using the difference between root search value and  step value return as priority,it introduces a sampling bias. \n",
        "#sample bias: sampling bias is a bias in which a sample is collected in such a way that some members of the intended population have a lower or higher sampling probability than others.\n",
        "#intituively the samples having higher probablity will be sampled more leading to the model getting baised into optimising for these samples more.Equivalent of having bigger loss for higher\n",
        "#probablity samples since they are sampled more.\n",
        "#(also when uniform sampling the expectations of the evironment are preserved but when prioity is given it messes up the expectations and thus we need to correct expectations of transitions.)\n",
        "#to correct for this we use importance sampling (depends on probality value of sample) and scale down the loss for sample using it.  \n",
        "#also search v  changes as the model is trained so we might need to update the search v?????? will skip it here?? maybe try with value networks value output instead of search value because it\n",
        "#can be updated easily unless we need to recompute the new search value after every training step\n",
        "#as fast as possible so,even though the paper mentions abs difference between n step returns z and search value ...\n",
        "# the priority sampling is done to train network to reduce error between network predicted value and n step returns as fast as possible.\n",
        "#so it makes more sense to make priority as abs diff between n step return z and the network predicted value rather than search value. and also the priority can be updated easily after training step.\n",
        "#but if we use search value(instead of value net predicted value) then after evey batch training we have to recompute the search value for priority.\n",
        "#(tho search value  can better estimate the convergence of value network since it also takes values,policy and rewards predicted by net(bascially everything that is learnt) for future states too into account.\n",
        "#but since after evey batch reward,value and policy changes . it might make it a bit unstable will try in 2.0)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwI18Z6p1oGP",
        "outputId": "287f7b1f-f449-4cd6-9049-030f41afe12c"
      },
      "source": [
        "!pip install gym[all]\n",
        "!pip install box2d-py\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym[all] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[all]) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (1.19.5)\n",
            "Requirement already satisfied: Pillow; extra == \"all\" in /usr/local/lib/python3.7/dist-packages (from gym[all]) (7.1.2)\n",
            "Requirement already satisfied: box2d-py~=2.3.5; extra == \"all\" in /usr/local/lib/python3.7/dist-packages (from gym[all]) (2.3.8)\n",
            "Requirement already satisfied: opencv-python; extra == \"all\" in /usr/local/lib/python3.7/dist-packages (from gym[all]) (4.1.2.30)\n",
            "Collecting mujoco-py<2.0,>=1.50; extra == \"all\"\n",
            "  Using cached https://files.pythonhosted.org/packages/cf/8c/64e0630b3d450244feef0688d90eab2448631e40ba6bdbd90a70b84898e7/mujoco-py-1.50.1.68.tar.gz\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"all\" in /usr/local/lib/python3.7/dist-packages (from gym[all]) (0.2.6)\n",
            "Requirement already satisfied: imageio; extra == \"all\" in /usr/local/lib/python3.7/dist-packages (from gym[all]) (2.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[all]) (0.16.0)\n",
            "Requirement already satisfied: glfw>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (2.1.0)\n",
            "Requirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.7/dist-packages (from mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (0.29.22)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.7/dist-packages (from mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (1.14.5)\n",
            "Requirement already satisfied: lockfile>=0.12.2 in /usr/local/lib/python3.7/dist-packages (from mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (0.12.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0; extra == \"all\"->gym[all]) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.10->mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (2.20)\n",
            "Building wheels for collected packages: mujoco-py\n",
            "  Building wheel for mujoco-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for mujoco-py\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for mujoco-py\n",
            "Failed to build mujoco-py\n",
            "Installing collected packages: mujoco-py\n",
            "    Running setup.py install for mujoco-py ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-pvo7o57o/mujoco-py/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-pvo7o57o/mujoco-py/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-im8izzgd/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n",
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.7/dist-packages (2.3.8)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQRen3PlNkiB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53da5ba5-97f1-44aa-8cbc-9efb17885ea5"
      },
      "source": [
        "import numpy as np\n",
        "def stcat(x,support=5):\n",
        "  x = np.sign(x) * ((abs(x) + 1)**0.5 - 1) + 0.001 * x\n",
        "  x = np.clip(x, -support, support)\n",
        "  floor = np.floor(x)\n",
        "  prob = x - floor\n",
        "  logits = np.zeros( 2 * support + 1)\n",
        "  first_index = int(floor + support)\n",
        "  second_index = int(floor + support+1)\n",
        "  logits[first_index] = 1-prob\n",
        "  if prob>0:\n",
        "    logits[second_index] = prob\n",
        "  return logits\n",
        "def catts(x,support=5):\n",
        "  support = np.arange(-support, support+1, 1)\n",
        "  x = np.sum(support*x)\n",
        "  x = np.sign(x) * ((((1 + 4 * 0.001 * (abs(x) + 1 + 0.001))**0.5 - 1) / (2 * 0.001))** 2- 1)\n",
        "  return x  \n",
        "\n",
        "#cat = stcat(58705)\n",
        "#print(cat)\n",
        "#scalar = catts(cat)\n",
        "#print(scalar)\n",
        "print(\"done\")        \n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhf3dyDa2GYq",
        "outputId": "c72903d1-7156-4b0a-81d4-49034cdee777"
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MuZeroNet(nn.Module):\n",
        "    def __init__(self, input_size, action_space_n, reward_support_size, value_support_size):\n",
        "        super().__init__()\n",
        "        self.hx_size = 32\n",
        "        self._representation = nn.Sequential(nn.Linear(input_size, self.hx_size),\n",
        "                                             nn.Tanh())\n",
        "        self._dynamics_state = nn.Sequential(nn.Linear(self.hx_size + action_space_n, 64),\n",
        "                                             nn.Tanh(),\n",
        "                                             nn.Linear(64, self.hx_size),\n",
        "                                             nn.Tanh())\n",
        "        self._dynamics_reward = nn.Sequential(nn.Linear(self.hx_size + action_space_n, 64),\n",
        "                                              nn.LeakyReLU(),\n",
        "                                              nn.Linear(64, 2*reward_support_size+1))\n",
        "        self._prediction_actor = nn.Sequential(nn.Linear(self.hx_size, 64),\n",
        "                                               nn.LeakyReLU(),\n",
        "                                               nn.Linear(64, action_space_n))\n",
        "        self._prediction_value = nn.Sequential(nn.Linear(self.hx_size, 64),\n",
        "                                               nn.LeakyReLU(),\n",
        "                                               nn.Linear(64, 2*value_support_size+1))\n",
        "        self.action_space_n = action_space_n\n",
        "\n",
        "        self._prediction_value[-1].weight.data.fill_(0)\n",
        "        self._prediction_value[-1].bias.data.fill_(0)\n",
        "        self._dynamics_reward[-1].weight.data.fill_(0)\n",
        "        self._dynamics_reward[-1].bias.data.fill_(0)\n",
        "\n",
        "    def p(self, state):\n",
        "        actor = torch.softmax(self._prediction_actor(state),dim=1)\n",
        "        value = torch.softmax(self._prediction_value(state),dim=1)\n",
        "        return actor, value\n",
        "\n",
        "    def h(self, obs_history):\n",
        "        return self._representation(obs_history)\n",
        "\n",
        "    def g(self, state, action):\n",
        "        x = torch.cat((state, action), dim=1)\n",
        "        next_state = self._dynamics_state(x)\n",
        "        reward = torch.softmax(self._dynamics_reward(x),dim=1)\n",
        "        return next_state, reward     \n",
        "\n",
        "    def initial_state(self, x):\n",
        "        hout = self.h(x)\n",
        "        prob,v= self.p(hout)\n",
        "        return hout,prob,v\n",
        "    def next_state(self,hin,a):\n",
        "        hout,r = self.g(hin,a)\n",
        "        prob,v= self.p(hout)\n",
        "        return hout,r,prob,v\n",
        "    def inference_initial_state(self, x):\n",
        "        with torch.no_grad():\n",
        "          hout = self.h(x)\n",
        "          prob,v=self.p(hout)\n",
        "\n",
        "          return hout,prob,v\n",
        "    def inference_next_state(self,hin,a):\n",
        "        with torch.no_grad():\n",
        "          hout,r = self.g(hin,a)\n",
        "          prob,v=self.p(hout)\n",
        "          return hout,r,prob,v     \n",
        "\n",
        "\n",
        "print(\"done\")                                      "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkKMTKa6wYtR"
      },
      "source": [
        "\n",
        "#MTCS    MUzero modified for intermeditate rewards settings and using predicted rewards\n",
        "#accepts policy as a list\n",
        "import torch\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "def dynamics(net,state,action):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    #print(state,action) \n",
        "    next_state,reward,prob,value = net.inference_next_state(state.to(device),torch.tensor([action]).float().to(device))\n",
        "    reward = catts(reward.cpu().numpy().ravel())\n",
        "    value = catts(value.cpu().numpy().ravel())\n",
        "    prob = prob.cpu().tolist()[0]\n",
        "    #print(\"dynamics\",prob)\n",
        "    return next_state.cpu(),reward,prob,value\n",
        "\n",
        "\n",
        "class MinMaxStats:\n",
        "    \"\"\"A class that holds the min-max values of the tree.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.MAXIMUM_FLOAT_VALUE = float('inf')       \n",
        "        self.maximum =  -self.MAXIMUM_FLOAT_VALUE\n",
        "        self.minimum =  self.MAXIMUM_FLOAT_VALUE\n",
        "\n",
        "    def update(self, value: float):\n",
        "        if value is None:\n",
        "            raise ValueError\n",
        "\n",
        "        self.maximum = max(self.maximum, value)\n",
        "        self.minimum = min(self.minimum, value)\n",
        "\n",
        "    def normalize(self, value: float) -> float:\n",
        "        # If the value is unknow, by default we set it to the minimum possible value\n",
        "        if value is None:\n",
        "            return 0.0\n",
        "\n",
        "        if self.maximum > self.minimum:\n",
        "            # We normalize only when we have set the maximum and minimum values.\n",
        "            return (value - self.minimum) / (self.maximum - self.minimum)\n",
        "        return value\n",
        "\n",
        "\n",
        "class Node:\n",
        "    \"\"\"A class that represent nodes inside the MCTS tree\"\"\"\n",
        "\n",
        "    def __init__(self, prior: float):\n",
        "        self.visit_count = 0\n",
        "        self.to_play = -1\n",
        "        self.prior = prior\n",
        "        self.value_sum = 0\n",
        "        self.children = {}\n",
        "        self.hidden_state = None\n",
        "        self.reward = 0\n",
        "\n",
        "    def expanded(self):\n",
        "        return len(self.children) > 0\n",
        "\n",
        "    def value(self):\n",
        "        if self.visit_count == 0:\n",
        "            return None\n",
        "        return self.value_sum / self.visit_count\n",
        "\n",
        "\n",
        "def softmax_sample(visit_counts, actions, t):\n",
        "    counts_exp = np.exp(visit_counts) * (1 / t)\n",
        "    probs = counts_exp / np.sum(counts_exp, axis=0)\n",
        "    action_idx = np.random.choice(len(actions), p=probs)\n",
        "    return actions[action_idx]\n",
        "\n",
        "\n",
        "\"\"\"MCTS module: where MuZero thinks inside the tree.\"\"\"\n",
        "\n",
        "\n",
        "def add_exploration_noise( node):\n",
        "    \"\"\"\n",
        "    At the start of each search, we add dirichlet noise to the prior of the root\n",
        "    to encourage the search to explore new actions.\n",
        "    \"\"\"\n",
        "    actions = list(node.children.keys())\n",
        "    noise = np.random.dirichlet([0.25] * len(actions)) # config.root_dirichlet_alpha\n",
        "    frac = 0.25#config.root_exploration_fraction\n",
        "    for a, n in zip(actions, noise):\n",
        "        node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac\n",
        "\n",
        "\n",
        "\n",
        "def ucb_score(parent, child,min_max_stats):\n",
        "    \"\"\"\n",
        "    The score for a node is based on its value, plus an exploration bonus based on\n",
        "    the prior.\n",
        "\n",
        "    \"\"\"\n",
        "    pb_c_base = 19652\n",
        "    pb_c_init = 1.25\n",
        "    pb_c = math.log((parent.visit_count + pb_c_base + 1) / pb_c_base) + pb_c_init\n",
        "    pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
        "\n",
        "    prior_score = pb_c * child.prior\n",
        "    value_score = min_max_stats.normalize(child.value())\n",
        "    return  value_score + prior_score \n",
        "\n",
        "def select_child(node, min_max_stats):\n",
        "    \"\"\"\n",
        "    Select the child with the highest UCB score.\n",
        "    \"\"\"\n",
        "    # When the parent visit count is zero, all ucb scores are zeros, therefore we return a random child\n",
        "    if node.visit_count == 0:\n",
        "        return random.sample(node.children.items(), 1)[0]\n",
        "\n",
        "    _, action, child = max(\n",
        "        (ucb_score(node, child, min_max_stats), action,\n",
        "         child) for action, child in node.children.items())\n",
        "    return action, child\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def expand_node(node, to_play, actions_space,hidden_state,reward,policy):\n",
        "    \"\"\"\n",
        "    We expand a node using the value, reward and policy prediction obtained from\n",
        "    the neural networks.\n",
        "    \"\"\"\n",
        "    node.to_play = to_play\n",
        "    node.hidden_state = hidden_state\n",
        "    node.reward = reward\n",
        "    policy = {a:policy[a] for a in actions_space}\n",
        "    policy_sum = sum(policy.values())\n",
        "    for action, p in policy.items():\n",
        "        node.children[action] = Node(p / policy_sum) # not needed since mine are already softmax but its fine \n",
        "\n",
        "\n",
        "def backpropagate(search_path, value,to_play,discount, min_max_stats):\n",
        "    \"\"\"\n",
        "    At the end of a simulation, we propagate the evaluation all the way up the\n",
        "    tree to the root.\n",
        "    \"\"\"\n",
        "    for node in search_path[::-1]: #[::-1] means reversed\n",
        "        node.value_sum += value \n",
        "        node.visit_count += 1\n",
        "        min_max_stats.update(node.value())\n",
        "\n",
        "        value = node.reward + discount * value\n",
        "\n",
        "\n",
        "def select_action(node, mode ='softmax'):\n",
        "    \"\"\"\n",
        "    After running simulations inside in MCTS, we select an action based on the root's children visit counts.\n",
        "    During training we use a softmax sample for exploration.\n",
        "    During evaluation we select the most visited child.\n",
        "    \"\"\"\n",
        "    visit_counts = [child.visit_count for child in node.children.values()]\n",
        "    actions = [action for action in node.children.keys()]\n",
        "    action = None\n",
        "    if mode == 'softmax':\n",
        "        t = 1.0\n",
        "        action = softmax_sample(visit_counts, actions, t)\n",
        "    elif mode == 'max':\n",
        "        action, _ = max(node.children.items(), key=lambda item: item[1].visit_count)\n",
        "    counts_exp = np.exp(visit_counts)\n",
        "    probs = counts_exp / np.sum(counts_exp, axis=0)    \n",
        "    #return action ,probs,node.value()\n",
        "    return action ,np.array(visit_counts)/sum(visit_counts),node.value()\n",
        "\n",
        "def run_mcts(net, state,prob,root_value,num_simulations,discount = 0.9):\n",
        "    \"\"\"\n",
        "    Core Monte Carlo Tree Search algorithm.\n",
        "    To decide on an action, we run N simulations, always starting at the root of\n",
        "    the search tree and traversing the tree according to the UCB formula until we\n",
        "    reach a leaf node.\n",
        "    \"\"\"\n",
        "    prob, root_value = prob.tolist()[0] ,catts(root_value.numpy().ravel())\n",
        "    to_play = True\n",
        "    action_space=[ i for i in range(len(prob))]#history.action_space()\n",
        "    #print(\"action space\",action_space)\n",
        "    root = Node(0)\n",
        "    expand_node(root, to_play,action_space,state,0.0,prob)#node, to_play, actions_space ,hidden_state,reward,policy\n",
        "    add_exploration_noise( root)\n",
        "\n",
        "\n",
        "    min_max_stats = MinMaxStats()\n",
        "\n",
        "    for _ in range(num_simulations): \n",
        "        node = root\n",
        "        search_path = [node]\n",
        "\n",
        "        while node.expanded():\n",
        "            action, node = select_child( node, min_max_stats)\n",
        "            search_path.append(node)\n",
        "\n",
        "        # Inside the search tree we use the dynamics function to obtain the next\n",
        "        # hidden state given an action and the previous hidden state.\n",
        "        parent = search_path[-2]\n",
        "        \n",
        "        #network_output = network.recurrent_inference(parent.hidden_state, action)\n",
        "        next_state,r,action_probs, value = dynamics(net,parent.hidden_state,onehot(action,len(action_space))) \n",
        "        expand_node(node, to_play, action_space,next_state,r,action_probs)#node, to_play, actions_space ,hidden_state,reward,policy\n",
        "\n",
        "        backpropagate(search_path, value, to_play, discount, min_max_stats)#search_path, value,,discount, min_max_stats\n",
        "    return root    \n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-NpN4lU12kW"
      },
      "source": [
        "import gym\n",
        "class ScalingObservationWrapper(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    Wrapper that apply a min-max scaling of observations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env, low=None, high=None):\n",
        "        super().__init__(env)\n",
        "        assert isinstance(env.observation_space, gym.spaces.Box)\n",
        "\n",
        "        low = np.array(self.observation_space.low if low is None else low)\n",
        "        high = np.array(self.observation_space.high if high is None else high)\n",
        "\n",
        "        self.mean = (high + low) / 2\n",
        "        self.max = high - self.mean\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return (observation - self.mean) / self.max"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waeVGfWytBB1"
      },
      "source": [
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "def onehot(a,n=2):\n",
        "  return np.eye(n)[a]\n",
        "def play_game(env,net,n_sim,discount,render,device,n_act,max_steps):\n",
        "    trajectory=[]\n",
        "    state = env.reset() \n",
        "    done = False\n",
        "    r =0 \n",
        "    stp=0\n",
        "    while not done:\n",
        "        if render:\n",
        "          env.render()\n",
        "        stp+=1  \n",
        "        h ,prob,pred_value= net.inference_initial_state(torch.tensor([state]).float().to(device)) \n",
        "        root  = run_mcts(net,h.cpu(),prob.cpu(),pred_value.cpu(),num_simulations=n_sim,discount=discount)\n",
        "        action,action_prob,mcts_val = select_action(root) \n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        r+=reward\n",
        "        if stp>max_steps:\n",
        "          done = True\n",
        "        data = (state,onehot(action,n_act),action_prob,mcts_val,reward,catts(pred_value.cpu().numpy().ravel()))\n",
        "        trajectory.append(data)\n",
        "        state = next_state\n",
        "    print(\"DATA collection:played for \",len(trajectory),\" steps , rewards\",r)   \n",
        "    return trajectory    \n",
        "def eval_game(env,net,n_sim,render,device,max_steps):\n",
        "    state = env.reset() \n",
        "    done = False\n",
        "    r = 0\n",
        "    stp=0\n",
        "    while not done:\n",
        "        if render:\n",
        "          env.render()\n",
        "        stp+=1  \n",
        "        h ,prob,value= net.inference_initial_state(torch.tensor([state]).float().to(device)) \n",
        "        root  = run_mcts(net,h.cpu(),prob.cpu(),value.cpu(),num_simulations=n_sim,discount=discount)\n",
        "        action,action_prob,mcts_val = select_action(root,\"max\")\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        if stp>max_steps:\n",
        "          done = True\n",
        "        r+=reward\n",
        "        state = next_state\n",
        "    print(\"Eval:played for \",r ,\" rewards\")   \n",
        "    \n",
        "def sample_games(buffer,batch_size):\n",
        "    # Sample game from buffer either uniformly or according to some priority\n",
        "    #print(\"samplig from .\",len(buffer))\n",
        "    return list(np.random.choice(len(buffer),batch_size))\n",
        "\n",
        "def sample_position(trajectory,priority=None):\n",
        "    # Sample position from game either uniformly or according to some priority.\n",
        "    if priority == None:\n",
        "      return np.random.choice(len(trajectory),1)[0]\n",
        "    return np.random.choice(len(trajectory),1,p = priority)[0]\n",
        "    #return np.random.choice(list(range(0, len(trajectory))),1,p = priority)[0]\n",
        "def get_priorities(root_values,pred_values,rewards,discount=0.99, td_steps=10):\n",
        "    z_values = []\n",
        "    alpha = 1\n",
        "    beta = 1 \n",
        "    for current_index in range(len(root_values)):\n",
        "        bootstrap_index = current_index + td_steps\n",
        "        if bootstrap_index < len(root_values):\n",
        "            value = root_values[bootstrap_index] * discount ** td_steps\n",
        "        else:\n",
        "            value = 0\n",
        "\n",
        "        for i, reward in enumerate(rewards[current_index:bootstrap_index]):\n",
        "            value += reward * discount ** i\n",
        "\n",
        "        if current_index < len(root_values):\n",
        "            z_values.append(value)\n",
        "    #print(\"get priorities\",pred_values,z_values)        \n",
        "    p = np.abs(np.array(pred_values)-np.array(z_values))**alpha  + 0.00001\n",
        "    priority = p /np.sum(p)\n",
        "    N= len(pred_values) \n",
        "    weights = (1/(N*priority))**beta\n",
        "    return list(priority),list(weights)\n",
        "def update_priorites(buffer,indexes,new_pred_values):\n",
        "    #buffer is a list and is passed as refernce so changes made here will reflect in buffer\n",
        "    for i in range(len(indexes)):\n",
        "      x,y = indexes[i]\n",
        "      old_state,old_onehot_action,old_action_prob,old_mcts_val,old_reward,old_pred_value = buffer[x][y]\n",
        "      buffer[x][y]=(old_state,old_onehot_action,old_action_prob,old_mcts_val,old_reward,new_pred_values[i])\n",
        "\n",
        "\n",
        "def sample_batch(action_space_size,buffer,discount,batch_size,num_unroll_steps, td_steps,per):\n",
        "    obs_batch, action_batch, reward_batch, value_batch, policy_batch,weights_batch = [], [], [], [], [],[]\n",
        "    indexes=[]\n",
        "    game_idx = sample_games(buffer,batch_size)\n",
        "    for gi in game_idx:\n",
        "      g = buffer[gi]\n",
        "      state,action,action_prob,root_val,reward,pred_val = zip(*g)\n",
        "      state,action,action_prob,root_val,reward,pred_val  =list(state),list(action),list(action_prob),list(root_val),list(reward),list(pred_val)\n",
        "      #print(\"pred val sample batch\",pred_val)\n",
        "      if per:\n",
        "        #make priority for sampling from root_value and n_step value\n",
        "        priority,weights = get_priorities(root_val,pred_val,reward,discount=discount, td_steps=td_steps)\n",
        "        \n",
        "        game_pos = sample_position(g,priority)#state index sampled using priority\n",
        "      else:  \n",
        "        weights = [1.0]*len(root_val)\n",
        "        game_pos = sample_position(g)#state index sampled using priority\n",
        "      _actions = action[game_pos:game_pos + num_unroll_steps]\n",
        "      # random action selection to complete num_unroll_steps\n",
        "      _actions += [onehot(np.random.randint(0, action_space_size),action_space_size)for _ in range(num_unroll_steps - len(_actions))]\n",
        "\n",
        "      obs_batch.append(state[game_pos])\n",
        "      action_batch.append(_actions)\n",
        "      value, reward, policy = make_target(child_visits=action_prob ,root_values=root_val,rewards=reward,state_index=game_pos,discount=discount, num_unroll_steps=num_unroll_steps, td_steps=td_steps)\n",
        "      reward_batch.append(reward)\n",
        "      value_batch.append(value)\n",
        "      policy_batch.append(policy)\n",
        "      weights_batch.append(weights[game_pos])\n",
        "      indexes.append((gi,game_pos))\n",
        "\n",
        "\n",
        "\n",
        "    obs_batch = torch.tensor(obs_batch).float()\n",
        "    action_batch = torch.tensor(action_batch).long()\n",
        "    reward_batch = torch.tensor(reward_batch).float()\n",
        "    value_batch = torch.tensor(value_batch).float()\n",
        "    policy_batch = torch.tensor(policy_batch).float()\n",
        "    weights_batch = torch.tensor(weights_batch).float()\n",
        "    return obs_batch, action_batch, reward_batch, value_batch, policy_batch,weights_batch,indexes\n",
        "\n",
        "\n",
        "def make_target(child_visits,root_values,rewards,state_index,discount=0.99, num_unroll_steps=5, td_steps=10):\n",
        "        # The value target is the discounted root value of the search tree N steps into the future, plus\n",
        "        # the discounted sum of all rewards until then.\n",
        "        target_values, target_rewards, target_policies = [], [], []\n",
        "        for current_index in range(state_index, state_index + num_unroll_steps + 1):\n",
        "            bootstrap_index = current_index + td_steps\n",
        "            if bootstrap_index < len(root_values):\n",
        "                value = root_values[bootstrap_index] * discount ** td_steps\n",
        "            else:\n",
        "                value = 0\n",
        "\n",
        "            for i, reward in enumerate(rewards[current_index:bootstrap_index]):\n",
        "                value += reward * discount ** i\n",
        "\n",
        "            if current_index < len(root_values):\n",
        "                target_values.append(stcat(value))\n",
        "                target_rewards.append(stcat(rewards[current_index]))\n",
        "                target_policies.append(child_visits[current_index])\n",
        "\n",
        "            else:\n",
        "                # States past the end of games are treated as absorbing states.\n",
        "                target_values.append(stcat(0))\n",
        "                target_rewards.append(stcat(0))\n",
        "                # Note: Target policy is  set to 0 so that no policy loss is calculated for them\n",
        "                #target_policies.append([0 for _ in range(len(child_visits[0]))])\n",
        "                target_policies.append(child_visits[0]*0.0)\n",
        "\n",
        "        return target_values, target_rewards, target_policies\n",
        "\n",
        "\n",
        "def scalar_reward_loss( prediction, target):\n",
        "        return -(torch.log(prediction) * target).sum(1)\n",
        "\n",
        "def scalar_value_loss( prediction, target):\n",
        "        return -(torch.log(prediction) * target).sum(1)\n",
        "def update_weights(model, action_space_size, optimizer, replay_buffer,discount,batch_size,num_unroll_steps, td_steps,per ):\n",
        "    batch = sample_batch(action_space_size,replay_buffer,discount,batch_size,num_unroll_steps, td_steps,per)\n",
        "    obs_batch, action_batch, target_reward, target_value, target_policy,target_weights,indexes = batch\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    obs_batch = obs_batch.to(device)\n",
        "    action_batch = action_batch.to(device) \n",
        "    target_reward = target_reward.to(device)\n",
        "    target_value = target_value.to(device)\n",
        "    target_policy = target_policy.to(device)\n",
        "    target_weights = target_weights.to(device)\n",
        "    target_reward_phi =target_reward \n",
        "    target_value_phi = target_value\n",
        "\n",
        "    hidden_state, policy_prob,value  = model.initial_state(obs_batch) # initial model_call #\n",
        "    value_loss = scalar_value_loss(value, target_value_phi[:, 0])\n",
        "    policy_loss = -(torch.log(policy_prob) * target_policy[:, 0]).sum(1)\n",
        "    reward_loss = torch.zeros(batch_size, device=device)\n",
        "\n",
        "    gradient_scale = 1 / num_unroll_steps\n",
        "    for step_i in range(num_unroll_steps):\n",
        "        hidden_state, reward,policy_prob,value  = model.next_state(hidden_state, action_batch[:, step_i]) \n",
        "        #h,pred_reward,pred_policy,pred_value= net.next_state(h,act)\n",
        "        policy_loss += -(torch.log(policy_prob) * target_policy[:, step_i + 1]).sum(1)\n",
        "        value_loss += scalar_value_loss(value, target_value_phi[:, step_i + 1])\n",
        "        reward_loss += scalar_reward_loss(reward, target_reward_phi[:, step_i])\n",
        "        hidden_state.register_hook(lambda grad: grad * 0.5)\n",
        "\n",
        "    # optimize\n",
        "    value_loss_coeff = 1\n",
        "    loss = (policy_loss + value_loss_coeff * value_loss + reward_loss) # find value loss coefficiet = 1?\n",
        "    weights = target_weights#/target_weights.max()#dividing by max doesnt work\n",
        "    weighted_loss = (weights * loss).mean()#1?\n",
        "    weighted_loss.register_hook(lambda grad: grad * gradient_scale)\n",
        "    loss = loss.mean()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    weighted_loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "    optimizer.step()\n",
        "    if per:\n",
        "      #can we recompute search value???\n",
        "      updated_h,updated_prob,updated_pred_value= model.inference_initial_state(obs_batch) \n",
        "      return indexes,updated_pred_value.cpu().numpy()\n",
        "    return None,None  \n",
        "\n",
        "def adjust_lr(optimizer, step_count):\n",
        "\n",
        "    lr_init=0.05\n",
        "    lr_decay_rate=0.01\n",
        "    lr_decay_steps=10000\n",
        "    lr = lr_init * lr_decay_rate ** (step_count / lr_decay_steps)\n",
        "    lr = max(lr, 0.001)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    return lr\n",
        "def get_scalars(new_pred_values):\n",
        "    vals = []\n",
        "    for i in range(new_pred_values.shape[0]):\n",
        "      #print(new_pred_values[i,:].shape)\n",
        "      vals.append(catts(new_pred_values[i,:]))\n",
        "    return vals\n",
        "learning_rate = [0.05]   \n",
        "def net_train(net,  action_space_size, replay_buffer,discount,batch_size,num_unroll_steps, td_steps,training_steps=1000,per = False):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model =net\n",
        "    #MuZeroNet(input_size=4, action_space_n=2, reward_support_size=5, value_support_size=5).to(device) #training fresh net\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate[0], momentum=0.9,weight_decay=1e-4)\n",
        "    #training_steps=training_steps=500#20000\n",
        "    # wait for replay buffer to be non-empty\n",
        "    while len(replay_buffer) == 0:\n",
        "        pass\n",
        "\n",
        "    for step_count in tqdm(range(training_steps)):\n",
        "        learning_rate[0] = adjust_lr( optimizer, step_count)\n",
        "        indexes,new_pred_values = update_weights(model, action_space_size, optimizer, replay_buffer,discount,batch_size,num_unroll_steps, td_steps,per)\n",
        "        if per:\n",
        "          #print(\"new pred val net train\",new_pred_values,new_pred_values.shape)\n",
        "          new_pred_values = get_scalars(new_pred_values)\n",
        "          #print(\"new pred val net train\",new_pred_values)\n",
        "          update_priorites(replay_buffer,indexes,new_pred_values)\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZWra51wFVvb",
        "outputId": "0b603510-8bd5-41a6-988e-1ce9b4064d32"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "render = False\n",
        "episodes_per_train=30\n",
        "episodes_per_eval =5\n",
        "buffer =[]\n",
        "#buffer = deque(maxlen = episodes_per_train)\n",
        "training_steps=50\n",
        "max_steps=5000\n",
        "n_sim= 50\n",
        "discount = 0.95\n",
        "batch_size = 512\n",
        "envs = ['CartPole-v1','MountainCar-v0','LunarLander-v2']\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"training for \",envs[2])\n",
        "env=gym.make(envs[2])\n",
        "#env=env.unwrapped\n",
        "#env = ScalingObservationWrapper(env, low=[-2.4, -2.0, -0.42, -3.5], high=[2.4, 2.0, 0.42, 3.5])\n",
        "\n",
        "s_dim =env.observation_space.shape[0]\n",
        "print(\"s_dim: \",s_dim)\n",
        "a_dim =env.action_space.n\n",
        "print(\"a_dim: \",a_dim)\n",
        "a_bound =1 #env.action_space.high[0]\n",
        "print(\"a_bound: \",a_bound)\n",
        "\n",
        "\n",
        "\n",
        "net = MuZeroNet(input_size=s_dim, action_space_n=a_dim, reward_support_size=5, value_support_size=5).to(device)\n",
        "\n",
        "for t in range(training_steps):\n",
        "  buffer =[] # onpolicy \n",
        "  for _ in range(episodes_per_train):\n",
        "    buffer.append(play_game(env,net,n_sim,discount,render,device,a_dim,max_steps))\n",
        "  print(\"training from \",len(buffer),\" games\")  \n",
        "  if t<20:\n",
        "    priority = True \n",
        "    tr_stp=500\n",
        "  else :\n",
        "    tr_stp=2000\n",
        "    priority =False\n",
        "  print(\"training with \",\" priority \",priority,\" training_steps \",tr_stp,\" discount \",discount,\" batch_size \",batch_size)  \n",
        "  net = net_train(net,  action_space_size=a_dim, replay_buffer=buffer,discount=discount,batch_size=batch_size,num_unroll_steps=5, td_steps=10,training_steps=tr_stp,per = priority)\n",
        "  for _ in range(episodes_per_eval):\n",
        "    eval_game(env,net,n_sim,render,device,max_steps)\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training for  LunarLander-v2\n",
            "s_dim:  8\n",
            "a_dim:  4\n",
            "a_bound:  1\n",
            "DATA collection:played for  98  steps , rewards -141.1932960548482\n",
            "DATA collection:played for  123  steps , rewards -318.149614259283\n",
            "DATA collection:played for  92  steps , rewards -120.40991421839625\n",
            "DATA collection:played for  79  steps , rewards -255.60368568737937\n",
            "DATA collection:played for  65  steps , rewards -50.78488507906782\n",
            "DATA collection:played for  72  steps , rewards -63.80596752078115\n",
            "DATA collection:played for  96  steps , rewards -303.63623156771456\n",
            "DATA collection:played for  97  steps , rewards -398.13930087144246\n",
            "DATA collection:played for  90  steps , rewards -221.23524556531473\n",
            "DATA collection:played for  92  steps , rewards -170.1451203553965\n",
            "DATA collection:played for  79  steps , rewards -218.2174499935831\n",
            "DATA collection:played for  69  steps , rewards -107.55340198997106\n",
            "DATA collection:played for  100  steps , rewards -405.331853471014\n",
            "DATA collection:played for  87  steps , rewards -238.1258709309695\n",
            "DATA collection:played for  90  steps , rewards -296.91850407219476\n",
            "DATA collection:played for  87  steps , rewards -318.15604430492436\n",
            "DATA collection:played for  68  steps , rewards -60.99341766109809\n",
            "DATA collection:played for  70  steps , rewards -138.11715780039023\n",
            "DATA collection:played for  96  steps , rewards -248.04268233578892\n",
            "DATA collection:played for  71  steps , rewards -263.75254605745533\n",
            "DATA collection:played for  1000  steps , rewards 73.97502933924213\n",
            "DATA collection:played for  60  steps , rewards -120.26189876415901\n",
            "DATA collection:played for  106  steps , rewards -141.01791040453867\n",
            "DATA collection:played for  110  steps , rewards -315.75180419481217\n",
            "DATA collection:played for  112  steps , rewards -124.05989076611762\n",
            "DATA collection:played for  79  steps , rewards -242.56156926930126\n",
            "DATA collection:played for  120  steps , rewards -170.10928301132202\n",
            "DATA collection:played for  100  steps , rewards 39.62699823738777\n",
            "DATA collection:played for  91  steps , rewards -99.99265012556931\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  71  steps , rewards -256.9756022405569\n",
            "training from  30  games\n",
            "training with   priority  True  training_steps  500  discount  0.95  batch_size  512\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [06:09<00:00,  1.35it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  -118.12993951320635  rewards\n",
            "Eval:played for  -328.5296606723067  rewards\n",
            "Eval:played for  -449.2996395342301  rewards\n",
            "Eval:played for  -58.62213750870227  rewards\n",
            "Eval:played for  -261.04693803902444  rewards\n",
            "DATA collection:played for  93  steps , rewards -256.28613396155936\n",
            "DATA collection:played for  73  steps , rewards -142.51435582981713\n",
            "DATA collection:played for  76  steps , rewards -96.07710255819276\n",
            "DATA collection:played for  111  steps , rewards -108.1100175495263\n",
            "DATA collection:played for  72  steps , rewards -213.72808817631926\n",
            "DATA collection:played for  71  steps , rewards -319.5609865467464\n",
            "DATA collection:played for  57  steps , rewards -226.54275364133832\n",
            "DATA collection:played for  103  steps , rewards -63.700376028895874\n",
            "DATA collection:played for  96  steps , rewards -165.5319761762274\n",
            "DATA collection:played for  83  steps , rewards -394.9698797266119\n",
            "DATA collection:played for  64  steps , rewards -57.32425336125553\n",
            "DATA collection:played for  89  steps , rewards -411.393815348514\n",
            "DATA collection:played for  80  steps , rewards -368.87923971885425\n",
            "DATA collection:played for  101  steps , rewards -198.66168192254298\n",
            "DATA collection:played for  76  steps , rewards -414.84695775239777\n",
            "DATA collection:played for  73  steps , rewards -108.925516346291\n",
            "DATA collection:played for  84  steps , rewards -351.0610205024867\n",
            "DATA collection:played for  57  steps , rewards -200.73149025003926\n",
            "DATA collection:played for  81  steps , rewards -342.5771421178997\n",
            "DATA collection:played for  71  steps , rewards -82.79698610762472\n",
            "DATA collection:played for  70  steps , rewards -174.82521880600552\n",
            "DATA collection:played for  76  steps , rewards -372.82423938624027\n",
            "DATA collection:played for  78  steps , rewards -64.86861245087294\n",
            "DATA collection:played for  70  steps , rewards -27.732415672432495\n",
            "DATA collection:played for  86  steps , rewards -99.40278625562766\n",
            "DATA collection:played for  107  steps , rewards -319.78571409475734\n",
            "DATA collection:played for  104  steps , rewards -318.9261203137584\n",
            "DATA collection:played for  58  steps , rewards -137.04820005729243\n",
            "DATA collection:played for  106  steps , rewards -127.82614927048743\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  86  steps , rewards -243.48067082663306\n",
            "training from  30  games\n",
            "training with   priority  True  training_steps  500  discount  0.95  batch_size  512\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [05:10<00:00,  1.61it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  -496.3270503579622  rewards\n",
            "Eval:played for  -261.55316766794203  rewards\n",
            "Eval:played for  -496.98035724620996  rewards\n",
            "Eval:played for  -342.54789474327254  rewards\n",
            "Eval:played for  -428.92334515892026  rewards\n",
            "DATA collection:played for  79  steps , rewards -248.5859691933526\n",
            "DATA collection:played for  77  steps , rewards -299.3381354159354\n",
            "DATA collection:played for  117  steps , rewards -414.2497524144708\n",
            "DATA collection:played for  95  steps , rewards -211.30545633909284\n",
            "DATA collection:played for  75  steps , rewards -209.1598491816002\n",
            "DATA collection:played for  94  steps , rewards -369.73116009844927\n",
            "DATA collection:played for  100  steps , rewards -453.6303363505119\n",
            "DATA collection:played for  109  steps , rewards -371.890377902187\n",
            "DATA collection:played for  89  steps , rewards -335.4439110399028\n",
            "DATA collection:played for  114  steps , rewards -345.8017810881313\n",
            "DATA collection:played for  103  steps , rewards -384.5200076468918\n",
            "DATA collection:played for  89  steps , rewards -212.61224950265145\n",
            "DATA collection:played for  117  steps , rewards -497.34347521700573\n",
            "DATA collection:played for  95  steps , rewards -417.8634201584791\n",
            "DATA collection:played for  107  steps , rewards -546.6183750898924\n",
            "DATA collection:played for  98  steps , rewards -357.1656544483975\n",
            "DATA collection:played for  96  steps , rewards -226.83387231846922\n",
            "DATA collection:played for  112  steps , rewards -402.6923396552527\n",
            "DATA collection:played for  115  steps , rewards -255.61545541634493\n",
            "DATA collection:played for  117  steps , rewards -489.53125732113335\n",
            "DATA collection:played for  115  steps , rewards -403.49602217237305\n",
            "DATA collection:played for  96  steps , rewards -410.851217994336\n",
            "DATA collection:played for  90  steps , rewards -398.30588145798225\n",
            "DATA collection:played for  116  steps , rewards -366.8329900436904\n",
            "DATA collection:played for  92  steps , rewards -255.93631703618988\n",
            "DATA collection:played for  185  steps , rewards -158.06770052660488\n",
            "DATA collection:played for  102  steps , rewards -446.25766708211813\n",
            "DATA collection:played for  91  steps , rewards -375.69034578640475\n",
            "DATA collection:played for  102  steps , rewards -466.9406895540273\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  96  steps , rewards -367.3395977188769\n",
            "training from  30  games\n",
            "training with   priority  True  training_steps  500  discount  0.95  batch_size  512\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [05:44<00:00,  1.45it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  -34.78971397468369  rewards\n",
            "Eval:played for  -103.23995846838147  rewards\n",
            "Eval:played for  -108.10055473971157  rewards\n",
            "Eval:played for  -134.45093101696884  rewards\n",
            "Eval:played for  -28.95291156740649  rewards\n",
            "DATA collection:played for  98  steps , rewards -77.1688787969922\n",
            "DATA collection:played for  71  steps , rewards -85.79495689795029\n",
            "DATA collection:played for  85  steps , rewards -79.26584299584137\n",
            "DATA collection:played for  118  steps , rewards -12.249188844742292\n",
            "DATA collection:played for  205  steps , rewards -15.518814939952776\n",
            "DATA collection:played for  74  steps , rewards -81.18132655671519\n",
            "DATA collection:played for  79  steps , rewards 2.431991846486099\n",
            "DATA collection:played for  81  steps , rewards -119.12401267191721\n",
            "DATA collection:played for  73  steps , rewards -74.46770357750663\n",
            "DATA collection:played for  105  steps , rewards -130.53407399904944\n",
            "DATA collection:played for  101  steps , rewards -52.88992403201395\n",
            "DATA collection:played for  74  steps , rewards -54.2501053715026\n",
            "DATA collection:played for  85  steps , rewards -151.18740907972256\n",
            "DATA collection:played for  66  steps , rewards -33.663695277188225\n",
            "DATA collection:played for  102  steps , rewards -55.292131463504745\n",
            "DATA collection:played for  72  steps , rewards -86.45217872691885\n",
            "DATA collection:played for  79  steps , rewards -172.8126064894617\n",
            "DATA collection:played for  94  steps , rewards -73.33123509126192\n",
            "DATA collection:played for  104  steps , rewards -44.3875199430486\n",
            "DATA collection:played for  75  steps , rewards -77.7177373179396\n",
            "DATA collection:played for  116  steps , rewards -97.0269095889\n",
            "DATA collection:played for  110  steps , rewards -142.08884729622457\n",
            "DATA collection:played for  88  steps , rewards -85.26878335808378\n",
            "DATA collection:played for  92  steps , rewards -129.27840885111345\n",
            "DATA collection:played for  106  steps , rewards -132.72684344791116\n",
            "DATA collection:played for  104  steps , rewards -108.96534757129626\n",
            "DATA collection:played for  81  steps , rewards -152.8766013956729\n",
            "DATA collection:played for  99  steps , rewards -118.89052687376522\n",
            "DATA collection:played for  124  steps , rewards -64.25513942479884\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  132  steps , rewards -22.929406171416872\n",
            "training from  30  games\n",
            "training with   priority  True  training_steps  500  discount  0.95  batch_size  512\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [05:34<00:00,  1.50it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  -34.37022207219856  rewards\n",
            "Eval:played for  -19.11673029205359  rewards\n",
            "Eval:played for  22.846586021302045  rewards\n",
            "Eval:played for  -41.242831391969354  rewards\n",
            "Eval:played for  -46.399075744343115  rewards\n",
            "DATA collection:played for  92  steps , rewards 18.887956287189652\n",
            "DATA collection:played for  165  steps , rewards -213.8869636055587\n",
            "DATA collection:played for  65  steps , rewards -63.28574058627558\n",
            "DATA collection:played for  161  steps , rewards -174.07561930947844\n",
            "DATA collection:played for  81  steps , rewards 0.1342848893852704\n",
            "DATA collection:played for  137  steps , rewards -31.047827748222176\n",
            "DATA collection:played for  75  steps , rewards 6.441040627992692\n",
            "DATA collection:played for  160  steps , rewards -138.61033718548353\n",
            "DATA collection:played for  115  steps , rewards -28.328141292593983\n",
            "DATA collection:played for  86  steps , rewards 43.978652696038466\n",
            "DATA collection:played for  225  steps , rewards -107.4939775281121\n",
            "DATA collection:played for  115  steps , rewards -44.75763647722842\n",
            "DATA collection:played for  79  steps , rewards -68.45248789653922\n",
            "DATA collection:played for  191  steps , rewards -155.40804954863\n",
            "DATA collection:played for  122  steps , rewards -21.81442778769984\n",
            "DATA collection:played for  102  steps , rewards 2.3160202196786486\n",
            "DATA collection:played for  252  steps , rewards -75.42871975228208\n",
            "DATA collection:played for  72  steps , rewards -50.63133739590421\n",
            "DATA collection:played for  103  steps , rewards 20.661443443556323\n",
            "DATA collection:played for  151  steps , rewards -178.83867761014244\n",
            "DATA collection:played for  98  steps , rewards 33.901290515663135\n",
            "DATA collection:played for  191  steps , rewards -150.30845634830922\n",
            "DATA collection:played for  194  steps , rewards -163.55688962549732\n",
            "DATA collection:played for  170  steps , rewards -63.83710359852928\n",
            "DATA collection:played for  71  steps , rewards -34.10594300857265\n",
            "DATA collection:played for  127  steps , rewards -1.195772265247868\n",
            "DATA collection:played for  78  steps , rewards -49.993208855706484\n",
            "DATA collection:played for  65  steps , rewards -98.07182587302835\n",
            "DATA collection:played for  82  steps , rewards -100.31890393190179\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  393  steps , rewards -376.5714589366171\n",
            "training from  30  games\n",
            "training with   priority  True  training_steps  500  discount  0.95  batch_size  512\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 30%|███       | 151/500 [02:01<04:40,  1.25it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO0ybT-uW7RG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}