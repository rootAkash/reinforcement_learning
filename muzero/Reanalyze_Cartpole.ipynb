{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reanalyze:Cartpole.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNsu8+J9E/syoCRmQqFDQYm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rootAkash/reinforcement_learning/blob/master/muzero/Reanalyze_Cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBD-aKyF5fUr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfJ7r4ZLfn6u"
      },
      "source": [
        "takes too much time to train for reanalyze only advisable if simulation or episode data is costly to get"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcnQSsnTkUjI"
      },
      "source": [
        " what is reanalyze variant of muzero?\n",
        "off  policy varient of muzero that uses old data by recomputing fresh target values using a target value net and \n",
        "also provides fresh policy targets by recomputing tree search .\n",
        "the priorities will also change because z and root v changed because v function changes during training, z changes because of the new  updated targets.?do we update priorities?recompute search value v?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfUbzbuAWDbM"
      },
      "source": [
        "new experinces have priority pred value and z value  and importance samplong weights will be calculated using that\n",
        "and the l1 loss between predicted value and returns z is calculated from the initial training forward pass and not after backprop for updating priorities\n",
        "but that anyways will require extra loop of converting them to scalars from category outputs to calculate l1 unless vectorised which it is\n",
        "so it will be fastter than looping since catts supports vectors so looping is not nneeded so priority update before backprop is fine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwI18Z6p1oGP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "592b969b-538b-401f-8783-f35bff0e7405"
      },
      "source": [
        "!pip install gym[all]\n",
        "!pip install box2d-py\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym[all] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[all]) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[all]) (1.4.1)\n",
            "Requirement already satisfied: imageio; extra == \"all\" in /usr/local/lib/python3.7/dist-packages (from gym[all]) (2.4.1)\n",
            "Requirement already satisfied: opencv-python; extra == \"all\" in /usr/local/lib/python3.7/dist-packages (from gym[all]) (4.1.2.30)\n",
            "Requirement already satisfied: Pillow; extra == \"all\" in /usr/local/lib/python3.7/dist-packages (from gym[all]) (7.1.2)\n",
            "Collecting box2d-py~=2.3.5; extra == \"all\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/34/da5393985c3ff9a76351df6127c275dcb5749ae0abbe8d5210f06d97405d/box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 5.4MB/s \n",
            "\u001b[?25hCollecting mujoco-py<2.0,>=1.50; extra == \"all\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/8c/64e0630b3d450244feef0688d90eab2448631e40ba6bdbd90a70b84898e7/mujoco-py-1.50.1.68.tar.gz (120kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: atari-py~=0.2.0; extra == \"all\" in /usr/local/lib/python3.7/dist-packages (from gym[all]) (0.2.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[all]) (0.16.0)\n",
            "Collecting glfw>=1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/d7/79c091c877493de7f8286ed62c77bf0f2c51105656073846b2326021b524/glfw-2.1.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (205kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.7/dist-packages (from mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (0.29.22)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.7/dist-packages (from mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (1.14.5)\n",
            "Collecting lockfile>=0.12.2\n",
            "  Downloading https://files.pythonhosted.org/packages/c8/22/9460e311f340cb62d26a38c419b1381b8593b0bb6b5d1f056938b086d362/lockfile-0.12.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0; extra == \"all\"->gym[all]) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.10->mujoco-py<2.0,>=1.50; extra == \"all\"->gym[all]) (2.20)\n",
            "Building wheels for collected packages: mujoco-py\n",
            "  Building wheel for mujoco-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for mujoco-py\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for mujoco-py\n",
            "Failed to build mujoco-py\n",
            "Installing collected packages: box2d-py, glfw, lockfile, mujoco-py\n",
            "    Running setup.py install for mujoco-py ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-6ca2re0r/mujoco-py/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-6ca2re0r/mujoco-py/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-bcc18lr6/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n",
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.7/dist-packages (2.3.8)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "Suggested packages:\n",
            "  libgle3\n",
            "The following NEW packages will be installed:\n",
            "  python-opengl\n",
            "0 upgraded, 1 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 496 kB of archives.\n",
            "After this operation, 5,416 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Fetched 496 kB in 2s (290 kB/s)\n",
            "Selecting previously unselected package python-opengl.\n",
            "(Reading database ... 160690 files and directories currently installed.)\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,270 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.9 [784 kB]\n",
            "Fetched 784 kB in 2s (422 kB/s)\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 163045 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.9_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQRen3PlNkiB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc09f4a0-a4bd-44b6-8e52-ebe64890f3a3"
      },
      "source": [
        "import numpy as np\n",
        "def stcat(x,support=15):\n",
        "  x = np.sign(x) * ((abs(x) + 1)**0.5 - 1) + 0.001 * x\n",
        "  x = np.clip(x, -support, support)\n",
        "  floor = np.floor(x)\n",
        "  prob = x - floor\n",
        "  logits = np.zeros( 2 * support + 1)\n",
        "  first_index = int(floor + support)\n",
        "  second_index = int(floor + support+1)\n",
        "  logits[first_index] = 1-prob\n",
        "  if prob>0:\n",
        "    logits[second_index] = prob\n",
        "  return logits\n",
        "#allow for batch processing  \n",
        "def catts(x,support=15):\n",
        "  support = np.arange(-support, support+1, 1)\n",
        "  if len(x.shape)==2:\n",
        "    #for  batch of x\\\n",
        "    x = np.sum(support*x,axis=1)\n",
        "  elif len(x.shape)==1:\n",
        "    #for single x\n",
        "    x = np.sum(support*x)  \n",
        "  else:\n",
        "    print(\"wrong input for conversion to  scalar\")  \n",
        "  x = np.sign(x) * ((((1 + 4 * 0.001 * (np.abs(x) + 1 + 0.001))**0.5 - 1) / (2 * 0.001))** 2- 1)\n",
        "  return x  \n",
        "\n",
        "#cat = stcat(5)#test 1 example\n",
        "cat = np.array([stcat(15),stcat(-15)]) # test batch example\n",
        "print(cat,cat.shape)\n",
        "scalar = catts(cat)\n",
        "print(scalar)\n",
        "print(\"done\")        \n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            "  0.    0.    0.    0.    0.    0.    0.985 0.015 0.    0.    0.    0.\n",
            "  0.    0.    0.    0.    0.    0.    0.   ]\n",
            " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.015\n",
            "  0.985 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
            "  0.    0.    0.    0.    0.    0.    0.   ]] (2, 31)\n",
            "[ 15. -15.]\n",
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhf3dyDa2GYq",
        "outputId": "b1268008-57f0-4878-e742-ebb8eed7f642"
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MuZeroNet(nn.Module):\n",
        "    def __init__(self, input_size, action_space_n, reward_support_size, value_support_size):\n",
        "        super().__init__()\n",
        "        self.hx_size = 32\n",
        "        self._representation = nn.Sequential(nn.Linear(input_size, self.hx_size),\n",
        "                                             nn.Tanh())\n",
        "        self._dynamics_state = nn.Sequential(nn.Linear(self.hx_size + action_space_n, 64),\n",
        "                                             nn.Tanh(),\n",
        "                                             nn.Linear(64, self.hx_size),\n",
        "                                             nn.Tanh())\n",
        "        self._dynamics_reward = nn.Sequential(nn.Linear(self.hx_size + action_space_n, 64),\n",
        "                                              nn.LeakyReLU(),\n",
        "                                              nn.Linear(64, 2*reward_support_size+1))\n",
        "        self._prediction_actor = nn.Sequential(nn.Linear(self.hx_size, 64),\n",
        "                                               nn.LeakyReLU(),\n",
        "                                               nn.Linear(64, action_space_n))\n",
        "        self._prediction_value = nn.Sequential(nn.Linear(self.hx_size, 64),\n",
        "                                               nn.LeakyReLU(),\n",
        "                                               nn.Linear(64, 2*value_support_size+1))\n",
        "        self.action_space_n = action_space_n\n",
        "\n",
        "        self._prediction_value[-1].weight.data.fill_(0)\n",
        "        self._prediction_value[-1].bias.data.fill_(0)\n",
        "        self._dynamics_reward[-1].weight.data.fill_(0)\n",
        "        self._dynamics_reward[-1].bias.data.fill_(0)\n",
        "\n",
        "    def p(self, state):\n",
        "        actor = torch.softmax(self._prediction_actor(state),dim=1)\n",
        "        value = torch.softmax(self._prediction_value(state),dim=1)\n",
        "        return actor, value\n",
        "\n",
        "    def h(self, obs_history):\n",
        "        return self._representation(obs_history)\n",
        "\n",
        "    def g(self, state, action):\n",
        "        x = torch.cat((state, action), dim=1)\n",
        "        next_state = self._dynamics_state(x)\n",
        "        reward = torch.softmax(self._dynamics_reward(x),dim=1)\n",
        "        return next_state, reward     \n",
        "\n",
        "    def initial_state(self, x):\n",
        "        hout = self.h(x)\n",
        "        prob,v= self.p(hout)\n",
        "        return hout,prob,v\n",
        "    def next_state(self,hin,a):\n",
        "        hout,r = self.g(hin,a)\n",
        "        prob,v= self.p(hout)\n",
        "        return hout,r,prob,v\n",
        "    def inference_initial_state(self, x):\n",
        "        with torch.no_grad():\n",
        "          hout = self.h(x)\n",
        "          prob,v=self.p(hout)\n",
        "\n",
        "          return hout,prob,v\n",
        "    def inference_next_state(self,hin,a):\n",
        "        with torch.no_grad():\n",
        "          hout,r = self.g(hin,a)\n",
        "          prob,v=self.p(hout)\n",
        "          return hout,r,prob,v     \n",
        "\n",
        "\n",
        "print(\"done\")                                      "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkKMTKa6wYtR"
      },
      "source": [
        "\n",
        "#MTCS    MUzero modified for intermeditate rewards settings and using predicted rewards\n",
        "#accepts policy as a list\n",
        "import torch\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "def dynamics(net,state,action):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    #print(state,action) \n",
        "    next_state,reward,prob,value = net.inference_next_state(state.to(device),torch.tensor([action]).float().to(device))\n",
        "    reward = catts(reward.cpu().numpy().ravel())\n",
        "    value = catts(value.cpu().numpy().ravel())\n",
        "    prob = prob.cpu().tolist()[0]\n",
        "    #print(\"dynamics\",prob)\n",
        "    return next_state.cpu(),reward,prob,value\n",
        "\n",
        "\n",
        "class MinMaxStats:\n",
        "    \"\"\"A class that holds the min-max values of the tree.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.MAXIMUM_FLOAT_VALUE = float('inf')       \n",
        "        self.maximum =  -self.MAXIMUM_FLOAT_VALUE\n",
        "        self.minimum =  self.MAXIMUM_FLOAT_VALUE\n",
        "\n",
        "    def update(self, value: float):\n",
        "        if value is None:\n",
        "            raise ValueError\n",
        "\n",
        "        self.maximum = max(self.maximum, value)\n",
        "        self.minimum = min(self.minimum, value)\n",
        "\n",
        "    def normalize(self, value: float) -> float:\n",
        "        # If the value is unknow, by default we set it to the minimum possible value\n",
        "        if value is None:\n",
        "            return 0.0\n",
        "\n",
        "        if self.maximum > self.minimum:\n",
        "            # We normalize only when we have set the maximum and minimum values.\n",
        "            return (value - self.minimum) / (self.maximum - self.minimum)\n",
        "        return value\n",
        "\n",
        "\n",
        "class Node:\n",
        "    \"\"\"A class that represent nodes inside the MCTS tree\"\"\"\n",
        "\n",
        "    def __init__(self, prior: float):\n",
        "        self.visit_count = 0\n",
        "        self.to_play = -1\n",
        "        self.prior = prior\n",
        "        self.value_sum = 0\n",
        "        self.children = {}\n",
        "        self.hidden_state = None\n",
        "        self.reward = 0\n",
        "\n",
        "    def expanded(self):\n",
        "        return len(self.children) > 0\n",
        "\n",
        "    def value(self):\n",
        "        if self.visit_count == 0:\n",
        "            return None\n",
        "        return self.value_sum / self.visit_count\n",
        "\n",
        "\n",
        "def softmax_sample(visit_counts, actions, t):\n",
        "    counts_exp = np.exp(visit_counts) * (1 / t)\n",
        "    probs = counts_exp / np.sum(counts_exp, axis=0)\n",
        "    action_idx = np.random.choice(len(actions), p=probs)\n",
        "    return actions[action_idx]\n",
        "\n",
        "\n",
        "\"\"\"MCTS module: where MuZero thinks inside the tree.\"\"\"\n",
        "\n",
        "\n",
        "def add_exploration_noise( node):\n",
        "    \"\"\"\n",
        "    At the start of each search, we add dirichlet noise to the prior of the root\n",
        "    to encourage the search to explore new actions.\n",
        "    \"\"\"\n",
        "    actions = list(node.children.keys())\n",
        "    noise = np.random.dirichlet([0.25] * len(actions)) # config.root_dirichlet_alpha\n",
        "    frac = 0.25#config.root_exploration_fraction\n",
        "    for a, n in zip(actions, noise):\n",
        "        node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac\n",
        "\n",
        "\n",
        "\n",
        "def ucb_score(parent, child,min_max_stats):\n",
        "    \"\"\"\n",
        "    The score for a node is based on its value, plus an exploration bonus based on\n",
        "    the prior.\n",
        "\n",
        "    \"\"\"\n",
        "    pb_c_base = 19652\n",
        "    pb_c_init = 1.25\n",
        "    pb_c = math.log((parent.visit_count + pb_c_base + 1) / pb_c_base) + pb_c_init\n",
        "    pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
        "\n",
        "    prior_score = pb_c * child.prior\n",
        "    value_score = min_max_stats.normalize(child.value())\n",
        "    return  value_score + prior_score \n",
        "\n",
        "def select_child(node, min_max_stats):\n",
        "    \"\"\"\n",
        "    Select the child with the highest UCB score.\n",
        "    \"\"\"\n",
        "    # When the parent visit count is zero, all ucb scores are zeros, therefore we return a random child\n",
        "    if node.visit_count == 0:\n",
        "        return random.sample(node.children.items(), 1)[0]\n",
        "\n",
        "    _, action, child = max(\n",
        "        (ucb_score(node, child, min_max_stats), action,\n",
        "         child) for action, child in node.children.items())\n",
        "    return action, child\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def expand_node(node, to_play, actions_space,hidden_state,reward,policy):\n",
        "    \"\"\"\n",
        "    We expand a node using the value, reward and policy prediction obtained from\n",
        "    the neural networks.\n",
        "    \"\"\"\n",
        "    node.to_play = to_play\n",
        "    node.hidden_state = hidden_state\n",
        "    node.reward = reward\n",
        "    policy = {a:policy[a] for a in actions_space}\n",
        "    policy_sum = sum(policy.values())\n",
        "    for action, p in policy.items():\n",
        "        node.children[action] = Node(p / policy_sum) # not needed since mine are already softmax but its fine \n",
        "\n",
        "\n",
        "def backpropagate(search_path, value,to_play,discount, min_max_stats):\n",
        "    \"\"\"\n",
        "    At the end of a simulation, we propagate the evaluation all the way up the\n",
        "    tree to the root.\n",
        "    \"\"\"\n",
        "    for node in search_path[::-1]: #[::-1] means reversed\n",
        "        node.value_sum += value \n",
        "        node.visit_count += 1\n",
        "        min_max_stats.update(node.value())\n",
        "\n",
        "        value = node.reward + discount * value\n",
        "\n",
        "\n",
        "def select_action(node, mode ='softmax'):\n",
        "    \"\"\"\n",
        "    After running simulations inside in MCTS, we select an action based on the root's children visit counts.\n",
        "    During training we use a softmax sample for exploration.\n",
        "    During evaluation we select the most visited child.\n",
        "    \"\"\"\n",
        "    visit_counts = [child.visit_count for child in node.children.values()]\n",
        "    actions = [action for action in node.children.keys()]\n",
        "    action = None\n",
        "    if mode == 'softmax':\n",
        "        t = 1.0\n",
        "        action = softmax_sample(visit_counts, actions, t)\n",
        "    elif mode == 'max':\n",
        "        action, _ = max(node.children.items(), key=lambda item: item[1].visit_count)\n",
        "    counts_exp = np.exp(visit_counts)\n",
        "    probs = counts_exp / np.sum(counts_exp, axis=0)    \n",
        "    #return action ,probs,node.value()\n",
        "    return action ,np.array(visit_counts)/sum(visit_counts),node.value()\n",
        "\n",
        "def run_mcts(net, state,prob,root_value,num_simulations,discount = 0.9):\n",
        "    \"\"\"\n",
        "    Core Monte Carlo Tree Search algorithm.\n",
        "    To decide on an action, we run N simulations, always starting at the root of\n",
        "    the search tree and traversing the tree according to the UCB formula until we\n",
        "    reach a leaf node.\n",
        "    \"\"\"\n",
        "    prob, root_value = prob.tolist()[0] ,catts(root_value.numpy().ravel())\n",
        "    to_play = True\n",
        "    action_space=[ i for i in range(len(prob))]#history.action_space()\n",
        "    #print(\"action space\",action_space)\n",
        "    root = Node(0)\n",
        "    expand_node(root, to_play,action_space,state,0.0,prob)#node, to_play, actions_space ,hidden_state,reward,policy\n",
        "    add_exploration_noise( root)\n",
        "\n",
        "\n",
        "    min_max_stats = MinMaxStats()\n",
        "\n",
        "    for _ in range(num_simulations): \n",
        "        node = root\n",
        "        search_path = [node]\n",
        "\n",
        "        while node.expanded():\n",
        "            action, node = select_child( node, min_max_stats)\n",
        "            search_path.append(node)\n",
        "\n",
        "        # Inside the search tree we use the dynamics function to obtain the next\n",
        "        # hidden state given an action and the previous hidden state.\n",
        "        parent = search_path[-2]\n",
        "        \n",
        "        #network_output = network.recurrent_inference(parent.hidden_state, action)\n",
        "        next_state,r,action_probs, value = dynamics(net,parent.hidden_state,onehot(action,len(action_space))) \n",
        "        expand_node(node, to_play, action_space,next_state,r,action_probs)#node, to_play, actions_space ,hidden_state,reward,policy\n",
        "\n",
        "        backpropagate(search_path, value, to_play, discount, min_max_stats)#search_path, value,,discount, min_max_stats\n",
        "    return root    \n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-NpN4lU12kW"
      },
      "source": [
        "import gym\n",
        "class ScalingObservationWrapper(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    Wrapper that apply a min-max scaling of observations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env, low=None, high=None):\n",
        "        super().__init__(env)\n",
        "        assert isinstance(env.observation_space, gym.spaces.Box)\n",
        "\n",
        "        low = np.array(self.observation_space.low if low is None else low)\n",
        "        high = np.array(self.observation_space.high if high is None else high)\n",
        "\n",
        "        self.mean = (high + low) / 2\n",
        "        self.max = high - self.mean\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return (observation - self.mean) / self.max"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waeVGfWytBB1"
      },
      "source": [
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "def onehot(a,n=2):\n",
        "  return np.eye(n)[a]\n",
        "def play_game(env,net,targetnet,n_sim,discount,render,device,n_act,max_steps,td_steps,per):\n",
        "    trajectory=[]\n",
        "    root_values,pred_values,rewards=[],[],[]\n",
        "    state = env.reset() \n",
        "    done = False\n",
        "    r =0 \n",
        "    stp=0\n",
        "    while not done:\n",
        "        if render:\n",
        "          env.render()\n",
        "        stp+=1  \n",
        "        h ,prob,pred_value= net.inference_initial_state(torch.tensor([state]).float().to(device)) \n",
        "        root  = run_mcts(net,h.cpu(),prob.cpu(),pred_value.cpu(),num_simulations=n_sim,discount=discount)\n",
        "        action,action_prob,mcts_val = select_action(root) \n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        r+=reward\n",
        "        if stp>max_steps:\n",
        "          done = True\n",
        "        data = [state,onehot(action,n_act),action_prob,mcts_val,reward,1]#state,onehotaction,action_prob,mcts_val,reward,priority\n",
        "        if targetnet is None:\n",
        "          root_values.append(mcts_val)\n",
        "        else:\n",
        "          ht ,tarprob,target_pred_value= targetnet.inference_initial_state(torch.tensor([state]).float().to(device))\n",
        "          root_values.append(catts(target_pred_value.cpu().numpy().ravel()))\n",
        "        pred_values.append(catts(pred_value.cpu().numpy().ravel()))\n",
        "        rewards.append(reward)\n",
        "        trajectory.append(data)\n",
        "        state = next_state\n",
        "    #calculating priority as z - pred value\n",
        "    if per:  \n",
        "      priorities =get_initial_priorities(root_values,pred_values,rewards,discount=discount, td_steps=td_steps)\n",
        "      #update trajectory priority\n",
        "      assert len(trajectory) == len(priorities)\n",
        "      for i in range(len(trajectory)):\n",
        "        trajectory[i][5]=priorities[i]\n",
        "    print(\"DATA collection:played for \",len(trajectory),\" steps , rewards\",r,\" last state \",state)   \n",
        "    return trajectory    \n",
        "def get_initial_priorities(root_values,pred_values,rewards,discount=0.99, td_steps=10):\n",
        "    z_values = []\n",
        "    alpha = 1\n",
        "    beta = 1 \n",
        "    for current_index in range(len(root_values)):\n",
        "        bootstrap_index = current_index + td_steps\n",
        "        if bootstrap_index < len(root_values):\n",
        "            value = root_values[bootstrap_index] * discount ** td_steps\n",
        "        else:\n",
        "            value = 0\n",
        "\n",
        "        for i, reward in enumerate(rewards[current_index:bootstrap_index]):\n",
        "            value += reward * discount ** i\n",
        "\n",
        "        if current_index < len(root_values):\n",
        "            z_values.append(value)\n",
        "    #print(\"get priorities\",pred_values,z_values)        \n",
        "    p = np.abs(np.array(pred_values)-np.array(z_values))**alpha  + 0.00001\n",
        "    #priority = p /np.sum(p)\n",
        "    #N= len(pred_values) \n",
        "    #weights = (1/(N*priority))**beta\n",
        "    return list(p)#,list(weights)\n",
        "def eval_game(env,net,n_sim,render,device,max_steps):\n",
        "    state = env.reset() \n",
        "    done = False\n",
        "    r = 0\n",
        "    stp=0\n",
        "    while not done:\n",
        "        if render:\n",
        "          env.render()\n",
        "        stp+=1  \n",
        "        h ,prob,value= net.inference_initial_state(torch.tensor([state]).float().to(device)) \n",
        "        root  = run_mcts(net,h.cpu(),prob.cpu(),value.cpu(),num_simulations=n_sim,discount=discount)\n",
        "        action,action_prob,mcts_val = select_action(root,\"max\")\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        if stp>max_steps:\n",
        "          done = True\n",
        "        r+=reward\n",
        "        state = next_state\n",
        "    print(\"Eval:played for \",r ,\" rewards\",\" last state \",state)   \n",
        "    \n",
        "def sample_games(buffer,batch_size):\n",
        "    # Sample game from buffer either uniformly or according to some priority\n",
        "    #print(\"samplig from .\",len(buffer))\n",
        "    return list(np.random.choice(len(buffer),batch_size))\n",
        "\n",
        "def sample_position(trajectory,priority=None):\n",
        "    # Sample position from game either uniformly or according to some priority.\n",
        "    if priority == None:\n",
        "      return np.random.choice(len(trajectory),1)[0]\n",
        "    return np.random.choice(len(trajectory),1,p = priority)[0]\n",
        "    #return np.random.choice(list(range(0, len(trajectory))),1,p = priority)[0]\n",
        "\n",
        "def update_priorites(buffer,indexes,new_priority):\n",
        "    #buffer is a list and is passed as refernce so changes made here will reflect in buffer\n",
        "    for i in range(len(indexes)):\n",
        "      x,y = indexes[i]\n",
        "      #old_state,old_onehot_action,old_action_prob,old_mcts_val,old_reward,old_pred_value = buffer[x][y]\n",
        "      #buffer[x][y]=(old_state,old_onehot_action,old_action_prob,old_mcts_val,old_reward,new_pred_values[i])\n",
        "      buffer[x][y][5]=new_priority[i]\n",
        "\n",
        "\n",
        "def sample_batch(model,targetmodel,action_space_size,buffer,discount,batch_size,num_unroll_steps, td_steps,n_sim,per):\n",
        "    obs_batch, action_batch, reward_batch, value_batch, policy_batch,weights_batch = [], [], [], [], [],[]\n",
        "    indexes=[]\n",
        "    game_idx = sample_games(buffer,batch_size)\n",
        "    for gi in game_idx:\n",
        "      g = buffer[gi]\n",
        "      state,action,action_prob,root_val,reward,priority = zip(*g)\n",
        "      state,action,action_prob,root_val,reward,priority  =list(state),list(action),list(action_prob),list(root_val),list(reward),list(priority)\n",
        "      #print(\"pred val sample batch\",priority)\n",
        "      if per:\n",
        "        #make priority for sampling from root_value and n_step value\n",
        "        ps  = np.array(priority)/np.sum(np.array(priority))\n",
        "        game_pos = sample_position(g,list(ps))#state index sampled using priority\n",
        "        beta =1 \n",
        "        N = len(g)\n",
        "        weight =(1/(N*ps[game_pos]))**beta\n",
        "        #N= len(pred_values) \n",
        "        #weights = (1/(N*priority))**beta\n",
        "      else:  \n",
        "        weight = 1.0\n",
        "        game_pos = sample_position(g)#state index sampled using priority\n",
        "      _actions = action[game_pos:game_pos + num_unroll_steps]\n",
        "      # random action selection to complete num_unroll_steps\n",
        "      _actions += [onehot(np.random.randint(0, action_space_size),action_space_size)for _ in range(num_unroll_steps - len(_actions))]\n",
        "\n",
        "      obs_batch.append(state[game_pos])\n",
        "      action_batch.append(_actions)\n",
        "      value, reward, policy = make_target(model=model,target_model=targetmodel,buffer=buffer,gameindex=gi,states_trajectory=state,child_visits=action_prob ,root_values=root_val,\n",
        "                                          rewards=reward,state_index=game_pos,discount=discount, num_unroll_steps=num_unroll_steps, td_steps=td_steps,n_sim=n_sim)\n",
        "      reward_batch.append(reward)\n",
        "      value_batch.append(value)\n",
        "      policy_batch.append(policy)\n",
        "      weights_batch.append(weight)\n",
        "      indexes.append((gi,game_pos))\n",
        "\n",
        "\n",
        "\n",
        "    obs_batch = torch.tensor(obs_batch).float()\n",
        "    action_batch = torch.tensor(action_batch).long()\n",
        "    reward_batch = torch.tensor(reward_batch).float()\n",
        "    value_batch = torch.tensor(value_batch).float()\n",
        "    policy_batch = torch.tensor(policy_batch).float()\n",
        "    weights_batch = torch.tensor(weights_batch).float()\n",
        "    return obs_batch, action_batch, reward_batch, value_batch, policy_batch,weights_batch,indexes\n",
        "\n",
        "\n",
        "def make_target(model,target_model,buffer,gameindex,states_trajectory,child_visits,root_values,rewards,state_index,discount=0.99, num_unroll_steps=5, td_steps=5,n_sim=50):\n",
        "        # The value target is the discounted root value of the search tree or value by target network N steps into the future, plus\n",
        "        # the discounted sum of all rewards until then.\n",
        "        target_values, target_rewards, target_policies = [], [], []\n",
        "        for current_index in range(state_index, state_index + num_unroll_steps + 1):\n",
        "            bootstrap_index = current_index + td_steps\n",
        "            if bootstrap_index < len(root_values):\n",
        "                if target_model is None:\n",
        "                    value = root_values[bootstrap_index] * discount ** td_steps\n",
        "                else:\n",
        "                    #  a target network  based on recent parameters is used to provide a fresher,\n",
        "                    # stable n-step bootstrapped target for the value function\n",
        "                    obs = states_trajectory[bootstrap_index]\n",
        "                    ht ,tarprob,target_pred_value= target_model.inference_initial_state(torch.tensor([obs]).float().to(device)) #using target model\n",
        "                    #ht ,tarprob,target_pred_value= model.inference_initial_state(torch.tensor([obs]).float().to(device))#try recent model(same as target update freq as 1 and hard update ) ######################\n",
        "                    value=catts(target_pred_value.cpu().numpy().ravel()) * discount ** td_steps\n",
        "            else:\n",
        "                value = 0\n",
        "\n",
        "            for i, reward in enumerate(rewards[current_index:bootstrap_index]):\n",
        "                value += reward * discount ** i\n",
        "\n",
        "            if current_index < len(root_values):\n",
        "                target_values.append(stcat(value))\n",
        "                target_rewards.append(stcat(rewards[current_index]))\n",
        "                if target_model is not None and np.random.random() <= 0.8: \n",
        "                    #we recompute policy for current_index using latest params model  \n",
        "                    #and then change it in buffer also use it as target policy 80 percent of time to keep labels stable\n",
        "                    obs = states_trajectory[current_index]\n",
        "                    h ,prob,pred_value= model.inference_initial_state(torch.tensor([obs]).float().to(device)) ##############################################\n",
        "                    root  = run_mcts(model,h.cpu(),prob.cpu(),pred_value.cpu(),num_simulations=n_sim,discount=discount)\n",
        "                    action,action_prob,mcts_val = select_action(root) \n",
        "                    buffer[gameindex][current_index][2]=action_prob #only change this rest values depend on the original trajectory\n",
        "                    child_visits[current_index] =action_prob\n",
        "\n",
        "                target_policies.append(child_visits[current_index])\n",
        "\n",
        "            else:\n",
        "                # States past the end of games are treated as absorbing states.\n",
        "                target_values.append(stcat(0))\n",
        "                target_rewards.append(stcat(0))\n",
        "                # Note: Target policy is  set to 0 so that no policy loss is calculated for them\n",
        "                #target_policies.append([0 for _ in range(len(child_visits[0]))])\n",
        "                target_policies.append(child_visits[0]*0.0)\n",
        "\n",
        "        return target_values, target_rewards, target_policies\n",
        "\n",
        "\n",
        "def scalar_reward_loss( prediction, target):\n",
        "        return -(torch.log(prediction) * target).sum(1)\n",
        "\n",
        "def scalar_value_loss( prediction, target):\n",
        "        return -(torch.log(prediction) * target).sum(1)\n",
        "def update_weights(model,targetmodel,action_space_size, optimizer, replay_buffer,discount,batch_size,num_unroll_steps, td_steps,n_sim,per ):\n",
        "    batch = sample_batch(model,targetmodel,action_space_size,replay_buffer,discount,batch_size,num_unroll_steps, td_steps,n_sim,per)\n",
        "    obs_batch, action_batch, target_reward, target_value, target_policy,target_weights,indexes = batch\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    obs_batch = obs_batch.to(device)\n",
        "    action_batch = action_batch.to(device) \n",
        "    target_reward = target_reward.to(device)\n",
        "    target_value = target_value.to(device)\n",
        "    target_policy = target_policy.to(device)\n",
        "    target_weights = target_weights.to(device)\n",
        "    target_reward_phi =target_reward \n",
        "    target_value_phi = target_value\n",
        "\n",
        "    hidden_state, policy_prob,value  = model.initial_state(obs_batch) # initial model_call #\n",
        "    \n",
        "    value_loss = scalar_value_loss(value, target_value_phi[:, 0])\n",
        "    policy_loss = -(torch.log(policy_prob) * target_policy[:, 0]).sum(1)\n",
        "    reward_loss = torch.zeros(batch_size, device=device)\n",
        "    initial_state_values = value.detach()\n",
        "    gradient_scale = 1 / num_unroll_steps\n",
        "    for step_i in range(num_unroll_steps):\n",
        "        hidden_state, reward,policy_prob,value  = model.next_state(hidden_state, action_batch[:, step_i]) \n",
        "        #h,pred_reward,pred_policy,pred_value= net.next_state(h,act)\n",
        "        policy_loss += -(torch.log(policy_prob) * target_policy[:, step_i + 1]).sum(1)\n",
        "        value_loss += scalar_value_loss(value, target_value_phi[:, step_i + 1])\n",
        "        reward_loss += scalar_reward_loss(reward, target_reward_phi[:, step_i])\n",
        "        hidden_state.register_hook(lambda grad: grad * 0.5)\n",
        "\n",
        "    # optimize\n",
        "    if targetmodel is None:\n",
        "      value_loss_coeff = 1\n",
        "    else:  \n",
        "      value_loss_coeff = 0.25 #to reduce value overfiiting due to off policy \n",
        "    loss = (policy_loss + value_loss_coeff * value_loss + reward_loss) # find value loss coefficiet = 1?\n",
        "    weights = target_weights#/target_weights.max()#dividing by max doesnt work\n",
        "    weighted_loss = (weights * loss).mean()#1?\n",
        "    weighted_loss.register_hook(lambda grad: grad * gradient_scale)\n",
        "    loss = loss.mean()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    weighted_loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "    optimizer.step()\n",
        "    if per:\n",
        "      #remvoing 2nd forward pass can do it also should be chill???\n",
        "      #updated_h,updated_prob,updated_pred_value= model.inference_initial_state(obs_batch) \n",
        "      #return indexes,updated_pred_value.cpu().numpy()\n",
        "      return indexes,np.abs(catts(initial_state_values.cpu().numpy())-catts(target_value[:, 0].cpu().numpy())) +0.00001\n",
        "    return None,None  \n",
        "\n",
        "def adjust_lr(optimizer, step_count):\n",
        "\n",
        "    lr_init=0.05\n",
        "    lr_decay_rate=0.01\n",
        "    lr_decay_steps=10000\n",
        "    lr = lr_init * lr_decay_rate ** (step_count / lr_decay_steps)\n",
        "    lr = max(lr, 0.001)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    return lr\n",
        "def soft_update(target, source, tau):\n",
        "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
        "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
        "    return target\n",
        "def get_scalars(new_pred_values):\n",
        "    vals = []\n",
        "    for i in range(new_pred_values.shape[0]):\n",
        "      #print(new_pred_values[i,:].shape)\n",
        "      vals.append(catts(new_pred_values[i,:]))\n",
        "    return vals\n",
        "learning_rate = [0.05]   \n",
        "stp=[0]\n",
        "def net_train(net, targetnet, action_space_size, replay_buffer,discount,batch_size,num_unroll_steps, td_steps,training_steps=1000,target_update=50,tou=1,n_sim=50,per = False):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model =net\n",
        "    target_model = targetnet\n",
        "    #MuZeroNet(input_size=4, action_space_n=2, reward_support_size=5, value_support_size=5).to(device) #training fresh net\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate[0], momentum=0.9,weight_decay=1e-4)\n",
        "    #training_steps=training_steps=500#20000\n",
        "    # wait for replay buffer to be non-empty\n",
        "    while len(replay_buffer) == 0:\n",
        "        pass\n",
        "\n",
        "    for step_count in tqdm(range(training_steps)):\n",
        "        stp[0]+=1\n",
        "        learning_rate[0] = adjust_lr( optimizer, step_count)\n",
        "        indexes,new_priority = update_weights(model,target_model, action_space_size, optimizer, replay_buffer,discount,batch_size,num_unroll_steps, td_steps,n_sim,per)\n",
        "        if target_model is not None:\n",
        "          if stp[0] % target_update==0:\n",
        "            #print(\"softupdate \", tou)\n",
        "            soft_update(target=target_model, source=model, tau=tou)\n",
        "        if per:\n",
        "          #print(\"new pred val net train\",new_pred_values,new_pred_values.shape)\n",
        "          #new_pred_values = get_scalars(new_pred_values)\n",
        "          #print(\"new pred val net train\",new_pred_values)\n",
        "          update_priorites(replay_buffer,indexes,new_priority)\n",
        "\n",
        "    return model,target_model\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZWra51wFVvb",
        "outputId": "f935df48-1134-43b0-df6f-5087d4691994"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "render = False\n",
        "episodes_per_train=30\n",
        "episodes_per_eval =5\n",
        "buffer =[]\n",
        "#buffer = deque(maxlen = episodes_per_train)\n",
        "training_steps=50\n",
        "max_steps=5000\n",
        "n_sim= 25\n",
        "discount = 0.99\n",
        "target_update_frewq = 100 # C\n",
        "update_frac=1 # hard update tou\n",
        "batch_size = 126\n",
        "envs = ['CartPole-v1','MountainCar-v0','LunarLander-v2']\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"training for \",envs[0])\n",
        "env=gym.make(envs[0])\n",
        "#env=env.unwrapped\n",
        "env = ScalingObservationWrapper(env, low=[-2.4, -2.0, -0.42, -3.5], high=[2.4, 2.0, 0.42, 3.5])\n",
        "\n",
        "s_dim =env.observation_space.shape[0]\n",
        "print(\"s_dim: \",s_dim)\n",
        "a_dim =env.action_space.n\n",
        "print(\"a_dim: \",a_dim)\n",
        "a_bound =1 #env.action_space.high[0]\n",
        "print(\"a_bound: \",a_bound)\n",
        "\n",
        "\n",
        "\n",
        "net = MuZeroNet(input_size=s_dim, action_space_n=a_dim, reward_support_size=15, value_support_size=15).to(device)\n",
        "targetnet = MuZeroNet(input_size=s_dim, action_space_n=a_dim, reward_support_size=15, value_support_size=15).to(device) #None for not using reanalyze\n",
        "targetnet = soft_update(target=targetnet, source=net, tau=1)#make them same\n",
        "for t in range(training_steps):\n",
        "  if t<0:\n",
        "    priority = True \n",
        "    tr_stp=20\n",
        "  else :\n",
        "    tr_stp=500\n",
        "    priority = True \n",
        "  if targetnet is None:\n",
        "    buffer =[] # onpolicy \n",
        "  for _ in range(episodes_per_train):\n",
        "    buffer.append(play_game(env,net,targetnet,n_sim,discount,render,device,a_dim,max_steps,td_steps=5,per=priority))\n",
        "  print(\"training from \",len(buffer),\" games\")  \n",
        "\n",
        "  print(\"training with \",\" priority \",priority,\" training_steps \",tr_stp,\" discount \",discount,\" batch_size \",batch_size)  \n",
        "  net,targetnet = net_train(net,targetnet,action_space_size=a_dim, replay_buffer=buffer,discount=discount,batch_size=batch_size,num_unroll_steps=5, \n",
        "                            td_steps=5,training_steps=tr_stp,target_update=target_update_frewq,tou=update_frac,n_sim=n_sim,per = priority)\n",
        "  if t>5:\n",
        "    for _ in range(episodes_per_eval):\n",
        "      eval_game(env,net,n_sim,render,device,max_steps)\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training for  CartPole-v1\n",
            "s_dim:  4\n",
            "a_dim:  2\n",
            "a_bound:  1\n",
            "DATA collection:played for  58  steps , rewards 58.0  last state  [-0.15431019 -0.77445405  0.51780215  0.55903597]\n",
            "DATA collection:played for  14  steps , rewards 14.0  last state  [ 0.06103812  0.56370056 -0.51481705 -0.53929262]\n",
            "DATA collection:played for  36  steps , rewards 36.0  last state  [ 0.00793828  0.01872424 -0.52886311 -0.36515194]\n",
            "DATA collection:played for  23  steps , rewards 23.0  last state  [ 0.02831681  0.10830081 -0.51524068 -0.25013732]\n",
            "DATA collection:played for  13  steps , rewards 13.0  last state  [-0.06151515 -0.67700191  0.52985892  0.6190931 ]\n",
            "DATA collection:played for  25  steps , rewards 25.0  last state  [ 0.03255765  0.13109527 -0.50433279 -0.23099076]\n",
            "DATA collection:played for  26  steps , rewards 26.0  last state  [ 0.00342875  0.19162591 -0.56528247 -0.33828269]\n",
            "DATA collection:played for  23  steps , rewards 23.0  last state  [-0.06087723 -0.29881526  0.542338    0.36979774]\n",
            "DATA collection:played for  18  steps , rewards 18.0  last state  [ 0.07483588  0.39891254 -0.53598248 -0.42904152]\n",
            "DATA collection:played for  27  steps , rewards 27.0  last state  [-0.01495693 -0.31811222  0.5294361   0.40149866]\n",
            "DATA collection:played for  16  steps , rewards 16.0  last state  [-0.03847615 -0.37549273  0.502868    0.44115301]\n",
            "DATA collection:played for  15  steps , rewards 15.0  last state  [ 0.04713605  0.69058035 -0.549284   -0.65867858]\n",
            "DATA collection:played for  29  steps , rewards 29.0  last state  [ 0.06399564  0.09555341 -0.53425227 -0.22414176]\n",
            "DATA collection:played for  21  steps , rewards 21.0  last state  [-0.04554435 -0.13123235  0.52610354  0.24426035]\n",
            "DATA collection:played for  12  steps , rewards 12.0  last state  [ 0.05711979  0.59345974 -0.53573788 -0.58272157]\n",
            "DATA collection:played for  18  steps , rewards 18.0  last state  [-0.07183883 -0.42086672  0.54523792  0.44064385]\n",
            "DATA collection:played for  14  steps , rewards 14.0  last state  [ 0.06482649  0.38644038 -0.50825827 -0.4112102 ]\n",
            "DATA collection:played for  25  steps , rewards 25.0  last state  [-0.07059056 -0.66784667  0.50242678  0.56934757]\n",
            "DATA collection:played for  13  steps , rewards 13.0  last state  [-0.05926686 -0.50030879  0.50735193  0.47421035]\n",
            "DATA collection:played for  13  steps , rewards 13.0  last state  [ 0.059352    0.70560756 -0.49898143 -0.62522652]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.07127671  0.80021648 -0.59887651 -0.72433411]\n",
            "DATA collection:played for  17  steps , rewards 17.0  last state  [ 0.04333832  0.71189315 -0.57705421 -0.66866113]\n",
            "DATA collection:played for  37  steps , rewards 37.0  last state  [-0.03662065 -0.07394263 -0.51006055 -0.29443386]\n",
            "DATA collection:played for  11  steps , rewards 11.0  last state  [ 0.05487633  0.30340961 -0.51245909 -0.34751113]\n",
            "DATA collection:played for  34  steps , rewards 34.0  last state  [-0.10454682 -0.19638845  0.52582055  0.19497645]\n",
            "DATA collection:played for  11  steps , rewards 11.0  last state  [ 0.06298081  0.47202613 -0.54939106 -0.4858956 ]\n",
            "DATA collection:played for  17  steps , rewards 17.0  last state  [ 0.04455047  0.29290908 -0.51423287 -0.37246295]\n",
            "DATA collection:played for  35  steps , rewards 35.0  last state  [-0.0456452  -0.49300446  0.52039083  0.52060845]\n",
            "DATA collection:played for  26  steps , rewards 26.0  last state  [-0.03966955 -0.57571306  0.51861305  0.53856159]\n",
            "DATA collection:played for  15  steps , rewards 15.0  last state  [-0.0312329  -0.28855495  0.52776398  0.38515215]\n",
            "training from  30  games\n",
            "training with   priority  True  training_steps  500  discount  0.99  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [1:02:02<00:00,  7.44s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.08565562  0.96634365 -0.58941766 -0.8856724 ]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.07605704  0.87581755 -0.5997628  -0.82517014]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.04699571  0.86561142 -0.54933384 -0.80291138]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.05665913  0.88940717 -0.57012864 -0.81718497]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.07522005  0.8911587  -0.5279891  -0.80878092]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.08642674  0.96742804 -0.51566876 -0.85425188]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.05690712  0.85710831 -0.61363596 -0.81962729]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.09480445  0.99808149 -0.55676053 -0.85956301]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.06684843  0.96109438 -0.5417526  -0.86859608]\n",
            "DATA collection:played for  8  steps , rewards 8.0  last state  [ 0.04647181  0.80794139 -0.5273718  -0.73126059]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.06209258  0.97647328 -0.57935885 -0.86665063]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.05870677  1.00177992 -0.62675233 -0.89669056]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.07238396  0.89836372 -0.58523096 -0.82112562]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.03893715  0.88271816 -0.52406508 -0.80491024]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.07292444  0.87478308 -0.56058103 -0.8167359 ]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.08161839  1.00118116 -0.59320214 -0.87128601]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.06077422  0.9911066  -0.60617284 -0.87410102]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.06433408  0.86119989 -0.52370561 -0.80319907]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.05472517  0.98672345 -0.52909176 -0.8509148 ]\n",
            "DATA collection:played for  11  steps , rewards 11.0  last state  [ 0.09557896  1.09270086 -0.63208406 -0.94037999]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.09711313  0.99713298 -0.57016607 -0.87773603]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.06194978  0.85658135 -0.50012425 -0.78572203]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.07421856  0.9010208  -0.60159285 -0.80837322]\n",
            "DATA collection:played for  8  steps , rewards 8.0  last state  [ 0.06108667  0.76426114 -0.51274227 -0.73398313]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.0648942   0.97397167 -0.51061328 -0.84574729]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.08989524  0.953233   -0.50548066 -0.84937487]\n",
            "DATA collection:played for  8  steps , rewards 8.0  last state  [ 0.06219169  0.79532498 -0.51852913 -0.73235874]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.0746651   0.86732079 -0.53209626 -0.80598498]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.05635132  0.89563283 -0.55929891 -0.81311782]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.0497346   0.90308474 -0.53760809 -0.79789679]\n",
            "training from  60  games\n",
            "training with   priority  True  training_steps  500  discount  0.99  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [55:24<00:00,  6.65s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.05912245 -0.89621762  0.52682264  0.79893364]\n",
            "DATA collection:played for  8  steps , rewards 8.0  last state  [-0.04035548 -0.80635177  0.50601791  0.73140021]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.05581096 -0.97817488  0.53023713  0.85231911]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.03974244 -0.86440971  0.54634601  0.80707152]\n",
            "DATA collection:played for  8  steps , rewards 8.0  last state  [-0.05350151 -0.80491458  0.51149194  0.73692676]\n",
            "DATA collection:played for  8  steps , rewards 8.0  last state  [-0.0530282  -0.76568827  0.54334273  0.74220937]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.04402145 -0.86138796  0.50819648  0.78273438]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.08056916 -0.89897024  0.58391555  0.80230218]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.04263009 -0.90109951  0.54675306  0.80511053]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.06904624 -0.85962004  0.62055399  0.83312267]\n",
            "DATA collection:played for  8  steps , rewards 8.0  last state  [-0.06379029 -0.77742344  0.50678076  0.73384918]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.0817093  -0.96780707  0.62894848  0.88519041]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.03804565 -0.86261962  0.54530018  0.80063284]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.07881125 -0.98988717  0.59453079  0.87834015]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.06206115 -0.86664264  0.54983903  0.80965823]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.05232348 -0.95926452  0.60616288  0.89168621]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.05780073 -0.90149671  0.55405672  0.81575228]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.06471065 -0.98671377  0.57947563  0.87149256]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.06694536 -0.99187685  0.55216511  0.87171688]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.08144692 -0.90387143  0.56040189  0.81275749]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.07871995 -0.99663843  0.58801386  0.87879948]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.08522078 -0.96080956  0.57460258  0.86936589]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.05522989 -0.90322605  0.60233956  0.81631913]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.04491961 -0.86161225  0.59103143  0.8157529 ]\n",
            "DATA collection:played for  8  steps , rewards 8.0  last state  [-0.04571821 -0.79202983  0.53585561  0.74011013]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.05656063 -0.89669867  0.54333858  0.80458414]\n",
            "DATA collection:played for  8  steps , rewards 8.0  last state  [-0.04002463 -0.78476724  0.51308266  0.72063039]\n",
            "DATA collection:played for  8  steps , rewards 8.0  last state  [-0.05569016 -0.79673458  0.52351341  0.73376481]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.04758478 -0.86547367  0.54958395  0.80583088]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.09473117 -0.98734272  0.61556927  0.8867375 ]\n",
            "training from  90  games\n",
            "training with   priority  True  training_steps  500  discount  0.99  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [53:05<00:00,  6.37s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  82  steps , rewards 82.0  last state  [-0.25594682 -0.55895256 -0.51606467 -0.06851432]\n",
            "DATA collection:played for  82  steps , rewards 82.0  last state  [-0.25397465 -0.55277168 -0.51579132 -0.10100478]\n",
            "DATA collection:played for  127  steps , rewards 127.0  last state  [-0.25053431 -0.62280784 -0.50308242 -0.02250036]\n",
            "DATA collection:played for  137  steps , rewards 137.0  last state  [0.3302624  0.61946946 0.50200996 0.03887599]\n",
            "DATA collection:played for  119  steps , rewards 119.0  last state  [-0.26570324 -0.62991303 -0.50340428 -0.02469783]\n",
            "DATA collection:played for  142  steps , rewards 142.0  last state  [-0.25987733 -0.54788588 -0.51237783 -0.08441706]\n",
            "DATA collection:played for  82  steps , rewards 82.0  last state  [-0.26500054 -0.55567813 -0.52013724 -0.09709091]\n",
            "DATA collection:played for  122  steps , rewards 122.0  last state  [-0.25310737 -0.55935238 -0.50982611 -0.07902501]\n",
            "DATA collection:played for  92  steps , rewards 92.0  last state  [-0.24873617 -0.55079203 -0.49972613 -0.07803578]\n",
            "DATA collection:played for  120  steps , rewards 120.0  last state  [0.33988248 0.54190866 0.52781425 0.13041654]\n",
            "DATA collection:played for  151  steps , rewards 151.0  last state  [-0.24370171 -0.45647686 -0.50284378 -0.13409966]\n",
            "DATA collection:played for  132  steps , rewards 132.0  last state  [0.29040355 0.54298074 0.5237697  0.11783136]\n",
            "DATA collection:played for  119  steps , rewards 119.0  last state  [0.28896444 0.45473103 0.49994226 0.14089275]\n",
            "DATA collection:played for  136  steps , rewards 136.0  last state  [-0.23290904 -0.56779712 -0.5070198  -0.039416  ]\n",
            "DATA collection:played for  142  steps , rewards 142.0  last state  [-0.23448257 -0.52236383 -0.5107097  -0.09467171]\n",
            "DATA collection:played for  137  steps , rewards 137.0  last state  [0.31735834 0.42526716 0.50340479 0.15444123]\n",
            "DATA collection:played for  90  steps , rewards 90.0  last state  [-0.2259019  -0.54904913 -0.51156586 -0.08144891]\n",
            "DATA collection:played for  109  steps , rewards 109.0  last state  [ 0.28632455  0.64157791  0.50391645 -0.00732597]\n",
            "DATA collection:played for  89  steps , rewards 89.0  last state  [-0.24620804 -0.45170152 -0.50131119 -0.16131237]\n",
            "DATA collection:played for  145  steps , rewards 145.0  last state  [0.28008138 0.43538216 0.50623034 0.16354573]\n",
            "DATA collection:played for  100  steps , rewards 100.0  last state  [-0.27146357 -0.55614663 -0.50539003 -0.05252504]\n",
            "DATA collection:played for  112  steps , rewards 112.0  last state  [-0.26666665 -0.54319912 -0.51801351 -0.10250772]\n",
            "DATA collection:played for  129  steps , rewards 129.0  last state  [-0.25955487 -0.62680101 -0.50304558 -0.03178813]\n",
            "DATA collection:played for  74  steps , rewards 74.0  last state  [-0.25180157 -0.55382016 -0.52233118 -0.09471132]\n",
            "DATA collection:played for  102  steps , rewards 102.0  last state  [-0.26125111 -0.54334495 -0.52370012 -0.09946217]\n",
            "DATA collection:played for  142  steps , rewards 142.0  last state  [-0.23944542 -0.56286467 -0.50452263 -0.04885256]\n",
            "DATA collection:played for  84  steps , rewards 84.0  last state  [-0.25707918 -0.55155456 -0.52000538 -0.09083832]\n",
            "DATA collection:played for  98  steps , rewards 98.0  last state  [-0.25947173 -0.55391097 -0.50910322 -0.0708529 ]\n",
            "DATA collection:played for  138  steps , rewards 138.0  last state  [-0.26553301 -0.54760165 -0.52232285 -0.09045987]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  107  steps , rewards 107.0  last state  [-0.25697394 -0.43879296 -0.50162005 -0.17347384]\n",
            "training from  120  games\n",
            "training with   priority  True  training_steps  500  discount  0.99  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [1:04:31<00:00,  7.74s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  123  steps , rewards 123.0  last state  [0.7009952  0.99044709 0.50826655 0.04031314]\n",
            "DATA collection:played for  229  steps , rewards 229.0  last state  [0.75498617 0.97391417 0.49961143 0.06292939]\n",
            "DATA collection:played for  223  steps , rewards 223.0  last state  [0.86093367 0.97899191 0.5233022  0.1039374 ]\n",
            "DATA collection:played for  297  steps , rewards 297.0  last state  [-1.00076448 -0.83988698 -0.29454478  0.01940528]\n",
            "DATA collection:played for  230  steps , rewards 230.0  last state  [0.61610274 0.90066409 0.49933381 0.06257666]\n",
            "DATA collection:played for  179  steps , rewards 179.0  last state  [0.82001424 1.01488146 0.51638038 0.05803951]\n",
            "DATA collection:played for  279  steps , rewards 279.0  last state  [0.64809067 0.99556286 0.50174123 0.00438356]\n",
            "DATA collection:played for  229  steps , rewards 229.0  last state  [0.68592989 0.99827173 0.50995213 0.01088256]\n",
            "DATA collection:played for  265  steps , rewards 265.0  last state  [0.70120832 1.00591158 0.50416631 0.01158709]\n",
            "DATA collection:played for  135  steps , rewards 135.0  last state  [0.71656813 0.9823335  0.51890723 0.06051653]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.04474645  0.74050697  0.45852642  0.04150161]\n",
            "DATA collection:played for  151  steps , rewards 151.0  last state  [0.76352217 1.01102658 0.51326124 0.03523127]\n",
            "DATA collection:played for  149  steps , rewards 149.0  last state  [0.77000816 0.99403666 0.50444339 0.05170328]\n",
            "DATA collection:played for  166  steps , rewards 166.0  last state  [ 0.79338591  1.09687995  0.50247845 -0.02697692]\n",
            "DATA collection:played for  167  steps , rewards 167.0  last state  [0.81542232 1.01835029 0.49899895 0.05316818]\n",
            "DATA collection:played for  224  steps , rewards 224.0  last state  [-1.01664866 -0.91331871 -0.3591013  -0.06420287]\n",
            "DATA collection:played for  169  steps , rewards 169.0  last state  [0.8301937  1.002324   0.51121947 0.03733342]\n",
            "DATA collection:played for  167  steps , rewards 167.0  last state  [0.79638181 1.01238582 0.51582496 0.06693666]\n",
            "DATA collection:played for  208  steps , rewards 208.0  last state  [0.75264021 0.87764886 0.50225456 0.14193995]\n",
            "DATA collection:played for  171  steps , rewards 171.0  last state  [ 6.88236409e-01  9.93395267e-01  4.99564700e-01 -6.97349325e-04]\n",
            "DATA collection:played for  225  steps , rewards 225.0  last state  [0.73087021 1.01832214 0.50766482 0.01371701]\n",
            "DATA collection:played for  174  steps , rewards 174.0  last state  [0.77936253 0.9077659  0.50267659 0.15487252]\n",
            "DATA collection:played for  149  steps , rewards 149.0  last state  [0.7296746  0.97772687 0.50508307 0.03911736]\n",
            "DATA collection:played for  227  steps , rewards 227.0  last state  [0.77239054 1.01330421 0.51335272 0.05909892]\n",
            "DATA collection:played for  165  steps , rewards 165.0  last state  [0.77226946 1.01320083 0.50088971 0.05344524]\n",
            "DATA collection:played for  219  steps , rewards 219.0  last state  [0.70555617 0.97598943 0.50011004 0.05772493]\n",
            "DATA collection:played for  217  steps , rewards 217.0  last state  [0.72873987 1.00671902 0.50236044 0.00430149]\n",
            "DATA collection:played for  163  steps , rewards 163.0  last state  [0.71627258 0.99015951 0.50266495 0.05482415]\n",
            "DATA collection:played for  151  steps , rewards 151.0  last state  [0.68454454 0.9894044  0.51035871 0.03126122]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  187  steps , rewards 187.0  last state  [0.73747676 0.99896083 0.50033144 0.05552685]\n",
            "training from  150  games\n",
            "training with   priority  True  training_steps  500  discount  0.99  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [1:06:33<00:00,  7.99s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  500  steps , rewards 500.0  last state  [0.59314968 0.88991618 0.19345972 0.04600579]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.29593778 -0.00586821  0.01846517 -0.04239119]\n",
            "DATA collection:played for  393  steps , rewards 393.0  last state  [1.01303539 1.00088471 0.14616021 0.03719238]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [0.79619761 1.45867667 0.35990486 0.03048519]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.44686455 -0.17686278  0.02441361  0.03529522]\n",
            "DATA collection:played for  455  steps , rewards 455.0  last state  [-0.3210676   0.9965875   0.50005865 -0.01320217]\n",
            "DATA collection:played for  348  steps , rewards 348.0  last state  [-1.00730612 -0.70943965 -0.0263137   0.03188855]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.03787548 -0.01832276 -0.00124356  0.01134402]\n",
            "DATA collection:played for  458  steps , rewards 458.0  last state  [-1.00984804 -0.71389292 -0.00324016  0.06670605]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.21505587 -0.18418933  0.00699026  0.10245669]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.71108493  1.09868963  0.21671401 -0.08000591]\n",
            "DATA collection:played for  358  steps , rewards 358.0  last state  [0.20186996 1.28888424 0.4987491  0.07674336]\n",
            "DATA collection:played for  472  steps , rewards 472.0  last state  [ 1.00460343  1.81382685  0.43821097 -0.05633101]\n",
            "DATA collection:played for  490  steps , rewards 490.0  last state  [-0.60600141  0.9156266   0.50231626  0.03011898]\n",
            "DATA collection:played for  375  steps , rewards 375.0  last state  [1.01082764 1.15651202 0.22270913 0.07971843]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.58690302 -0.17611615  0.05030845 -0.05931268]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.00923227 -0.00897378 -0.01549445  0.00649323]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.02031679  0.91536687  0.37151904 -0.02491042]\n",
            "DATA collection:played for  369  steps , rewards 369.0  last state  [ 1.00869999  1.34111102  0.23779328 -0.01730159]\n",
            "DATA collection:played for  452  steps , rewards 452.0  last state  [1.01056381 1.60715083 0.39213849 0.08074094]\n",
            "DATA collection:played for  476  steps , rewards 476.0  last state  [-1.00709389 -0.33813336  0.0841442  -0.02947237]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.0080594  -0.00714413 -0.00711494  0.00392502]\n",
            "DATA collection:played for  313  steps , rewards 313.0  last state  [-0.35261893  0.98084474  0.51268921  0.02945339]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.82059995  1.63231118  0.43532613 -0.04393861]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.03353105 -0.01447742  0.02016143  0.00495982]\n",
            "DATA collection:played for  395  steps , rewards 395.0  last state  [1.01051937 1.15644451 0.19438879 0.04179525]\n",
            "DATA collection:played for  469  steps , rewards 469.0  last state  [-1.00621451 -0.42448513  0.04902458 -0.0946944 ]\n",
            "DATA collection:played for  380  steps , rewards 380.0  last state  [-1.00144996 -0.55489791  0.01352445 -0.0122233 ]\n",
            "DATA collection:played for  468  steps , rewards 468.0  last state  [-1.00442959 -0.3413084   0.08250588 -0.03114135]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  418  steps , rewards 418.0  last state  [ 1.01149069  1.65610064  0.34448167 -0.11549127]\n",
            "training from  180  games\n",
            "training with   priority  True  training_steps  500  discount  0.99  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [1:09:25<00:00,  8.33s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  197  steps , rewards 197.0  last state  [-0.73762881 -1.64099754  0.56352253  0.91866789]\n",
            "DATA collection:played for  325  steps , rewards 325.0  last state  [-0.49884023 -1.62814812  0.51784947  0.88523195]\n",
            "DATA collection:played for  269  steps , rewards 269.0  last state  [-0.87930138 -1.61110896  0.54383189  0.88769798]\n",
            "DATA collection:played for  326  steps , rewards 326.0  last state  [-0.66197545 -1.50594929  0.62733627  0.94729122]\n",
            "DATA collection:played for  245  steps , rewards 245.0  last state  [-0.72372729 -1.42722239  0.59845591  0.82243986]\n",
            "DATA collection:played for  332  steps , rewards 332.0  last state  [-1.00793217 -1.12133305  0.24754505  0.53850932]\n",
            "DATA collection:played for  281  steps , rewards 281.0  last state  [-1.00026399 -0.47120767 -0.04779461  0.04817821]\n",
            "DATA collection:played for  234  steps , rewards 234.0  last state  [-0.89014979 -1.53344908  0.53043071  0.83865592]\n",
            "DATA collection:played for  214  steps , rewards 214.0  last state  [-1.00507124 -1.35653085  0.55104375  0.80741467]\n",
            "DATA collection:played for  228  steps , rewards 228.0  last state  [-1.00012722 -0.93437136 -0.06031357  0.19653899]\n",
            "DATA collection:played for  312  steps , rewards 312.0  last state  [-0.48164559 -1.32730198  0.52027054  0.8578222 ]\n",
            "DATA collection:played for  273  steps , rewards 273.0  last state  [-0.83151463 -1.23758084  0.59055715  0.74360024]\n",
            "DATA collection:played for  256  steps , rewards 256.0  last state  [-0.88613806 -1.53344017  0.56379505  0.90786047]\n",
            "DATA collection:played for  259  steps , rewards 259.0  last state  [-0.62767444 -1.42551308  0.50413426  0.83429594]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.05389286  0.01850222  0.01837131  0.00997242]\n",
            "DATA collection:played for  242  steps , rewards 242.0  last state  [-0.93396295 -1.54661418  0.51839618  0.86884155]\n",
            "DATA collection:played for  388  steps , rewards 388.0  last state  [-0.67827561 -1.72717399  0.57898083  0.92321204]\n",
            "DATA collection:played for  278  steps , rewards 278.0  last state  [-0.72400917 -1.69223558  0.59143275  0.93870882]\n",
            "DATA collection:played for  303  steps , rewards 303.0  last state  [-0.8181523  -1.61096704  0.57075041  0.93011989]\n",
            "DATA collection:played for  284  steps , rewards 284.0  last state  [-1.00646197 -0.70122885 -0.15117125 -0.05740439]\n",
            "DATA collection:played for  275  steps , rewards 275.0  last state  [-0.91454372 -1.4348551   0.50013387  0.83696148]\n",
            "DATA collection:played for  197  steps , rewards 197.0  last state  [-0.97279311 -1.60726084  0.59022585  0.88852012]\n",
            "DATA collection:played for  244  steps , rewards 244.0  last state  [-0.65366355 -1.51234312  0.51029895  0.8658112 ]\n",
            "DATA collection:played for  307  steps , rewards 307.0  last state  [-1.02370869 -1.77544313  0.48923407  0.88383147]\n",
            "DATA collection:played for  308  steps , rewards 308.0  last state  [-1.01022571 -0.75541357 -0.07821439  0.17279717]\n",
            "DATA collection:played for  372  steps , rewards 372.0  last state  [-0.41404507 -1.89367879  0.57269787  0.96424427]\n",
            "DATA collection:played for  165  steps , rewards 165.0  last state  [-0.81405677 -1.4478247   0.53757959  0.80734101]\n",
            "DATA collection:played for  261  steps , rewards 261.0  last state  [-0.77003101 -1.6060189   0.53658483  0.90921847]\n",
            "DATA collection:played for  183  steps , rewards 183.0  last state  [-0.60215025 -1.44309009  0.51751244  0.87055073]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.07156233  0.01562979 -0.00104719  0.00244889]\n",
            "training from  210  games\n",
            "training with   priority  True  training_steps  500  discount  0.99  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [1:09:59<00:00,  8.40s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  500.0  rewards  last state  [-0.10541763 -0.00337024  0.00434391 -0.00327487]\n",
            "Eval:played for  500.0  rewards  last state  [-0.20016387 -0.01351435  0.00427069  0.00197869]\n",
            "Eval:played for  500.0  rewards  last state  [ 0.43348674  0.16627173  0.00452275 -0.11853752]\n",
            "Eval:played for  500.0  rewards  last state  [ 0.07780369  0.02096713  0.00065556 -0.01546962]\n",
            "Eval:played for  500.0  rewards  last state  [ 0.23348434 -0.7086395  -0.42504316 -0.09239831]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.06216962  0.01997123  0.00931507 -0.01565264]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.08872875 -0.00056369 -0.01155812 -0.00442922]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.17119251 -0.0091487   0.00515743  0.00044782]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.28169098 -0.0229247  -0.00696355  0.00454218]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.0271461   0.0081402   0.00221965 -0.00760441]\n",
            "DATA collection:played for  234  steps , rewards 234.0  last state  [ 1.00527442  0.7547642   0.2030134  -0.03218326]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.45852827 -0.19336415 -0.18278148 -0.00693709]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.03989205  0.00650583  0.00530508 -0.00682956]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.11075687 -0.00421046  0.00105002 -0.00239847]\n",
            "DATA collection:played for  185  steps , rewards 185.0  last state  [-1.01465841 -0.99287369 -0.3782297  -0.05698045]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.01714264  0.00884371  0.00031732 -0.00986029]\n",
            "DATA collection:played for  237  steps , rewards 237.0  last state  [ 1.00061036  0.66339037  0.18146819 -0.02113491]\n",
            "DATA collection:played for  199  steps , rewards 199.0  last state  [-1.01370219 -0.97812904 -0.3957984  -0.09755277]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.17458883 -0.90281719 -0.44233439  0.02611073]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.00912245  0.01274738  0.00898347 -0.01200423]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.29047634 -0.02549535 -0.00251507  0.00859517]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-1.55135500e-01 -1.08840395e-02 -1.54142549e-04 -3.62521502e-03]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.1154221   0.02526833  0.00623732 -0.0144852 ]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.03438151  0.0059868  -0.00810182 -0.00676376]\n",
            "DATA collection:played for  220  steps , rewards 220.0  last state  [-1.01180051 -0.90670733 -0.39608633 -0.10173347]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.09763393  0.02450883  0.00994073 -0.01476696]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.08163889  0.00134259  0.00278286 -0.0042065 ]\n",
            "DATA collection:played for  275  steps , rewards 275.0  last state  [-1.0099806  -0.79358161 -0.29053448 -0.01493337]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.00856148  0.00857449 -0.00497873 -0.009567  ]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.10389731  0.02312016 -0.01101765 -0.01295014]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.23108642 -0.01792724 -0.00232083  0.00634088]\n",
            "DATA collection:played for  205  steps , rewards 205.0  last state  [-1.01604998 -1.00786312 -0.4538778  -0.0816735 ]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.10886151 -0.00340602 -0.01166851 -0.00351995]\n",
            "DATA collection:played for  234  steps , rewards 234.0  last state  [1.00215197 0.56792402 0.15597669 0.0168479 ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.0896609   0.02283968  0.00732635 -0.01506065]\n",
            "training from  240  games\n",
            "training with   priority  True  training_steps  500  discount  0.99  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 11%|█▏        | 57/500 [08:03<1:02:37,  8.48s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frSxLTelrjQ7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}