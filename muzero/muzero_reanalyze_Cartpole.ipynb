{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "muzero reanalyze:Cartpole.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN8IrH+2ajzxAmDmgCfdlwm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rootAkash/reinforcement_learning/blob/master/muzero/muzero_reanalyze_Cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfJ7r4ZLfn6u"
      },
      "source": [
        "#takes too much time to train for reanalyze only advisable if simulation or episode data is costly \n",
        "was only able to do 200-300 batch trainig steps only since it was too slow per batch "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcnQSsnTkUjI"
      },
      "source": [
        "what is reanalyze variant of muzero?\n",
        "off  policy varient of muzero that uses old data by recomputing fresh target values using a target value net and \n",
        "also provides fresh policy targets by recomputing tree search .\n",
        "the priorities will also change because z and root v changed because v function changes during training, z changes because of the new  updated targets.?do we update priorities?recompute search value v?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfUbzbuAWDbM"
      },
      "source": [
        "new experinces have priority pred value and z value  and weights will be automatically(w = ((1/(N*1/N))**beta)) at the start\n",
        "and the l1 loss between predicted value and returns z is calculated from the initial training forward pass and not after backprop for updating priorities\n",
        "but that anyways will require extra loop of converting them to scalars from category outputs to calculate l1 unless vectorised which it is\n",
        "so it will be fastter than looping since catts supports vectors so looping is not nneeded so priority update before backprop is fine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwI18Z6p1oGP"
      },
      "source": [
        "!pip install gym[all]\n",
        "!pip install box2d-py\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFZbm7i9GNao"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "565N2GRAGNas"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQRen3PlNkiB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0afa0b23-6ee8-41ba-fc47-aa1bbbef92cb"
      },
      "source": [
        "import numpy as np\n",
        "def stcat(x,support=15):\n",
        "  x = np.sign(x) * ((abs(x) + 1)**0.5 - 1) + 0.001 * x\n",
        "  x = np.clip(x, -support, support)\n",
        "  floor = np.floor(x)\n",
        "  prob = x - floor\n",
        "  logits = np.zeros( 2 * support + 1)\n",
        "  first_index = int(floor + support)\n",
        "  second_index = int(floor + support+1)\n",
        "  logits[first_index] = 1-prob\n",
        "  if prob>0:\n",
        "    logits[second_index] = prob\n",
        "  return logits\n",
        "#allow for batch processing  \n",
        "def catts(x,support=15):\n",
        "  support = np.arange(-support, support+1, 1)\n",
        "  if len(x.shape)==2:\n",
        "    #for  batch of x\\\n",
        "    x = np.sum(support*x,axis=1)\n",
        "  elif len(x.shape)==1:\n",
        "    #for single x\n",
        "    x = np.sum(support*x)  \n",
        "  else:\n",
        "    print(\"wrong input for conversion to  scalar\")  \n",
        "  x = np.sign(x) * ((((1 + 4 * 0.001 * (np.abs(x) + 1 + 0.001))**0.5 - 1) / (2 * 0.001))** 2- 1)\n",
        "  return x  \n",
        "\n",
        "#cat = stcat(5)#test 1 example\n",
        "cat = np.array([stcat(150),stcat(-150)]) # test batch example\n",
        "print(cat,cat.shape)\n",
        "scalar = catts(cat)\n",
        "print(scalar)\n",
        "print(\"done\")        \n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.56179427 0.43820573 0.         0.\n",
            "  0.        ]\n",
            " [0.         0.         0.         0.43820573 0.56179427 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.        ]] (2, 31)\n",
            "[ 150. -150.]\n",
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhf3dyDa2GYq",
        "outputId": "fa61c7f4-fd3b-40b2-9c34-bd16140af04f"
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MuZeroNet(nn.Module):\n",
        "    def __init__(self, input_size, action_space_n, reward_support_size, value_support_size):\n",
        "        super().__init__()\n",
        "        self.hx_size = 32\n",
        "        self._representation = nn.Sequential(nn.Linear(input_size, self.hx_size),\n",
        "                                             nn.Tanh())\n",
        "        self._dynamics_state = nn.Sequential(nn.Linear(self.hx_size + action_space_n, 64),\n",
        "                                             nn.Tanh(),\n",
        "                                             nn.Linear(64, self.hx_size),\n",
        "                                             nn.Tanh())\n",
        "        self._dynamics_reward = nn.Sequential(nn.Linear(self.hx_size + action_space_n, 64),\n",
        "                                              nn.LeakyReLU(),\n",
        "                                              nn.Linear(64, 2*reward_support_size+1))\n",
        "        self._prediction_actor = nn.Sequential(nn.Linear(self.hx_size, 64),\n",
        "                                               nn.LeakyReLU(),\n",
        "                                               nn.Linear(64, action_space_n))\n",
        "        self._prediction_value = nn.Sequential(nn.Linear(self.hx_size, 64),\n",
        "                                               nn.LeakyReLU(),\n",
        "                                               nn.Linear(64, 2*value_support_size+1))\n",
        "        self.action_space_n = action_space_n\n",
        "\n",
        "        self._prediction_value[-1].weight.data.fill_(0)\n",
        "        self._prediction_value[-1].bias.data.fill_(0)\n",
        "        self._dynamics_reward[-1].weight.data.fill_(0)\n",
        "        self._dynamics_reward[-1].bias.data.fill_(0)\n",
        "\n",
        "    def p(self, state):\n",
        "        actor = torch.softmax(self._prediction_actor(state),dim=1)\n",
        "        value = torch.softmax(self._prediction_value(state),dim=1)\n",
        "        return actor, value\n",
        "\n",
        "    def h(self, obs_history):\n",
        "        return self._representation(obs_history)\n",
        "\n",
        "    def g(self, state, action):\n",
        "        x = torch.cat((state, action), dim=1)\n",
        "        next_state = self._dynamics_state(x)\n",
        "        reward = torch.softmax(self._dynamics_reward(x),dim=1)\n",
        "        return next_state, reward     \n",
        "\n",
        "    def initial_state(self, x):\n",
        "        hout = self.h(x)\n",
        "        prob,v= self.p(hout)\n",
        "        return hout,prob,v\n",
        "    def next_state(self,hin,a):\n",
        "        hout,r = self.g(hin,a)\n",
        "        prob,v= self.p(hout)\n",
        "        return hout,r,prob,v\n",
        "    def inference_initial_state(self, x):\n",
        "        with torch.no_grad():\n",
        "          hout = self.h(x)\n",
        "          prob,v=self.p(hout)\n",
        "\n",
        "          return hout,prob,v\n",
        "    def inference_next_state(self,hin,a):\n",
        "        with torch.no_grad():\n",
        "          hout,r = self.g(hin,a)\n",
        "          prob,v=self.p(hout)\n",
        "          return hout,r,prob,v     \n",
        "\n",
        "\n",
        "print(\"done\")                                      "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkKMTKa6wYtR"
      },
      "source": [
        "\n",
        "#MTCS    MUzero modified for intermeditate rewards settings and using predicted rewards\n",
        "#accepts policy as a list\n",
        "import torch\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "def dynamics(net,state,action):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    #print(state,action) \n",
        "    next_state,reward,prob,value = net.inference_next_state(state.to(device),torch.tensor([action]).float().to(device))\n",
        "    reward = catts(reward.cpu().numpy().ravel())\n",
        "    value = catts(value.cpu().numpy().ravel())\n",
        "    prob = prob.cpu().tolist()[0]\n",
        "    #print(\"dynamics\",prob)\n",
        "    return next_state.cpu(),reward,prob,value\n",
        "\n",
        "\n",
        "class MinMaxStats:\n",
        "    \"\"\"A class that holds the min-max values of the tree.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.MAXIMUM_FLOAT_VALUE = float('inf')       \n",
        "        self.maximum =  -self.MAXIMUM_FLOAT_VALUE\n",
        "        self.minimum =  self.MAXIMUM_FLOAT_VALUE\n",
        "\n",
        "    def update(self, value: float):\n",
        "        if value is None:\n",
        "            raise ValueError\n",
        "\n",
        "        self.maximum = max(self.maximum, value)\n",
        "        self.minimum = min(self.minimum, value)\n",
        "\n",
        "    def normalize(self, value: float) -> float:\n",
        "        # If the value is unknow, by default we set it to the minimum possible value\n",
        "        if value is None:\n",
        "            return 0.0\n",
        "\n",
        "        if self.maximum > self.minimum:\n",
        "            # We normalize only when we have set the maximum and minimum values.\n",
        "            return (value - self.minimum) / (self.maximum - self.minimum)\n",
        "        return value\n",
        "\n",
        "\n",
        "class Node:\n",
        "    \"\"\"A class that represent nodes inside the MCTS tree\"\"\"\n",
        "\n",
        "    def __init__(self, prior: float):\n",
        "        self.visit_count = 0\n",
        "        self.to_play = -1\n",
        "        self.prior = prior\n",
        "        self.value_sum = 0\n",
        "        self.children = {}\n",
        "        self.hidden_state = None\n",
        "        self.reward = 0\n",
        "\n",
        "    def expanded(self):\n",
        "        return len(self.children) > 0\n",
        "\n",
        "    def value(self):\n",
        "        if self.visit_count == 0:\n",
        "            return None\n",
        "        return self.value_sum / self.visit_count\n",
        "\n",
        "\n",
        "def softmax_sample(visit_counts, actions, t):\n",
        "    counts_exp = np.exp(visit_counts) * (1 / t)\n",
        "    probs = counts_exp / np.sum(counts_exp, axis=0)\n",
        "    action_idx = np.random.choice(len(actions), p=probs)\n",
        "    return actions[action_idx]\n",
        "\n",
        "\n",
        "\"\"\"MCTS module: where MuZero thinks inside the tree.\"\"\"\n",
        "\n",
        "\n",
        "def add_exploration_noise( node):\n",
        "    \"\"\"\n",
        "    At the start of each search, we add dirichlet noise to the prior of the root\n",
        "    to encourage the search to explore new actions.\n",
        "    \"\"\"\n",
        "    actions = list(node.children.keys())\n",
        "    noise = np.random.dirichlet([0.25] * len(actions)) # config.root_dirichlet_alpha\n",
        "    frac = 0.25#config.root_exploration_fraction\n",
        "    for a, n in zip(actions, noise):\n",
        "        node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac\n",
        "\n",
        "\n",
        "\n",
        "def ucb_score(parent, child,min_max_stats):\n",
        "    \"\"\"\n",
        "    The score for a node is based on its value, plus an exploration bonus based on\n",
        "    the prior.\n",
        "\n",
        "    \"\"\"\n",
        "    pb_c_base = 19652\n",
        "    pb_c_init = 1.25\n",
        "    pb_c = math.log((parent.visit_count + pb_c_base + 1) / pb_c_base) + pb_c_init\n",
        "    pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
        "\n",
        "    prior_score = pb_c * child.prior\n",
        "    value_score = min_max_stats.normalize(child.value())\n",
        "    return  value_score + prior_score \n",
        "\n",
        "def select_child(node, min_max_stats):\n",
        "    \"\"\"\n",
        "    Select the child with the highest UCB score.\n",
        "    \"\"\"\n",
        "    # When the parent visit count is zero, all ucb scores are zeros, therefore we return a random child\n",
        "    if node.visit_count == 0:\n",
        "        return random.sample(node.children.items(), 1)[0]\n",
        "\n",
        "    _, action, child = max(\n",
        "        (ucb_score(node, child, min_max_stats), action,\n",
        "         child) for action, child in node.children.items())\n",
        "    return action, child\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def expand_node(node, to_play, actions_space,hidden_state,reward,policy):\n",
        "    \"\"\"\n",
        "    We expand a node using the value, reward and policy prediction obtained from\n",
        "    the neural networks.\n",
        "    \"\"\"\n",
        "    node.to_play = to_play\n",
        "    node.hidden_state = hidden_state\n",
        "    node.reward = reward\n",
        "    policy = {a:policy[a] for a in actions_space}\n",
        "    policy_sum = sum(policy.values())\n",
        "    for action, p in policy.items():\n",
        "        node.children[action] = Node(p / policy_sum) # not needed since mine are already softmax but its fine \n",
        "\n",
        "\n",
        "def backpropagate(search_path, value,to_play,discount, min_max_stats):\n",
        "    \"\"\"\n",
        "    At the end of a simulation, we propagate the evaluation all the way up the\n",
        "    tree to the root.\n",
        "    \"\"\"\n",
        "    for node in search_path[::-1]: #[::-1] means reversed\n",
        "        node.value_sum += value \n",
        "        node.visit_count += 1\n",
        "        min_max_stats.update(node.value())\n",
        "\n",
        "        value = node.reward + discount * value\n",
        "\n",
        "\n",
        "def select_action(node, mode ='softmax'):\n",
        "    \"\"\"\n",
        "    After running simulations inside in MCTS, we select an action based on the root's children visit counts.\n",
        "    During training we use a softmax sample for exploration.\n",
        "    During evaluation we select the most visited child.\n",
        "    \"\"\"\n",
        "    visit_counts = [child.visit_count for child in node.children.values()]\n",
        "    actions = [action for action in node.children.keys()]\n",
        "    action = None\n",
        "    if mode == 'softmax':\n",
        "        t = 1.0\n",
        "        action = softmax_sample(visit_counts, actions, t)\n",
        "    elif mode == 'max':\n",
        "        action, _ = max(node.children.items(), key=lambda item: item[1].visit_count)\n",
        "    counts_exp = np.exp(visit_counts)\n",
        "    probs = counts_exp / np.sum(counts_exp, axis=0)    \n",
        "    #return action ,probs,node.value()\n",
        "    return action ,np.array(visit_counts)/sum(visit_counts),node.value()\n",
        "\n",
        "def run_mcts(net, state,prob,root_value,num_simulations,discount = 0.9):\n",
        "    \"\"\"\n",
        "    Core Monte Carlo Tree Search algorithm.\n",
        "    To decide on an action, we run N simulations, always starting at the root of\n",
        "    the search tree and traversing the tree according to the UCB formula until we\n",
        "    reach a leaf node.\n",
        "    \"\"\"\n",
        "    prob, root_value = prob.tolist()[0] ,catts(root_value.numpy().ravel())\n",
        "    to_play = True\n",
        "    action_space=[ i for i in range(len(prob))]#history.action_space()\n",
        "    #print(\"action space\",action_space)\n",
        "    root = Node(0)\n",
        "    expand_node(root, to_play,action_space,state,0.0,prob)#node, to_play, actions_space ,hidden_state,reward,policy\n",
        "    add_exploration_noise( root)\n",
        "\n",
        "\n",
        "    min_max_stats = MinMaxStats()\n",
        "\n",
        "    for _ in range(num_simulations): \n",
        "        node = root\n",
        "        search_path = [node]\n",
        "\n",
        "        while node.expanded():\n",
        "            action, node = select_child( node, min_max_stats)\n",
        "            search_path.append(node)\n",
        "\n",
        "        # Inside the search tree we use the dynamics function to obtain the next\n",
        "        # hidden state given an action and the previous hidden state.\n",
        "        parent = search_path[-2]\n",
        "        \n",
        "        #network_output = network.recurrent_inference(parent.hidden_state, action)\n",
        "        next_state,r,action_probs, value = dynamics(net,parent.hidden_state,onehot(action,len(action_space))) \n",
        "        expand_node(node, to_play, action_space,next_state,r,action_probs)#node, to_play, actions_space ,hidden_state,reward,policy\n",
        "\n",
        "        backpropagate(search_path, value, to_play, discount, min_max_stats)#search_path, value,,discount, min_max_stats\n",
        "    return root    \n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-NpN4lU12kW"
      },
      "source": [
        "import gym\n",
        "class ScalingObservationWrapper(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    Wrapper that apply a min-max scaling of observations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env, low=None, high=None):\n",
        "        super().__init__(env)\n",
        "        assert isinstance(env.observation_space, gym.spaces.Box)\n",
        "\n",
        "        low = np.array(self.observation_space.low if low is None else low)\n",
        "        high = np.array(self.observation_space.high if high is None else high)\n",
        "\n",
        "        self.mean = (high + low) / 2\n",
        "        self.max = high - self.mean\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return (observation - self.mean) / self.max"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waeVGfWytBB1"
      },
      "source": [
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "def onehot(a,n=2):\n",
        "  return np.eye(n)[a]\n",
        "def play_game(env,net,targetnet,n_sim,discount,render,device,n_act,max_steps,td_steps,per):\n",
        "    trajectory=[]\n",
        "    root_values,pred_values,rewards=[],[],[]\n",
        "    state = env.reset() \n",
        "    done = False\n",
        "    r =0 \n",
        "    stp=0\n",
        "    while not done:\n",
        "        if render:\n",
        "          env.render()\n",
        "        stp+=1  \n",
        "        h ,prob,pred_value= net.inference_initial_state(torch.tensor([state]).float().to(device)) \n",
        "        root  = run_mcts(net,h.cpu(),prob.cpu(),pred_value.cpu(),num_simulations=n_sim,discount=discount)\n",
        "        action,action_prob,mcts_val = select_action(root) \n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        r+=reward\n",
        "        if stp>max_steps:\n",
        "          done = True\n",
        "        data = [state,onehot(action,n_act),action_prob,mcts_val,reward,1]#state,onehotaction,action_prob,mcts_val,reward,priority\n",
        "        if targetnet is None:\n",
        "          root_values.append(mcts_val)\n",
        "        else:\n",
        "          #ht ,tarprob,target_pred_value= targetnet.inference_initial_state(torch.tensor([state]).float().to(device))\n",
        "          #root_values.append(catts(target_pred_value.cpu().numpy().ravel()))\n",
        "          root_values.append(catts(pred_value.cpu().numpy().ravel())) # using recent model\n",
        "        pred_values.append(catts(pred_value.cpu().numpy().ravel()))\n",
        "        rewards.append(reward)\n",
        "        trajectory.append(data)\n",
        "        state = next_state\n",
        "    #calculating priority as z - pred value\n",
        "    if per:  \n",
        "      priorities =get_initial_priorities(root_values,pred_values,rewards,discount=discount, td_steps=td_steps)\n",
        "      #update trajectory priority\n",
        "      assert len(trajectory) == len(priorities)\n",
        "      for i in range(len(trajectory)):\n",
        "        trajectory[i][5]=priorities[i]\n",
        "    print(\"DATA collection:played for \",len(trajectory),\" steps , rewards\",r,\" last state \",state)   \n",
        "    return trajectory    \n",
        "def get_initial_priorities(root_values,pred_values,rewards,discount=0.99, td_steps=10):\n",
        "    z_values = []\n",
        "    alpha = 1\n",
        "    beta = 1 \n",
        "    for current_index in range(len(root_values)):\n",
        "        bootstrap_index = current_index + td_steps\n",
        "        if bootstrap_index < len(root_values):\n",
        "            value = root_values[bootstrap_index] * discount ** td_steps\n",
        "        else:\n",
        "            value = 0\n",
        "\n",
        "        for i, reward in enumerate(rewards[current_index:bootstrap_index]):\n",
        "            value += reward * discount ** i\n",
        "\n",
        "        if current_index < len(root_values):\n",
        "            z_values.append(value)\n",
        "    #print(\"get priorities\",pred_values,z_values)        \n",
        "    p = np.abs(np.array(pred_values)-np.array(z_values))**alpha  + 0.00001\n",
        "    #priority = p /np.sum(p)\n",
        "    #N= len(pred_values) \n",
        "    #weights = (1/(N*priority))**beta\n",
        "    return list(p)#,list(weights)\n",
        "def eval_game(env,net,n_sim,render,device,max_steps):\n",
        "    state = env.reset() \n",
        "    done = False\n",
        "    r = 0\n",
        "    stp=0\n",
        "    while not done:\n",
        "        if render:\n",
        "          env.render()\n",
        "        stp+=1  \n",
        "        h ,prob,value= net.inference_initial_state(torch.tensor([state]).float().to(device)) \n",
        "        root  = run_mcts(net,h.cpu(),prob.cpu(),value.cpu(),num_simulations=n_sim,discount=discount)\n",
        "        action,action_prob,mcts_val = select_action(root,\"max\")\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        if stp>max_steps:\n",
        "          done = True\n",
        "        r+=reward\n",
        "        state = next_state\n",
        "    print(\"Eval:played for \",r ,\" rewards\",\" last state \",state)   \n",
        "    \n",
        "def sample_games(buffer,batch_size):\n",
        "    # Sample game from buffer either uniformly or according to some priority\n",
        "    #print(\"samplig from .\",len(buffer))\n",
        "    return list(np.random.choice(len(buffer),batch_size))\n",
        "\n",
        "def sample_position(trajectory,priority=None):\n",
        "    # Sample position from game either uniformly or according to some priority.\n",
        "    if priority == None:\n",
        "      return np.random.choice(len(trajectory),1)[0]\n",
        "    return np.random.choice(len(trajectory),1,p = priority)[0]\n",
        "    #return np.random.choice(list(range(0, len(trajectory))),1,p = priority)[0]\n",
        "\n",
        "def update_priorites(buffer,indexes,new_priority):\n",
        "    #buffer is a list and is passed as refernce so changes made here will reflect in buffer\n",
        "    for i in range(len(indexes)):\n",
        "      x,y = indexes[i]\n",
        "      #old_state,old_onehot_action,old_action_prob,old_mcts_val,old_reward,old_pred_value = buffer[x][y]\n",
        "      #buffer[x][y]=(old_state,old_onehot_action,old_action_prob,old_mcts_val,old_reward,new_pred_values[i])\n",
        "      buffer[x][y][5]=new_priority[i]\n",
        "\n",
        "\n",
        "def sample_batch(model,targetmodel,action_space_size,buffer,discount,batch_size,num_unroll_steps, td_steps,n_sim,per):\n",
        "    obs_batch, action_batch, reward_batch, value_batch, policy_batch,weights_batch = [], [], [], [], [],[]\n",
        "    indexes=[]\n",
        "    game_idx = sample_games(buffer,batch_size)\n",
        "    for gi in game_idx:\n",
        "      g = buffer[gi]\n",
        "      state,action,action_prob,root_val,reward,priority = zip(*g)\n",
        "      state,action,action_prob,root_val,reward,priority  =list(state),list(action),list(action_prob),list(root_val),list(reward),list(priority)\n",
        "      #print(\"pred val sample batch\",priority)\n",
        "      if per:\n",
        "        #make priority for sampling from root_value and n_step value\n",
        "        ps  = np.array(priority)/np.sum(np.array(priority))\n",
        "        game_pos = sample_position(g,list(ps))#state index sampled using priority\n",
        "        beta =1 \n",
        "        N = len(g)\n",
        "        weight =(1/(N*ps[game_pos]))**beta\n",
        "        #N= len(pred_values) \n",
        "        #weights = (1/(N*priority))**beta\n",
        "      else:  \n",
        "        weight = 1.0\n",
        "        game_pos = sample_position(g)#state index sampled using priority\n",
        "      _actions = action[game_pos:game_pos + num_unroll_steps]\n",
        "      # random action selection to complete num_unroll_steps\n",
        "      _actions += [onehot(np.random.randint(0, action_space_size),action_space_size)for _ in range(num_unroll_steps - len(_actions))]\n",
        "\n",
        "      obs_batch.append(state[game_pos])\n",
        "      action_batch.append(_actions)\n",
        "      value, reward, policy = make_target(model=model,target_model=targetmodel,buffer=buffer,gameindex=gi,states_trajectory=state,child_visits=action_prob ,root_values=root_val,\n",
        "                                          rewards=reward,state_index=game_pos,discount=discount, num_unroll_steps=num_unroll_steps, td_steps=td_steps,n_sim=n_sim)\n",
        "      reward_batch.append(reward)\n",
        "      value_batch.append(value)\n",
        "      policy_batch.append(policy)\n",
        "      weights_batch.append(weight)\n",
        "      indexes.append((gi,game_pos))\n",
        "\n",
        "\n",
        "\n",
        "    obs_batch = torch.tensor(obs_batch).float()\n",
        "    action_batch = torch.tensor(action_batch).long()\n",
        "    reward_batch = torch.tensor(reward_batch).float()\n",
        "    value_batch = torch.tensor(value_batch).float()\n",
        "    policy_batch = torch.tensor(policy_batch).float()\n",
        "    weights_batch = torch.tensor(weights_batch).float()\n",
        "    return obs_batch, action_batch, reward_batch, value_batch, policy_batch,weights_batch,indexes\n",
        "\n",
        "\n",
        "def make_target(model,target_model,buffer,gameindex,states_trajectory,child_visits,root_values,rewards,state_index,discount=0.99, num_unroll_steps=5, td_steps=5,n_sim=50):\n",
        "        # The value target is the discounted root value of the search tree or value by target network N steps into the future, plus\n",
        "        # the discounted sum of all rewards until then.\n",
        "        target_values, target_rewards, target_policies = [], [], []\n",
        "        for current_index in range(state_index, state_index + num_unroll_steps + 1):\n",
        "            bootstrap_index = current_index + td_steps\n",
        "            if bootstrap_index < len(root_values):\n",
        "                if target_model is None:\n",
        "                    value = root_values[bootstrap_index] * discount ** td_steps\n",
        "                else:\n",
        "                    #  a target network  based on recent parameters is used to provide a fresher,\n",
        "                    # stable n-step bootstrapped target for the value function\n",
        "                    obs = states_trajectory[bootstrap_index]\n",
        "                    #ht ,tarprob,target_pred_value= target_model.inference_initial_state(torch.tensor([obs]).float().to(device))\n",
        "                    ht ,tarprob,target_pred_value= model.inference_initial_state(torch.tensor([obs]).float().to(device))#try recent model (should be same as hard target update freq as 1  but this works well and that doesn't )\n",
        "                    value=catts(target_pred_value.cpu().numpy().ravel()) * discount ** td_steps\n",
        "            else:\n",
        "                value = 0\n",
        "\n",
        "            for i, reward in enumerate(rewards[current_index:bootstrap_index]):\n",
        "                value += reward * discount ** i\n",
        "\n",
        "            if current_index < len(root_values):\n",
        "                target_values.append(stcat(value))\n",
        "                target_rewards.append(stcat(rewards[current_index]))\n",
        "                if target_model is not None and np.random.random() <= 0.8: \n",
        "                    #we recompute policy for current_index using latest params model  \n",
        "                    #and then change it in buffer also use it as target policy 80 percent of time to keep labels stable\n",
        "                    obs = states_trajectory[current_index]\n",
        "                    h ,prob,pred_value= model.inference_initial_state(torch.tensor([obs]).float().to(device)) ##############################################\n",
        "                    root  = run_mcts(model,h.cpu(),prob.cpu(),pred_value.cpu(),num_simulations=n_sim,discount=discount)\n",
        "                    action,action_prob,mcts_val = select_action(root) \n",
        "                    buffer[gameindex][current_index][2]=action_prob #only change this rest values depend on the original trajectory\n",
        "                    child_visits[current_index] =action_prob\n",
        "\n",
        "                target_policies.append(child_visits[current_index])\n",
        "\n",
        "            else:\n",
        "                # States past the end of games are treated as absorbing states.\n",
        "                target_values.append(stcat(0))\n",
        "                target_rewards.append(stcat(0))\n",
        "                # Note: Target policy is  set to 0 so that no policy loss is calculated for them\n",
        "                #target_policies.append([0 for _ in range(len(child_visits[0]))])\n",
        "                target_policies.append(child_visits[0]*0.0)\n",
        "\n",
        "        return target_values, target_rewards, target_policies\n",
        "\n",
        "\n",
        "def scalar_reward_loss( prediction, target):\n",
        "        return -(torch.log(prediction) * target).sum(1)\n",
        "\n",
        "def scalar_value_loss( prediction, target):\n",
        "        return -(torch.log(prediction) * target).sum(1)\n",
        "def update_weights(model,targetmodel,action_space_size, optimizer, replay_buffer,discount,batch_size,num_unroll_steps, td_steps,n_sim,per ):\n",
        "    batch = sample_batch(model,targetmodel,action_space_size,replay_buffer,discount,batch_size,num_unroll_steps, td_steps,n_sim,per)\n",
        "    obs_batch, action_batch, target_reward, target_value, target_policy,target_weights,indexes = batch\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    obs_batch = obs_batch.to(device)\n",
        "    action_batch = action_batch.to(device) \n",
        "    target_reward = target_reward.to(device)\n",
        "    target_value = target_value.to(device)\n",
        "    target_policy = target_policy.to(device)\n",
        "    target_weights = target_weights.to(device)\n",
        "    target_reward_phi =target_reward \n",
        "    target_value_phi = target_value\n",
        "\n",
        "    hidden_state, policy_prob,value  = model.initial_state(obs_batch) # initial model_call #\n",
        "    \n",
        "    value_loss = scalar_value_loss(value, target_value_phi[:, 0])\n",
        "    policy_loss = -(torch.log(policy_prob) * target_policy[:, 0]).sum(1)\n",
        "    reward_loss = torch.zeros(batch_size, device=device)\n",
        "    initial_state_values = value.detach()\n",
        "    gradient_scale = 1 / num_unroll_steps\n",
        "    for step_i in range(num_unroll_steps):\n",
        "        hidden_state, reward,policy_prob,value  = model.next_state(hidden_state, action_batch[:, step_i]) \n",
        "        #h,pred_reward,pred_policy,pred_value= net.next_state(h,act)\n",
        "        policy_loss += -(torch.log(policy_prob) * target_policy[:, step_i + 1]).sum(1)\n",
        "        value_loss += scalar_value_loss(value, target_value_phi[:, step_i + 1])\n",
        "        reward_loss += scalar_reward_loss(reward, target_reward_phi[:, step_i])\n",
        "        hidden_state.register_hook(lambda grad: grad * 0.5)\n",
        "\n",
        "    # optimize\n",
        "    if targetmodel is None:\n",
        "      value_loss_coeff = 1\n",
        "    else:  \n",
        "      value_loss_coeff = 0.25 #to reduce value overfiiting due to off policy \n",
        "    loss = (policy_loss + value_loss_coeff * value_loss + reward_loss) # find value loss coefficiet = 1?\n",
        "    weights = target_weights#/target_weights.max()#dividing by max doesnt work\n",
        "    weighted_loss = (weights * loss).mean()#1?\n",
        "    weighted_loss.register_hook(lambda grad: grad * gradient_scale)\n",
        "    loss = loss.mean()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    weighted_loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "    optimizer.step()\n",
        "    if per:\n",
        "      #remvoing 2nd forward pass can do it also should be chill???\n",
        "      #updated_h,updated_prob,updated_pred_value= model.inference_initial_state(obs_batch) \n",
        "      #return indexes,updated_pred_value.cpu().numpy()\n",
        "      return indexes,np.abs(catts(initial_state_values.cpu().numpy())-catts(target_value[:, 0].cpu().numpy())) +0.00001\n",
        "    return None,None  \n",
        "\n",
        "def adjust_lr(optimizer, step_count):\n",
        "\n",
        "    lr_init=0.05\n",
        "    lr_decay_rate=0.01\n",
        "    lr_decay_steps=10000\n",
        "    lr = lr_init * lr_decay_rate ** (step_count / lr_decay_steps)\n",
        "    lr = max(lr, 0.001)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    return lr\n",
        "def soft_update(target, source, tau):\n",
        "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
        "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
        "    return target\n",
        "def get_scalars(new_pred_values):\n",
        "    vals = []\n",
        "    for i in range(new_pred_values.shape[0]):\n",
        "      #print(new_pred_values[i,:].shape)\n",
        "      vals.append(catts(new_pred_values[i,:]))\n",
        "    return vals\n",
        "learning_rate = [0.05]   \n",
        "stp=[0]\n",
        "def net_train(net, targetnet, action_space_size, replay_buffer,discount,batch_size,num_unroll_steps, td_steps,training_steps=1000,target_update=50,tou=1,n_sim=50,per = False):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model =net\n",
        "    target_model = targetnet\n",
        "    #MuZeroNet(input_size=4, action_space_n=2, reward_support_size=5, value_support_size=5).to(device) #training fresh net\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate[0], momentum=0.9,weight_decay=1e-4)\n",
        "    #training_steps=training_steps=500#20000\n",
        "    # wait for replay buffer to be non-empty\n",
        "    while len(replay_buffer) == 0:\n",
        "        pass\n",
        "\n",
        "    for step_count in tqdm(range(training_steps)):\n",
        "        stp[0]+=1\n",
        "        learning_rate[0] = adjust_lr( optimizer, step_count)\n",
        "        indexes,new_priority = update_weights(model,target_model, action_space_size, optimizer, replay_buffer,discount,batch_size,num_unroll_steps, td_steps,n_sim,per)\n",
        "        if target_model is not None:\n",
        "          if stp[0] % target_update==0:\n",
        "            #print(\"softupdate \", tou)\n",
        "            soft_update(target=target_model, source=model, tau=tou)\n",
        "        if per:\n",
        "          #print(\"new pred val net train\",new_pred_values,new_pred_values.shape)\n",
        "          #new_pred_values = get_scalars(new_pred_values)\n",
        "          #print(\"new pred val net train\",new_pred_values)\n",
        "          update_priorites(replay_buffer,indexes,new_priority)\n",
        "\n",
        "    return model,target_model\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZWra51wFVvb",
        "outputId": "6624e920-497e-4a67-c44e-c0af3ae07626"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "render = False\n",
        "episodes_per_train=30\n",
        "episodes_per_eval =5\n",
        "buffer =[]\n",
        "#buffer = deque(maxlen = episodes_per_train)\n",
        "training_steps=50\n",
        "max_steps=5000\n",
        "n_sim= 25\n",
        "discount = 0.99\n",
        "target_update_frewq = 1\n",
        "update_frac=0.01 # hard update\n",
        "batch_size = 126\n",
        "envs = ['CartPole-v1','MountainCar-v0','LunarLander-v2']\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"training for \",envs[0])\n",
        "env=gym.make(envs[0])\n",
        "#env=env.unwrapped\n",
        "env = ScalingObservationWrapper(env, low=[-2.4, -2.0, -0.42, -3.5], high=[2.4, 2.0, 0.42, 3.5])#only for cartpole\n",
        "\n",
        "s_dim =env.observation_space.shape[0]\n",
        "print(\"s_dim: \",s_dim)\n",
        "a_dim =env.action_space.n\n",
        "print(\"a_dim: \",a_dim)\n",
        "a_bound =1 #env.action_space.high[0]\n",
        "print(\"a_bound: \",a_bound)\n",
        "\n",
        "\n",
        "\n",
        "net = MuZeroNet(input_size=s_dim, action_space_n=a_dim, reward_support_size=15, value_support_size=15).to(device)\n",
        "targetnet = MuZeroNet(input_size=s_dim, action_space_n=a_dim, reward_support_size=15, value_support_size=15).to(device) #None for not using reanalyze\n",
        "targetnet = soft_update(target=targetnet, source=net, tau=1)#make them same\n",
        "for t in range(training_steps):\n",
        "  if t<0:\n",
        "    priority = True \n",
        "    tr_stp=20\n",
        "  else :\n",
        "    tr_stp=300\n",
        "    priority = True \n",
        "  if targetnet is None:\n",
        "    buffer =[] # onpolicy \n",
        "  for _ in range(episodes_per_train):\n",
        "    buffer.append(play_game(env,net,targetnet,n_sim,discount,render,device,a_dim,max_steps,td_steps=5,per=priority))\n",
        "  print(\"training from \",len(buffer),\" games\")  \n",
        "\n",
        "  print(\"training with \",\" priority \",priority,\" training_steps \",tr_stp,\" discount \",discount,\" batch_size \",batch_size)  \n",
        "  net,targetnet = net_train(net,targetnet,action_space_size=a_dim, replay_buffer=buffer,discount=discount,batch_size=batch_size,num_unroll_steps=5, \n",
        "                            td_steps=5,training_steps=tr_stp,target_update=target_update_frewq,tou=update_frac,n_sim=n_sim,per = priority)\n",
        "  if t>5:\n",
        "    for _ in range(episodes_per_eval):\n",
        "      eval_game(env,net,n_sim,render,device,max_steps)\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training for  CartPole-v1\n",
            "s_dim:  4\n",
            "a_dim:  2\n",
            "a_bound:  1\n",
            "DATA collection:played for  12  steps , rewards 12.0  last state  [-0.04671572 -0.58879377  0.57494821  0.60653748]\n",
            "DATA collection:played for  12  steps , rewards 12.0  last state  [-0.06979179 -0.60384381  0.55107236  0.57563857]\n",
            "DATA collection:played for  14  steps , rewards 14.0  last state  [-0.07669188 -0.61196448  0.50779621  0.56610597]\n",
            "DATA collection:played for  15  steps , rewards 15.0  last state  [-0.05426604 -0.47388567  0.591833    0.51083183]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.0384441  -0.85875786  0.61324746  0.81997295]\n",
            "DATA collection:played for  24  steps , rewards 24.0  last state  [-0.06161046 -0.56435644  0.54508524  0.52688315]\n",
            "DATA collection:played for  27  steps , rewards 27.0  last state  [-0.07884229 -0.3138262   0.54689502  0.35680871]\n",
            "DATA collection:played for  21  steps , rewards 21.0  last state  [-0.03751597 -0.70214002  0.51079924  0.68084745]\n",
            "DATA collection:played for  24  steps , rewards 24.0  last state  [-0.05534229 -0.20797468  0.52026907  0.29477688]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.05274362 -0.68408514  0.55189839  0.6507948 ]\n",
            "DATA collection:played for  21  steps , rewards 21.0  last state  [-0.09839718 -0.48367075  0.54729861  0.45016403]\n",
            "DATA collection:played for  12  steps , rewards 12.0  last state  [-0.03554243 -0.4052085   0.53185055  0.43588346]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.03717632 -0.5713629   0.57097815  0.5874941 ]\n",
            "DATA collection:played for  14  steps , rewards 14.0  last state  [-0.04265555 -0.22459223  0.52956648  0.28687314]\n",
            "DATA collection:played for  36  steps , rewards 36.0  last state  [-0.06296375 -0.39223219  0.5352699   0.43750872]\n",
            "DATA collection:played for  22  steps , rewards 22.0  last state  [-0.0837036  -0.20844951  0.52641061  0.27429845]\n",
            "DATA collection:played for  21  steps , rewards 21.0  last state  [-0.04751296 -0.67794207  0.60978549  0.66990131]\n",
            "DATA collection:played for  26  steps , rewards 26.0  last state  [-0.07647143 -0.8027484   0.52125181  0.68911674]\n",
            "DATA collection:played for  32  steps , rewards 32.0  last state  [-0.06432874 -0.61846155  0.55466309  0.6313105 ]\n",
            "DATA collection:played for  27  steps , rewards 27.0  last state  [-0.05847667 -0.49636285  0.52395615  0.46249843]\n",
            "DATA collection:played for  24  steps , rewards 24.0  last state  [-0.03192756 -0.37552676  0.50099123  0.42661228]\n",
            "DATA collection:played for  28  steps , rewards 28.0  last state  [-0.07399099 -0.59546465  0.53066559  0.54591393]\n",
            "DATA collection:played for  17  steps , rewards 17.0  last state  [-0.04063463 -0.48362978  0.55266145  0.52569843]\n",
            "DATA collection:played for  14  steps , rewards 14.0  last state  [-0.03121004 -0.20690454  0.5235339   0.29797988]\n",
            "DATA collection:played for  36  steps , rewards 36.0  last state  [-0.06296252 -0.393024    0.54134195  0.46206769]\n",
            "DATA collection:played for  13  steps , rewards 13.0  last state  [-0.07321222 -0.68359775  0.54047602  0.62147187]\n",
            "DATA collection:played for  15  steps , rewards 15.0  last state  [-0.05215718 -0.50230694  0.57345534  0.55341958]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.08134144 -0.79146854  0.49938107  0.68582026]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/300 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  39  steps , rewards 39.0  last state  [0.03538302 0.03856016 0.53448264 0.37585634]\n",
            "DATA collection:played for  16  steps , rewards 16.0  last state  [-0.05494382 -0.59923144  0.50671841  0.57462625]\n",
            "training from  30  games\n",
            "training with   priority  True  training_steps  300  discount  0.99  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [30:00<00:00,  6.00s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.05576791 -0.9851684   0.59798784  0.87191745]\n",
            "DATA collection:played for  11  steps , rewards 11.0  last state  [-0.0792023  -1.07718834  0.63376174  0.94250613]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.0676274  -0.98930723  0.57332876  0.88149227]\n",
            "DATA collection:played for  8  steps , rewards 8.0  last state  [-0.03092285 -0.79189416  0.5033238   0.72838417]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.08664857 -0.96373608  0.54499     0.85561883]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.05503727 -0.98834612  0.59197225  0.87277286]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.05044912 -0.95660373  0.59294824  0.88682658]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.06015094 -0.98772494  0.4987804   0.84359539]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.06940669 -0.86812582  0.61262672  0.82543343]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.05397512 -0.86420386  0.53378703  0.79062463]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.06556511 -0.90347988  0.59667958  0.80891369]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.04669583 -0.88456541  0.52752486  0.79395175]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.05573942 -0.89490317  0.52223009  0.79568871]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.06948512 -0.98718278  0.54174625  0.85489643]\n",
            "DATA collection:played for  8  steps , rewards 8.0  last state  [-0.0455454  -0.80142527  0.54245785  0.74407961]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.06192003 -0.86796712  0.54528804  0.81001739]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.05473809 -0.89144073  0.53759981  0.80738919]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.08215406 -1.00272647  0.60764977  0.87646301]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.0757368  -0.97048431  0.58540927  0.87498271]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.05741602 -0.90420489  0.58382909  0.80421062]\n",
            "DATA collection:played for  8  steps , rewards 8.0  last state  [-0.04650846 -0.80223824  0.51036851  0.72646843]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.07902505 -0.8939958   0.51365361  0.80032036]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.0552173  -0.89214225  0.50727562  0.7921235 ]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.05535394 -0.85978386  0.54147944  0.79395452]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.0682496  -0.87405939  0.54556518  0.80518567]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.07346257 -0.89573912  0.61557764  0.81627126]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.06219152 -0.98620659  0.60536527  0.88111265]\n",
            "DATA collection:played for  8  steps , rewards 8.0  last state  [-0.04855375 -0.78422833  0.51331032  0.7302152 ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/300 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.06523867 -0.95992881  0.62196426  0.87825722]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.06298875 -0.89484466  0.59903891  0.818315  ]\n",
            "training from  60  games\n",
            "training with   priority  True  training_steps  300  discount  0.99  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [26:52<00:00,  5.38s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  11  steps , rewards 11.0  last state  [ 0.09400378  1.05706539 -0.62171254 -0.93987155]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.06871177  0.99991756 -0.53715658 -0.85308117]\n",
            "DATA collection:played for  8  steps , rewards 8.0  last state  [ 0.06126963  0.76782099 -0.52350065 -0.73455175]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.05726215  0.90390191 -0.56582392 -0.80792622]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.05549708  0.95535589 -0.59568759 -0.87158445]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.05622886  0.95314705 -0.54627552 -0.85863163]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.07091883  0.85888502 -0.50760225 -0.79281648]\n",
            "DATA collection:played for  8  steps , rewards 8.0  last state  [ 0.05638984  0.7780898  -0.5292299  -0.73662374]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.05928817  0.90277997 -0.513387   -0.79192639]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.08904853  0.97686826 -0.57291733 -0.87213387]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.06130232  0.88450877 -0.60252038 -0.82520634]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.06272636  0.90007882 -0.57433258 -0.80087337]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.07006326  0.87054629 -0.57296473 -0.81963385]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.08175761  0.97706869 -0.57769979 -0.87954499]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.06489028  0.95280123 -0.58743727 -0.87977393]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.05312186  0.85735868 -0.5542274  -0.80690611]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.04413732  0.89776649 -0.54964257 -0.79288208]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.03706914  0.87017253 -0.51107454 -0.78563563]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.05336407  0.8677143  -0.60843037 -0.81146886]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.07686902  0.96032512 -0.61007929 -0.88197627]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.07402389  0.97719306 -0.52611282 -0.86312568]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.045348    0.89335679 -0.52680408 -0.79566123]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.05400988  0.96259054 -0.51331508 -0.84879186]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.09248214  0.99070155 -0.53824297 -0.85700387]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.05452196  0.90207371 -0.50163712 -0.78886608]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.0617943   0.95888884 -0.57717688 -0.86587279]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.07486947  0.88154198 -0.59439652 -0.82550031]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.06713003  0.96197973 -0.60330862 -0.8783191 ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/300 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  11  steps , rewards 11.0  last state  [ 0.0880832   1.07279331 -0.62410962 -0.93775831]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.05316271  0.98129719 -0.52071313 -0.84784616]\n",
            "training from  90  games\n",
            "training with   priority  True  training_steps  300  discount  0.99  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [25:50<00:00,  5.17s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  181  steps , rewards 181.0  last state  [0.24918869 0.62321786 0.50984264 0.04102695]\n",
            "DATA collection:played for  141  steps , rewards 141.0  last state  [0.20048946 0.62503921 0.51163898 0.06598153]\n",
            "DATA collection:played for  81  steps , rewards 81.0  last state  [0.28366871 0.45595972 0.49968148 0.21180159]\n",
            "DATA collection:played for  96  steps , rewards 96.0  last state  [0.26449499 0.53264968 0.51594712 0.10421272]\n",
            "DATA collection:played for  99  steps , rewards 99.0  last state  [0.36762434 0.8151081  0.50471966 0.01271193]\n",
            "DATA collection:played for  186  steps , rewards 186.0  last state  [-0.53962054 -0.55376614 -0.51014909 -0.0957066 ]\n",
            "DATA collection:played for  87  steps , rewards 87.0  last state  [0.2479692  0.6532983  0.50612209 0.01286136]\n",
            "DATA collection:played for  102  steps , rewards 102.0  last state  [0.27732359 0.55924618 0.51205015 0.13767239]\n",
            "DATA collection:played for  99  steps , rewards 99.0  last state  [0.2988461  0.62886015 0.50729324 0.09955511]\n",
            "DATA collection:played for  127  steps , rewards 127.0  last state  [0.31406025 0.61507786 0.50826206 0.06654632]\n",
            "DATA collection:played for  155  steps , rewards 155.0  last state  [0.15121901 0.62914147 0.50348489 0.01675607]\n",
            "DATA collection:played for  173  steps , rewards 173.0  last state  [-0.45395687 -0.46347148 -0.50601813 -0.18394657]\n",
            "DATA collection:played for  107  steps , rewards 107.0  last state  [0.26631045 0.61204133 0.51506969 0.09545208]\n",
            "DATA collection:played for  89  steps , rewards 89.0  last state  [0.28022931 0.6374551  0.51878148 0.10998975]\n",
            "DATA collection:played for  199  steps , rewards 199.0  last state  [0.28171653 0.62870608 0.50922711 0.12333019]\n",
            "DATA collection:played for  161  steps , rewards 161.0  last state  [-0.41113884 -0.64539512 -0.50628437 -0.00489782]\n",
            "DATA collection:played for  88  steps , rewards 88.0  last state  [0.24371338 0.54252016 0.51425162 0.13485941]\n",
            "DATA collection:played for  142  steps , rewards 142.0  last state  [0.22269814 0.53480469 0.50402807 0.08565555]\n",
            "DATA collection:played for  170  steps , rewards 170.0  last state  [-0.42771166 -0.54394371 -0.51565131 -0.09258874]\n",
            "DATA collection:played for  121  steps , rewards 121.0  last state  [0.32352047 0.61545089 0.50918533 0.10414593]\n",
            "DATA collection:played for  107  steps , rewards 107.0  last state  [0.24148077 0.46447625 0.50958729 0.16962105]\n",
            "DATA collection:played for  109  steps , rewards 109.0  last state  [0.31240435 0.60831687 0.53012685 0.16650519]\n",
            "DATA collection:played for  80  steps , rewards 80.0  last state  [0.30249542 0.70056792 0.51629458 0.07279757]\n",
            "DATA collection:played for  80  steps , rewards 80.0  last state  [0.2796047  0.74100737 0.50077859 0.00786209]\n",
            "DATA collection:played for  102  steps , rewards 102.0  last state  [0.32864043 0.71481592 0.51732531 0.07800381]\n",
            "DATA collection:played for  118  steps , rewards 118.0  last state  [0.27325536 0.53545461 0.50754609 0.17033418]\n",
            "DATA collection:played for  95  steps , rewards 95.0  last state  [0.25408941 0.61157139 0.51304821 0.06543036]\n",
            "DATA collection:played for  74  steps , rewards 74.0  last state  [0.25368067 0.53245948 0.50903435 0.12579918]\n",
            "DATA collection:played for  99  steps , rewards 99.0  last state  [0.353482   0.6476731  0.50325853 0.12904051]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/300 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  84  steps , rewards 84.0  last state  [0.35522922 0.74629997 0.5085155  0.02902224]\n",
            "training from  120  games\n",
            "training with   priority  True  training_steps  300  discount  0.99  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [28:39<00:00,  5.73s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  54  steps , rewards 54.0  last state  [-0.08325277 -0.3570433  -0.51227407 -0.13759328]\n",
            "DATA collection:played for  63  steps , rewards 63.0  last state  [-0.06852884 -0.46773972 -0.51067617 -0.03098211]\n",
            "DATA collection:played for  81  steps , rewards 81.0  last state  [-0.10206727 -0.27266526 -0.509754   -0.20163106]\n",
            "DATA collection:played for  72  steps , rewards 72.0  last state  [-0.08462283 -0.33946504 -0.51422243 -0.13180995]\n",
            "DATA collection:played for  56  steps , rewards 56.0  last state  [-0.10158038 -0.3772004  -0.51969404 -0.10438769]\n",
            "DATA collection:played for  88  steps , rewards 88.0  last state  [-0.10938626 -0.33661611 -0.49898818 -0.15404939]\n",
            "DATA collection:played for  62  steps , rewards 62.0  last state  [-0.15752675 -0.51898748 -0.50495452 -0.04711335]\n",
            "DATA collection:played for  72  steps , rewards 72.0  last state  [-0.04861278 -0.33494778 -0.51097633 -0.11848272]\n",
            "DATA collection:played for  95  steps , rewards 95.0  last state  [-0.20137019 -0.45235672 -0.51851518 -0.14505075]\n",
            "DATA collection:played for  63  steps , rewards 63.0  last state  [-0.14172586 -0.45747791 -0.52011198 -0.0967113 ]\n",
            "DATA collection:played for  64  steps , rewards 64.0  last state  [-0.13346144 -0.36199013 -0.51291899 -0.15120921]\n",
            "DATA collection:played for  69  steps , rewards 69.0  last state  [-0.13948999 -0.45442436 -0.52090131 -0.09860317]\n",
            "DATA collection:played for  60  steps , rewards 60.0  last state  [-0.11648138 -0.3509344  -0.51548896 -0.12784994]\n",
            "DATA collection:played for  58  steps , rewards 58.0  last state  [-0.16998856 -0.54238734 -0.50678205 -0.06343342]\n",
            "DATA collection:played for  84  steps , rewards 84.0  last state  [-0.10219659 -0.52640828 -0.51198834 -0.02249529]\n",
            "DATA collection:played for  62  steps , rewards 62.0  last state  [-0.14704936 -0.54276047 -0.51395618 -0.04269236]\n",
            "DATA collection:played for  84  steps , rewards 84.0  last state  [-0.06729074 -0.3602367  -0.52517762 -0.11215208]\n",
            "DATA collection:played for  72  steps , rewards 72.0  last state  [-0.16105015 -0.34568853 -0.49951125 -0.23763373]\n",
            "DATA collection:played for  83  steps , rewards 83.0  last state  [-0.18910938 -0.43289593 -0.5287123  -0.14321969]\n",
            "DATA collection:played for  57  steps , rewards 57.0  last state  [-0.09572695 -0.42792682 -0.51124712 -0.07168997]\n",
            "DATA collection:played for  80  steps , rewards 80.0  last state  [-0.12836935 -0.34467985 -0.51447138 -0.1576798 ]\n",
            "DATA collection:played for  88  steps , rewards 88.0  last state  [-0.16592195 -0.53087148 -0.52039492 -0.07374048]\n",
            "DATA collection:played for  50  steps , rewards 50.0  last state  [-0.09711024 -0.35442039 -0.51656445 -0.11693362]\n",
            "DATA collection:played for  75  steps , rewards 75.0  last state  [-0.18338285 -0.45895487 -0.5084945  -0.13210087]\n",
            "DATA collection:played for  63  steps , rewards 63.0  last state  [-0.12636867 -0.43105459 -0.50405776 -0.07570495]\n",
            "DATA collection:played for  72  steps , rewards 72.0  last state  [-0.08084649 -0.34333014 -0.50316469 -0.11520387]\n",
            "DATA collection:played for  98  steps , rewards 98.0  last state  [-0.13902221 -0.55460165 -0.50625454 -0.0098539 ]\n",
            "DATA collection:played for  62  steps , rewards 62.0  last state  [-0.15748091 -0.5404617  -0.50997619 -0.07064792]\n",
            "DATA collection:played for  150  steps , rewards 150.0  last state  [-0.08941843 -0.52392922 -0.50762076 -0.03192075]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/300 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  121  steps , rewards 121.0  last state  [-0.0994271  -0.43283884 -0.49901601 -0.10316005]\n",
            "training from  150  games\n",
            "training with   priority  True  training_steps  300  discount  0.99  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [29:50<00:00,  5.97s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  172  steps , rewards 172.0  last state  [-1.00041123 -0.91329075 -0.29629544 -0.15130128]\n",
            "DATA collection:played for  162  steps , rewards 162.0  last state  [-1.00116714 -0.92093178 -0.29272088 -0.08726568]\n",
            "DATA collection:played for  180  steps , rewards 180.0  last state  [-1.00207038 -0.89129503 -0.26935994 -0.12384313]\n",
            "DATA collection:played for  184  steps , rewards 184.0  last state  [-1.01176329 -1.07346853 -0.33132445 -0.02805972]\n",
            "DATA collection:played for  179  steps , rewards 179.0  last state  [-1.01321688 -0.9835343  -0.36091627 -0.04467241]\n",
            "DATA collection:played for  175  steps , rewards 175.0  last state  [-1.01397186 -1.01412786 -0.24698044  0.03640255]\n",
            "DATA collection:played for  154  steps , rewards 154.0  last state  [-1.00859846 -0.90593229 -0.31559967 -0.11058749]\n",
            "DATA collection:played for  159  steps , rewards 159.0  last state  [-1.00633774 -1.01312432 -0.33273173 -0.0978843 ]\n",
            "DATA collection:played for  158  steps , rewards 158.0  last state  [-1.00707754 -1.10469815 -0.32521151  0.01424368]\n",
            "DATA collection:played for  163  steps , rewards 163.0  last state  [-1.00391163 -0.99246067 -0.35803804 -0.0728456 ]\n",
            "DATA collection:played for  179  steps , rewards 179.0  last state  [-1.00979067 -0.97934433 -0.27003245  0.01183583]\n",
            "DATA collection:played for  165  steps , rewards 165.0  last state  [-1.00550215 -1.01111821 -0.27488424 -0.03060494]\n",
            "DATA collection:played for  180  steps , rewards 180.0  last state  [-1.00986072 -0.88225934 -0.29016646 -0.06822552]\n",
            "DATA collection:played for  154  steps , rewards 154.0  last state  [-1.0155719  -1.11379445 -0.33897351  0.0221174 ]\n",
            "DATA collection:played for  177  steps , rewards 177.0  last state  [-1.01152605 -0.98600109 -0.32920145 -0.08756071]\n",
            "DATA collection:played for  178  steps , rewards 178.0  last state  [-1.0131764  -1.1150926  -0.32354024  0.06095049]\n",
            "DATA collection:played for  173  steps , rewards 173.0  last state  [-1.01690145 -0.97098202 -0.28983687 -0.05658442]\n",
            "DATA collection:played for  183  steps , rewards 183.0  last state  [-1.00469092 -1.15950696 -0.34559093  0.0088312 ]\n",
            "DATA collection:played for  194  steps , rewards 194.0  last state  [-1.0052307  -1.2641444  -0.38613238  0.11590895]\n",
            "DATA collection:played for  182  steps , rewards 182.0  last state  [-1.01014381 -0.914747   -0.21936865  0.00553219]\n",
            "DATA collection:played for  178  steps , rewards 178.0  last state  [-1.01860012 -1.08609617 -0.34954598 -0.01514563]\n",
            "DATA collection:played for  172  steps , rewards 172.0  last state  [-1.0003767  -0.90980679 -0.29357775 -0.114338  ]\n",
            "DATA collection:played for  166  steps , rewards 166.0  last state  [-1.00186398 -0.91749464 -0.25715288 -0.05811471]\n",
            "DATA collection:played for  168  steps , rewards 168.0  last state  [-1.01150946 -0.92685873 -0.28469759 -0.05185646]\n",
            "DATA collection:played for  164  steps , rewards 164.0  last state  [-1.0070591  -0.89772666 -0.29835913 -0.15785566]\n",
            "DATA collection:played for  180  steps , rewards 180.0  last state  [-1.01647158 -1.09056495 -0.29696999  0.06094635]\n",
            "DATA collection:played for  166  steps , rewards 166.0  last state  [-1.01751243 -1.27431878 -0.37892675  0.09192898]\n",
            "DATA collection:played for  165  steps , rewards 165.0  last state  [-1.00850437 -1.19722206 -0.30747606  0.15068475]\n",
            "DATA collection:played for  204  steps , rewards 204.0  last state  [-1.00612479 -1.07223408 -0.33563021  0.01292104]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/300 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  176  steps , rewards 176.0  last state  [-1.00805165 -1.0911201  -0.2579894   0.02726537]\n",
            "training from  180  games\n",
            "training with   priority  True  training_steps  300  discount  0.99  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [31:16<00:00,  6.26s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.06475032  0.88234328 -0.50544485 -0.79224525]\n",
            "DATA collection:played for  8  steps , rewards 8.0  last state  [ 0.04164544  0.7739903  -0.49883825 -0.72894767]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.05975234  0.97045887 -0.61539947 -0.87545823]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.05083124  0.88620586 -0.52326539 -0.79323471]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.07528909  0.99387528 -0.51833121 -0.84746879]\n",
            "DATA collection:played for  8  steps , rewards 8.0  last state  [ 0.0480495   0.79170567 -0.53774152 -0.73771134]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.06457201  0.86809933 -0.4993778  -0.78121464]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.05054309  0.90481562 -0.60728578 -0.81334473]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.05107893  0.89180654 -0.60774202 -0.82726906]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.0491134   0.8904455  -0.60605642 -0.82596689]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.08491355  0.98006467 -0.5755752  -0.88242274]\n",
            "DATA collection:played for  8  steps , rewards 8.0  last state  [ 0.05003884  0.78380167 -0.51376722 -0.7328474 ]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.05464224  0.97361208 -0.56957568 -0.86877246]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.07766719  0.88956331 -0.57598566 -0.81810242]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.06853734  0.99222291 -0.53176717 -0.86477584]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.06177384  0.88306229 -0.58554926 -0.81082695]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.07017367  0.86461034 -0.58884701 -0.82336343]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.0547896   0.95977141 -0.57579536 -0.88341377]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.05878983  0.96709947 -0.56931414 -0.86680342]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.08170307  0.89806926 -0.5837092  -0.82012168]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.04507725  0.87507279 -0.5818694  -0.80188304]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.08530898  0.99130704 -0.5249977  -0.86860725]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.06549618  0.89456539 -0.56484454 -0.81601034]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.05382     0.87388237 -0.53347548 -0.78912162]\n",
            "DATA collection:played for  8  steps , rewards 8.0  last state  [ 0.03264251  0.79763636 -0.54229789 -0.74070452]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.08998692  0.97722189 -0.62773162 -0.88823406]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.04615512  0.86108605 -0.53269496 -0.79919455]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.09119526  0.97872368 -0.51834167 -0.85659042]\n",
            "DATA collection:played for  8  steps , rewards 8.0  last state  [ 0.04500847  0.79963097 -0.51865903 -0.73300779]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/300 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.05005728  0.95672721 -0.59135523 -0.87113837]\n",
            "training from  210  games\n",
            "training with   priority  True  training_steps  300  discount  0.99  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [29:50<00:00,  5.97s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  401.0  rewards  last state  [ 0.8769957   1.60716782 -0.61545821 -0.80552666]\n",
            "Eval:played for  340.0  rewards  last state  [ 0.33489786  2.28597506 -0.62810013 -1.04063632]\n",
            "Eval:played for  305.0  rewards  last state  [ 0.418469    2.53094716 -0.55910842 -0.99112659]\n",
            "Eval:played for  331.0  rewards  last state  [ 0.47668537  1.77736335 -0.51808999 -0.86485713]\n",
            "Eval:played for  332.0  rewards  last state  [ 0.5771375   1.89584294 -0.61196853 -0.87930681]\n",
            "DATA collection:played for  361  steps , rewards 361.0  last state  [ 0.45669544  2.00403849 -0.54388384 -0.92815337]\n",
            "DATA collection:played for  343  steps , rewards 343.0  last state  [ 0.39640271  2.35017616 -0.53373945 -0.96761125]\n",
            "DATA collection:played for  405  steps , rewards 405.0  last state  [ 0.8701925   1.23371443 -0.4995315  -0.62746735]\n",
            "DATA collection:played for  410  steps , rewards 410.0  last state  [ 0.89878246  2.25780244 -0.53767542 -0.83129444]\n",
            "DATA collection:played for  309  steps , rewards 309.0  last state  [ 0.21618267  1.99553878 -0.62148186 -0.99664883]\n",
            "DATA collection:played for  353  steps , rewards 353.0  last state  [ 0.65063007  1.99953984 -0.52111769 -0.84889264]\n",
            "DATA collection:played for  339  steps , rewards 339.0  last state  [ 0.4312595   1.99233785 -0.5062644  -0.8974062 ]\n",
            "DATA collection:played for  332  steps , rewards 332.0  last state  [ 0.5432085   2.08767526 -0.62773101 -0.97020968]\n",
            "DATA collection:played for  389  steps , rewards 389.0  last state  [ 1.00095701  1.58878844 -0.53986432 -0.69182983]\n",
            "DATA collection:played for  344  steps , rewards 344.0  last state  [ 0.59566374  2.0588081  -0.54143492 -0.87168437]\n",
            "DATA collection:played for  347  steps , rewards 347.0  last state  [ 0.58505092  2.00226362 -0.59881737 -0.92129586]\n",
            "DATA collection:played for  371  steps , rewards 371.0  last state  [ 1.00097617  0.26013417 -0.43064138  0.03548282]\n",
            "DATA collection:played for  355  steps , rewards 355.0  last state  [ 0.67171     1.80076906 -0.51038112 -0.81317458]\n",
            "DATA collection:played for  351  steps , rewards 351.0  last state  [ 0.71009975  1.99397306 -0.57008035 -0.85076678]\n",
            "DATA collection:played for  342  steps , rewards 342.0  last state  [ 0.71797052  1.69452669 -0.57179084 -0.75126405]\n",
            "DATA collection:played for  357  steps , rewards 357.0  last state  [ 0.34200888  1.98986776 -0.56461507 -0.93624877]\n",
            "DATA collection:played for  328  steps , rewards 328.0  last state  [ 0.36835867  2.05388662 -0.49880281 -0.90122304]\n",
            "DATA collection:played for  359  steps , rewards 359.0  last state  [ 0.29042042  1.99448681 -0.53149437 -0.93666177]\n",
            "DATA collection:played for  335  steps , rewards 335.0  last state  [ 0.92411185  1.76587206 -0.53930518 -0.70083314]\n",
            "DATA collection:played for  353  steps , rewards 353.0  last state  [ 0.60868549  2.15632735 -0.49920295 -0.90274518]\n",
            "DATA collection:played for  376  steps , rewards 376.0  last state  [ 0.87452274  1.52716601 -0.52491163 -0.62207106]\n",
            "DATA collection:played for  344  steps , rewards 344.0  last state  [ 0.28936458  2.2818566  -0.64624199 -1.04368092]\n",
            "DATA collection:played for  354  steps , rewards 354.0  last state  [ 0.19907725  2.26595896 -0.57332298 -1.02453728]\n",
            "DATA collection:played for  362  steps , rewards 362.0  last state  [ 0.80314509  1.70026946 -0.56888528 -0.83914865]\n",
            "DATA collection:played for  322  steps , rewards 322.0  last state  [ 0.68742065  1.89188489 -0.56261263 -0.83083879]\n",
            "DATA collection:played for  349  steps , rewards 349.0  last state  [ 0.48090464  2.16905405 -0.60386368 -0.99589384]\n",
            "DATA collection:played for  370  steps , rewards 370.0  last state  [ 0.8948585   1.67713097 -0.51217516 -0.62538802]\n",
            "DATA collection:played for  300  steps , rewards 300.0  last state  [ 0.33603624  1.70479074 -0.52122155 -0.89310239]\n",
            "DATA collection:played for  354  steps , rewards 354.0  last state  [ 0.56061929  2.09716105 -0.62309789 -0.97432083]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/300 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  296  steps , rewards 296.0  last state  [ 0.24610461  1.90353292 -0.51896115 -0.91395618]\n",
            "training from  240  games\n",
            "training with   priority  True  training_steps  300  discount  0.99  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [30:17<00:00,  6.06s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  400.0  rewards  last state  [1.00122965 0.9167157  0.08067315 0.01311958]\n",
            "Eval:played for  500.0  rewards  last state  [-0.47112341  0.20393503  0.18445238 -0.0456594 ]\n",
            "Eval:played for  407.0  rewards  last state  [1.00695206 1.01029495 0.11392497 0.01043499]\n",
            "Eval:played for  500.0  rewards  last state  [ 0.04448909  0.01412295 -0.00773239  0.00830037]\n",
            "Eval:played for  355.0  rewards  last state  [1.00433676 0.99369909 0.11916891 0.01065274]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.86523943  0.92359884  0.092036   -0.0289516 ]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [0.50139355 0.72251904 0.17346619 0.04856134]\n",
            "DATA collection:played for  374  steps , rewards 374.0  last state  [1.01198321 0.91627407 0.11966551 0.02091271]\n",
            "DATA collection:played for  452  steps , rewards 452.0  last state  [-0.46723006  0.92811449  0.50604131  0.03890459]\n",
            "DATA collection:played for  465  steps , rewards 465.0  last state  [ 1.00455784  1.17515264  0.20744983 -0.02715761]\n",
            "DATA collection:played for  388  steps , rewards 388.0  last state  [-1.00360737 -0.91072044 -0.03960882  0.01545693]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.69776067  0.16671542 -0.11249054  0.01700882]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.75142644 -0.01232616 -0.20103231  0.00965963]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.22136117  0.18750014  0.00135146 -0.08169866]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [0.19039284 0.92186268 0.31341332 0.05627214]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.56074964 -0.01587903 -0.13294022  0.030161  ]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-3.97754129e-02  9.32795710e-03  6.25402095e-05 -2.65026894e-02]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.43631609 -0.16169364  0.01530152  0.02084138]\n",
            "DATA collection:played for  489  steps , rewards 489.0  last state  [ 0.09008694  1.37676666  0.4996523  -0.05325819]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.29736815  1.10120201  0.34815677 -0.00390489]\n",
            "DATA collection:played for  396  steps , rewards 396.0  last state  [-1.00331966 -0.20377329  0.19003677 -0.00256753]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.02885725 -0.01687322  0.01346016  0.01625429]\n",
            "DATA collection:played for  404  steps , rewards 404.0  last state  [ 1.01875203  1.64170771  0.3377576  -0.03544613]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [0.05550888 0.01484517 0.00378554 0.00331688]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [0.79644291 0.91340401 0.12971158 0.04412603]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.45963453  0.17264452 -0.07906026 -0.01183826]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.50280214 -0.00397175 -0.12062403  0.03157108]\n",
            "DATA collection:played for  398  steps , rewards 398.0  last state  [1.01288827 0.92741746 0.06516326 0.00580251]\n",
            "DATA collection:played for  492  steps , rewards 492.0  last state  [1.01476381 0.90089262 0.08732331 0.08892352]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.49101902 -0.00940224 -0.17152194 -0.01774073]\n",
            "DATA collection:played for  458  steps , rewards 458.0  last state  [ 1.01480973  1.25253628  0.19109291 -0.04624472]\n",
            "DATA collection:played for  352  steps , rewards 352.0  last state  [-1.0033463  -0.34095348  0.13657944 -0.0232344 ]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.08970459 -0.01676933  0.0039492  -0.02058751]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.08346958  0.02582883 -0.03685302 -0.02440251]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/300 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  392  steps , rewards 392.0  last state  [1.00811121 0.9309965  0.07935682 0.00249157]\n",
            "training from  270  games\n",
            "training with   priority  True  training_steps  300  discount  0.99  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [30:52<00:00,  6.18s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  166.0  rewards  last state  [0.93442627 1.10684326 0.50098798 0.03411991]\n",
            "Eval:played for  228.0  rewards  last state  [0.91739518 1.10088087 0.49966632 0.0280097 ]\n",
            "Eval:played for  289.0  rewards  last state  [1.00681739 1.00233877 0.46162612 0.10340604]\n",
            "Eval:played for  222.0  rewards  last state  [1.00495508 1.10734052 0.48959633 0.06404449]\n",
            "Eval:played for  247.0  rewards  last state  [1.00235131 0.99784246 0.45207108 0.10722276]\n",
            "DATA collection:played for  264  steps , rewards 264.0  last state  [0.92667375 1.08901115 0.49885784 0.03830367]\n",
            "DATA collection:played for  219  steps , rewards 219.0  last state  [ 0.94786192  1.19537549  0.49922055 -0.01295742]\n",
            "DATA collection:played for  295  steps , rewards 295.0  last state  [0.99678386 1.16336552 0.5156085  0.0438563 ]\n",
            "DATA collection:played for  338  steps , rewards 338.0  last state  [1.01008443 1.07086166 0.47894257 0.08646206]\n",
            "DATA collection:played for  157  steps , rewards 157.0  last state  [ 0.88355918  1.16840683  0.49998957 -0.01976836]\n",
            "DATA collection:played for  184  steps , rewards 184.0  last state  [0.94217507 1.11333293 0.50098737 0.03566723]\n",
            "DATA collection:played for  355  steps , rewards 355.0  last state  [ 0.18122193  0.98653408  0.50068216 -0.01098961]\n",
            "DATA collection:played for  232  steps , rewards 232.0  last state  [0.98092465 1.09264022 0.50889192 0.06949116]\n",
            "DATA collection:played for  198  steps , rewards 198.0  last state  [1.01282228 1.10793896 0.51837073 0.0631274 ]\n",
            "DATA collection:played for  309  steps , rewards 309.0  last state  [0.99897803 1.1574092  0.5172424  0.05216649]\n",
            "DATA collection:played for  192  steps , rewards 192.0  last state  [1.01198421 1.09317784 0.48501061 0.07425066]\n",
            "DATA collection:played for  420  steps , rewards 420.0  last state  [ 0.48803744  1.11493941  0.50198558 -0.01929936]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.05171165 -0.02028923 -0.02264691  0.02288308]\n",
            "DATA collection:played for  235  steps , rewards 235.0  last state  [ 1.00634389  1.20305471  0.50297076 -0.03131224]\n",
            "DATA collection:played for  254  steps , rewards 254.0  last state  [0.99172673 1.08640047 0.50057698 0.05075151]\n",
            "DATA collection:played for  223  steps , rewards 223.0  last state  [0.9428894  1.1862341  0.50460049 0.00499245]\n",
            "DATA collection:played for  351  steps , rewards 351.0  last state  [0.15148168 0.97978269 0.50464756 0.0224249 ]\n",
            "DATA collection:played for  237  steps , rewards 237.0  last state  [0.8855584  0.99616412 0.50235302 0.13489295]\n",
            "DATA collection:played for  181  steps , rewards 181.0  last state  [1.01479187 1.17950767 0.49466199 0.03324439]\n",
            "DATA collection:played for  381  steps , rewards 381.0  last state  [0.23758782 0.99922178 0.50385294 0.00842322]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.03004111 -0.02239047  0.00583996  0.02129028]\n",
            "DATA collection:played for  172  steps , rewards 172.0  last state  [0.94076963 1.11152744 0.50620895 0.05118823]\n",
            "DATA collection:played for  210  steps , rewards 210.0  last state  [1.00327055 1.10882551 0.45501505 0.00453429]\n",
            "DATA collection:played for  375  steps , rewards 375.0  last state  [0.45511679 1.01236417 0.5063329  0.0498842 ]\n",
            "DATA collection:played for  151  steps , rewards 151.0  last state  [0.97525363 1.19332283 0.51052833 0.03201806]\n",
            "DATA collection:played for  371  steps , rewards 371.0  last state  [0.17421605 0.97916066 0.51121605 0.03945607]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.08288845 -0.01375802  0.01460152  0.01148432]\n",
            "DATA collection:played for  259  steps , rewards 259.0  last state  [ 1.01415158  1.19725957  0.48532689 -0.01409493]\n",
            "DATA collection:played for  267  steps , rewards 267.0  last state  [0.90636457 1.16496618 0.50487799 0.01245418]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/300 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  272  steps , rewards 272.0  last state  [0.95986695 1.07941743 0.50314099 0.0570388 ]\n",
            "training from  300  games\n",
            "training with   priority  True  training_steps  300  discount  0.99  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [30:49<00:00,  6.16s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  233.0  rewards  last state  [1.00923584 0.63460449 0.19169243 0.07695195]\n",
            "Eval:played for  500.0  rewards  last state  [ 0.09751201  0.00877324  0.00421152 -0.00459138]\n",
            "Eval:played for  500.0  rewards  last state  [ 0.19704261  0.02300723 -0.00323482 -0.01222667]\n",
            "Eval:played for  486.0  rewards  last state  [0.33017646 1.08161552 0.51968838 0.08118059]\n",
            "Eval:played for  237.0  rewards  last state  [1.0080426  0.6102725  0.20760616 0.10494167]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.08880802  0.00678158 -0.00536968  0.00109085]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.08952871  0.00665478 -0.00029163 -0.0002182 ]\n",
            "DATA collection:played for  490  steps , rewards 490.0  last state  [0.3845902  1.09621764 0.50233089 0.05953178]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.53148889 -0.01752708 -0.05532685 -0.02365266]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.10259004 -0.02456285 -0.00422995  0.02393579]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.10563653  0.00727962 -0.00324499 -0.0043266 ]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.05218335 -0.01555075  0.00858687  0.00882552]\n",
            "DATA collection:played for  244  steps , rewards 244.0  last state  [1.00295715 0.71257799 0.2168325  0.04048963]\n",
            "DATA collection:played for  429  steps , rewards 429.0  last state  [0.28471898 1.16468508 0.50292013 0.02349307]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.0908918  -0.02432102 -0.01076078  0.01153891]\n",
            "DATA collection:played for  457  steps , rewards 457.0  last state  [0.35853461 1.16981777 0.51144548 0.04568424]\n",
            "DATA collection:played for  232  steps , rewards 232.0  last state  [ 1.00826523  0.73771503  0.17197813 -0.04137575]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.21272461  0.02516734 -0.01388737 -0.01730804]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.03131673 -0.01376377 -0.01116335  0.01123615]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [0.37834041 0.92397367 0.38778293 0.07667991]\n",
            "DATA collection:played for  281  steps , rewards 281.0  last state  [-1.00033235 -0.28214888  0.02118764 -0.0332182 ]\n",
            "DATA collection:played for  487  steps , rewards 487.0  last state  [0.61531044 1.19162209 0.51334982 0.06553796]\n",
            "DATA collection:played for  267  steps , rewards 267.0  last state  [1.01129084 0.61895961 0.17100083 0.04181713]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.16760959  0.01939014 -0.01452958 -0.01300366]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.10125058 -0.02455143 -0.00490048  0.02248916]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.00832652 -0.00874943 -0.00056041  0.00357572]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.01660411 -0.00537934 -0.00171105  0.00131456]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.01606722 -0.00524367 -0.00217293  0.00204489]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.05521378  0.00062608 -0.01650009  0.0040553 ]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.15636619  0.01742631  0.01030161 -0.00943215]\n",
            "DATA collection:played for  288  steps , rewards 288.0  last state  [-1.00466847 -0.38292065 -0.01706615  0.03637989]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.0038942  -0.00605468  0.0231817   0.00096823]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.05969945  0.0005488  -0.01829449  0.00209518]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.02346503 -0.0108427   0.0030927   0.00063465]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/300 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.1120693   0.00981345 -0.0125145  -0.00863978]\n",
            "training from  330  games\n",
            "training with   priority  True  training_steps  300  discount  0.99  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [31:32<00:00,  6.31s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  500.0  rewards  last state  [ 0.05865279 -0.0038402  -0.00503125  0.01363278]\n",
            "Eval:played for  190.0  rewards  last state  [1.01105035 0.9073955  0.33513034 0.08582437]\n",
            "Eval:played for  500.0  rewards  last state  [ 0.18102652  0.01517332  0.00475378 -0.00253108]\n",
            "Eval:played for  500.0  rewards  last state  [ 0.12602969  0.00894125  0.00282718 -0.01585641]\n",
            "Eval:played for  500.0  rewards  last state  [ 0.05386885 -0.00596705 -0.00193797  0.0058488 ]\n",
            "DATA collection:played for  484  steps , rewards 484.0  last state  [0.25282747 1.08472137 0.50630282 0.0523174 ]\n",
            "DATA collection:played for  214  steps , rewards 214.0  last state  [1.01414753 0.89033716 0.3347798  0.08259401]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.22952582  0.02523373  0.00721434 -0.03829576]\n",
            "DATA collection:played for  179  steps , rewards 179.0  last state  [1.00320477 1.01403987 0.35654899 0.00103572]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.04346859 -0.00798669  0.01286853  0.01172798]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.01690184 -0.01505253  0.0161598   0.0102734 ]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.04801854 -0.00574245 -0.00928889  0.00513197]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.0534752  -0.01925491  0.01822528  0.0198876 ]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.14424761  0.00865699  0.01271591 -0.00537853]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.05793338 -0.02134702 -0.02179155  0.02177092]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.55575309  0.72375818  0.45284739  0.12319711]\n",
            "DATA collection:played for  186  steps , rewards 186.0  last state  [1.00592036 0.91987255 0.31348279 0.01003533]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.12661957  0.01184221  0.02636083 -0.00503136]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.07073047 -0.02340806 -0.00306961  0.02168947]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.26319213  0.02357998 -0.01340749 -0.00932577]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.2292435   0.02639946  0.0010958  -0.03707132]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.21289125  0.02048997  0.03581548 -0.01669892]\n",
            "DATA collection:played for  450  steps , rewards 450.0  last state  [0.14580302 1.06771889 0.50958568 0.06841942]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.10114618 -0.00088045 -0.03207295  0.00182148]\n",
            "DATA collection:played for  182  steps , rewards 182.0  last state  [1.01403443 0.92509101 0.31576483 0.03741998]\n",
            "DATA collection:played for  309  steps , rewards 309.0  last state  [-1.00322107 -0.29119441  0.00184416 -0.012446  ]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.0747007  -0.00403392  0.00770282 -0.00259462]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.04567515 -0.00737131 -0.02043927 -0.00622487]\n",
            "DATA collection:played for  187  steps , rewards 187.0  last state  [1.01208763 0.99646772 0.37748672 0.02538961]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.25198651  0.02569318 -0.00502067 -0.02692564]\n",
            "DATA collection:played for  207  steps , rewards 207.0  last state  [ 1.00649011  0.98529005  0.32047222 -0.0495357 ]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.03238169 -0.01965039 -0.00636564  0.01180311]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.02593761 -0.00829847 -0.01222678 -0.00311434]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.18807403  0.01618974 -0.00842308 -0.01730672]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/300 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.03126814 -0.00781316  0.00594304  0.01229311]\n",
            "training from  360  games\n",
            "training with   priority  True  training_steps  300  discount  0.99  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [31:38<00:00,  6.33s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  246.0  rewards  last state  [1.00637416 1.07969599 0.45266343 0.07126246]\n",
            "Eval:played for  259.0  rewards  last state  [0.97926599 1.18081095 0.50310187 0.02047384]\n",
            "Eval:played for  157.0  rewards  last state  [ 0.94409842  1.17444606  0.50242949 -0.00925469]\n",
            "Eval:played for  500.0  rewards  last state  [ 0.14918107 -0.00985304  0.00347328  0.01107166]\n",
            "Eval:played for  261.0  rewards  last state  [ 1.01116616  1.18372315  0.47954201 -0.01987676]\n",
            "DATA collection:played for  228  steps , rewards 228.0  last state  [1.00105137 1.09903515 0.43870287 0.00153968]\n",
            "DATA collection:played for  232  steps , rewards 232.0  last state  [1.00689239 1.10755066 0.49293665 0.05193174]\n",
            "DATA collection:played for  493  steps , rewards 493.0  last state  [1.01044009 0.81864621 0.36830106 0.06946447]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.53734451  0.01044605  0.09756103  0.06622884]\n",
            "DATA collection:played for  178  steps , rewards 178.0  last state  [ 1.018636    1.25282755  0.49561888 -0.04007075]\n",
            "DATA collection:played for  383  steps , rewards 383.0  last state  [1.00017264 0.97370566 0.4353768  0.12192967]\n",
            "DATA collection:played for  242  steps , rewards 242.0  last state  [1.00625552 1.09541945 0.45574712 0.01134722]\n",
            "DATA collection:played for  265  steps , rewards 265.0  last state  [1.01074643 1.00124224 0.45794704 0.12005471]\n",
            "DATA collection:played for  246  steps , rewards 246.0  last state  [1.00907476 1.09391581 0.47297185 0.04860071]\n",
            "DATA collection:played for  253  steps , rewards 253.0  last state  [1.00837216 1.01086484 0.43613546 0.08432704]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [0.6880215  0.69903337 0.32412501 0.08668229]\n",
            "DATA collection:played for  234  steps , rewards 234.0  last state  [1.00600224 1.09052571 0.49022513 0.11348729]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.07514107 -0.01867206 -0.03264973  0.01424025]\n",
            "DATA collection:played for  175  steps , rewards 175.0  last state  [1.01101571 1.15715872 0.48131382 0.02749773]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.41401576  0.54374602  0.26129803 -0.03260798]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.55136715  0.025696    0.01984951 -0.05091372]\n",
            "DATA collection:played for  157  steps , rewards 157.0  last state  [1.01267823 1.17459613 0.49915911 0.05002992]\n",
            "DATA collection:played for  323  steps , rewards 323.0  last state  [1.01071953 1.16265931 0.48297069 0.02774056]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.47858871  0.35723817  0.23473457  0.01732277]\n",
            "DATA collection:played for  220  steps , rewards 220.0  last state  [-1.00240673 -0.74454301 -0.27590103 -0.06135457]\n",
            "DATA collection:played for  279  steps , rewards 279.0  last state  [1.01321077 1.18672314 0.49190072 0.02632213]\n",
            "DATA collection:played for  217  steps , rewards 217.0  last state  [0.99257133 1.18826307 0.50011509 0.01913059]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.28496385  0.7238992   0.36558748 -0.04532202]\n",
            "DATA collection:played for  179  steps , rewards 179.0  last state  [1.00600177 1.20095011 0.4752546  0.00662502]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.52161755  0.52970247  0.29480065 -0.00880877]\n",
            "DATA collection:played for  170  steps , rewards 170.0  last state  [ 1.00293182  1.25666451  0.4887821  -0.02519198]\n",
            "DATA collection:played for  232  steps , rewards 232.0  last state  [1.01875873 1.08680175 0.47853776 0.0949244 ]\n",
            "DATA collection:played for  293  steps , rewards 293.0  last state  [ 1.00685388  1.18826321  0.47362513 -0.00584495]\n",
            "DATA collection:played for  299  steps , rewards 299.0  last state  [ 1.01672713  1.18036767  0.46863086 -0.02245473]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/300 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  271  steps , rewards 271.0  last state  [1.01462027 0.99262947 0.43317887 0.08467555]\n",
            "training from  390  games\n",
            "training with   priority  True  training_steps  300  discount  0.99  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 77%|███████▋  | 231/300 [24:24<07:03,  6.14s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHEjPMc9hvLv"
      },
      "source": [
        "Here using the current model to generate value targets worked better than using a time delayed stable target value network for cartpole.\n",
        "Also usig ucurrent model for intial priorities."
      ]
    }
  ]
}