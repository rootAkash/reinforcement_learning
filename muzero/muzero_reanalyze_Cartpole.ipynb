{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "muzero reanalyze:Cartpole.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOTYDJUev6dQg4OYrNjCQAk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rootAkash/reinforcement_learning/blob/master/muzero/muzero_reanalyze_Cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfJ7r4ZLfn6u"
      },
      "source": [
        "#takes too much time to train for reanalyze only advisable if simulation or episode data is costly \n",
        "was only able to do 200-300 batch trainig steps only since it was too slow per batch "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcnQSsnTkUjI"
      },
      "source": [
        "what is reanalyze variant of muzero?\n",
        "off  policy varient of muzero that uses old data by recomputing fresh target values using a target value net and \n",
        "also provides fresh policy targets by recomputing tree search .\n",
        "the priorities will also change because z and root v changed because v function changes during training, z changes because of the new  updated targets.?do we update priorities?recompute search value v?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfUbzbuAWDbM"
      },
      "source": [
        "new experinces have priority pred value and z value  and weights will be automatically(w = ((1/(N*1/N))**beta)) at the start\n",
        "and the l1 loss between predicted value and returns z is calculated from the initial training forward pass and not after backprop for updating priorities\n",
        "but that anyways will require extra loop of converting them to scalars from category outputs to calculate l1 unless vectorised which it is\n",
        "so it will be fastter than looping since catts supports vectors so looping is not nneeded so priority update before backprop is fine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwI18Z6p1oGP"
      },
      "source": [
        "!pip install gym[all]\n",
        "!pip install box2d-py\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFZbm7i9GNao"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "565N2GRAGNas"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQRen3PlNkiB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37a09a23-669e-4618-8a56-5ccb009b9f86"
      },
      "source": [
        "import numpy as np\n",
        "def stcat(x,support=15):\n",
        "  x = np.sign(x) * ((abs(x) + 1)**0.5 - 1) + 0.001 * x\n",
        "  x = np.clip(x, -support, support)\n",
        "  floor = np.floor(x)\n",
        "  prob = x - floor\n",
        "  logits = np.zeros( 2 * support + 1)\n",
        "  first_index = int(floor + support)\n",
        "  second_index = int(floor + support+1)\n",
        "  logits[first_index] = 1-prob\n",
        "  if prob>0:\n",
        "    logits[second_index] = prob\n",
        "  return logits\n",
        "#allow for batch processing  \n",
        "def catts(x,support=15):\n",
        "  support = np.arange(-support, support+1, 1)\n",
        "  if len(x.shape)==2:\n",
        "    #for  batch of x\\\n",
        "    x = np.sum(support*x,axis=1)\n",
        "  elif len(x.shape)==1:\n",
        "    #for single x\n",
        "    x = np.sum(support*x)  \n",
        "  else:\n",
        "    print(\"wrong input for conversion to  scalar\")  \n",
        "  x = np.sign(x) * ((((1 + 4 * 0.001 * (np.abs(x) + 1 + 0.001))**0.5 - 1) / (2 * 0.001))** 2- 1)\n",
        "  return x  \n",
        "\n",
        "#cat = stcat(5)#test 1 example\n",
        "cat = np.array([stcat(150),stcat(-150)]) # test batch example\n",
        "print(cat,cat.shape)\n",
        "scalar = catts(cat)\n",
        "print(scalar)\n",
        "print(\"done\")        \n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.56179427 0.43820573 0.         0.\n",
            "  0.        ]\n",
            " [0.         0.         0.         0.43820573 0.56179427 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.        ]] (2, 31)\n",
            "[ 150. -150.]\n",
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhf3dyDa2GYq",
        "outputId": "e7db47c5-121e-4639-ab8d-aff02036dc5a"
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MuZeroNet(nn.Module):\n",
        "    def __init__(self, input_size, action_space_n, reward_support_size, value_support_size):\n",
        "        super().__init__()\n",
        "        self.hx_size = 32\n",
        "        self._representation = nn.Sequential(nn.Linear(input_size, self.hx_size),\n",
        "                                             nn.Tanh())\n",
        "        self._dynamics_state = nn.Sequential(nn.Linear(self.hx_size + action_space_n, 64),\n",
        "                                             nn.Tanh(),\n",
        "                                             nn.Linear(64, self.hx_size),\n",
        "                                             nn.Tanh())\n",
        "        self._dynamics_reward = nn.Sequential(nn.Linear(self.hx_size + action_space_n, 64),\n",
        "                                              nn.LeakyReLU(),\n",
        "                                              nn.Linear(64, 2*reward_support_size+1))\n",
        "        self._prediction_actor = nn.Sequential(nn.Linear(self.hx_size, 64),\n",
        "                                               nn.LeakyReLU(),\n",
        "                                               nn.Linear(64, action_space_n))\n",
        "        self._prediction_value = nn.Sequential(nn.Linear(self.hx_size, 64),\n",
        "                                               nn.LeakyReLU(),\n",
        "                                               nn.Linear(64, 2*value_support_size+1))\n",
        "        self.action_space_n = action_space_n\n",
        "\n",
        "        self._prediction_value[-1].weight.data.fill_(0)\n",
        "        self._prediction_value[-1].bias.data.fill_(0)\n",
        "        self._dynamics_reward[-1].weight.data.fill_(0)\n",
        "        self._dynamics_reward[-1].bias.data.fill_(0)\n",
        "\n",
        "    def p(self, state):\n",
        "        actor = torch.softmax(self._prediction_actor(state),dim=1)\n",
        "        value = torch.softmax(self._prediction_value(state),dim=1)\n",
        "        return actor, value\n",
        "\n",
        "    def h(self, obs_history):\n",
        "        return self._representation(obs_history)\n",
        "\n",
        "    def g(self, state, action):\n",
        "        x = torch.cat((state, action), dim=1)\n",
        "        next_state = self._dynamics_state(x)\n",
        "        reward = torch.softmax(self._dynamics_reward(x),dim=1)\n",
        "        return next_state, reward     \n",
        "\n",
        "    def initial_state(self, x):\n",
        "        hout = self.h(x)\n",
        "        prob,v= self.p(hout)\n",
        "        return hout,prob,v\n",
        "    def next_state(self,hin,a):\n",
        "        hout,r = self.g(hin,a)\n",
        "        prob,v= self.p(hout)\n",
        "        return hout,r,prob,v\n",
        "    def inference_initial_state(self, x):\n",
        "        with torch.no_grad():\n",
        "          hout = self.h(x)\n",
        "          prob,v=self.p(hout)\n",
        "\n",
        "          return hout,prob,v\n",
        "    def inference_next_state(self,hin,a):\n",
        "        with torch.no_grad():\n",
        "          hout,r = self.g(hin,a)\n",
        "          prob,v=self.p(hout)\n",
        "          return hout,r,prob,v     \n",
        "\n",
        "\n",
        "print(\"done\")                                      "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkKMTKa6wYtR"
      },
      "source": [
        "\n",
        "#MTCS    MUzero modified for intermeditate rewards settings and using predicted rewards\n",
        "#accepts policy as a list\n",
        "import torch\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "def dynamics(net,state,action):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    #print(state,action) \n",
        "    next_state,reward,prob,value = net.inference_next_state(state.to(device),torch.tensor([action]).float().to(device))\n",
        "    reward = catts(reward.cpu().numpy().ravel())\n",
        "    value = catts(value.cpu().numpy().ravel())\n",
        "    prob = prob.cpu().tolist()[0]\n",
        "    #print(\"dynamics\",prob)\n",
        "    return next_state.cpu(),reward,prob,value\n",
        "\n",
        "\n",
        "class MinMaxStats:\n",
        "    \"\"\"A class that holds the min-max values of the tree.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.MAXIMUM_FLOAT_VALUE = float('inf')       \n",
        "        self.maximum =  -self.MAXIMUM_FLOAT_VALUE\n",
        "        self.minimum =  self.MAXIMUM_FLOAT_VALUE\n",
        "\n",
        "    def update(self, value: float):\n",
        "        if value is None:\n",
        "            raise ValueError\n",
        "\n",
        "        self.maximum = max(self.maximum, value)\n",
        "        self.minimum = min(self.minimum, value)\n",
        "\n",
        "    def normalize(self, value: float) -> float:\n",
        "        # If the value is unknow, by default we set it to the minimum possible value\n",
        "        if value is None:\n",
        "            return 0.0\n",
        "\n",
        "        if self.maximum > self.minimum:\n",
        "            # We normalize only when we have set the maximum and minimum values.\n",
        "            return (value - self.minimum) / (self.maximum - self.minimum)\n",
        "        return value\n",
        "\n",
        "\n",
        "class Node:\n",
        "    \"\"\"A class that represent nodes inside the MCTS tree\"\"\"\n",
        "\n",
        "    def __init__(self, prior: float):\n",
        "        self.visit_count = 0\n",
        "        self.to_play = -1\n",
        "        self.prior = prior\n",
        "        self.value_sum = 0\n",
        "        self.children = {}\n",
        "        self.hidden_state = None\n",
        "        self.reward = 0\n",
        "\n",
        "    def expanded(self):\n",
        "        return len(self.children) > 0\n",
        "\n",
        "    def value(self):\n",
        "        if self.visit_count == 0:\n",
        "            return None\n",
        "        return self.value_sum / self.visit_count\n",
        "\n",
        "\n",
        "def softmax_sample(visit_counts, actions, t):\n",
        "    counts_exp = np.exp(visit_counts) * (1 / t)\n",
        "    probs = counts_exp / np.sum(counts_exp, axis=0)\n",
        "    action_idx = np.random.choice(len(actions), p=probs)\n",
        "    return actions[action_idx]\n",
        "\n",
        "\n",
        "\"\"\"MCTS module: where MuZero thinks inside the tree.\"\"\"\n",
        "\n",
        "\n",
        "def add_exploration_noise( node):\n",
        "    \"\"\"\n",
        "    At the start of each search, we add dirichlet noise to the prior of the root\n",
        "    to encourage the search to explore new actions.\n",
        "    \"\"\"\n",
        "    actions = list(node.children.keys())\n",
        "    noise = np.random.dirichlet([0.25] * len(actions)) # config.root_dirichlet_alpha\n",
        "    frac = 0.25#config.root_exploration_fraction\n",
        "    for a, n in zip(actions, noise):\n",
        "        node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac\n",
        "\n",
        "\n",
        "\n",
        "def ucb_score(parent, child,min_max_stats):\n",
        "    \"\"\"\n",
        "    The score for a node is based on its value, plus an exploration bonus based on\n",
        "    the prior.\n",
        "\n",
        "    \"\"\"\n",
        "    pb_c_base = 19652\n",
        "    pb_c_init = 1.25\n",
        "    pb_c = math.log((parent.visit_count + pb_c_base + 1) / pb_c_base) + pb_c_init\n",
        "    pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
        "\n",
        "    prior_score = pb_c * child.prior\n",
        "    value_score = min_max_stats.normalize(child.value())\n",
        "    return  value_score + prior_score \n",
        "\n",
        "def select_child(node, min_max_stats):\n",
        "    \"\"\"\n",
        "    Select the child with the highest UCB score.\n",
        "    \"\"\"\n",
        "    # When the parent visit count is zero, all ucb scores are zeros, therefore we return a random child\n",
        "    if node.visit_count == 0:\n",
        "        return random.sample(node.children.items(), 1)[0]\n",
        "\n",
        "    _, action, child = max(\n",
        "        (ucb_score(node, child, min_max_stats), action,\n",
        "         child) for action, child in node.children.items())\n",
        "    return action, child\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def expand_node(node, to_play, actions_space,hidden_state,reward,policy):\n",
        "    \"\"\"\n",
        "    We expand a node using the value, reward and policy prediction obtained from\n",
        "    the neural networks.\n",
        "    \"\"\"\n",
        "    node.to_play = to_play\n",
        "    node.hidden_state = hidden_state\n",
        "    node.reward = reward\n",
        "    policy = {a:policy[a] for a in actions_space}\n",
        "    policy_sum = sum(policy.values())\n",
        "    for action, p in policy.items():\n",
        "        node.children[action] = Node(p / policy_sum) # not needed since mine are already softmax but its fine \n",
        "\n",
        "\n",
        "def backpropagate(search_path, value,to_play,discount, min_max_stats):\n",
        "    \"\"\"\n",
        "    At the end of a simulation, we propagate the evaluation all the way up the\n",
        "    tree to the root.\n",
        "    \"\"\"\n",
        "    for node in search_path[::-1]: #[::-1] means reversed\n",
        "        node.value_sum += value \n",
        "        node.visit_count += 1\n",
        "        min_max_stats.update(node.value())\n",
        "\n",
        "        value = node.reward + discount * value\n",
        "\n",
        "\n",
        "def select_action(node, mode ='softmax'):\n",
        "    \"\"\"\n",
        "    After running simulations inside in MCTS, we select an action based on the root's children visit counts.\n",
        "    During training we use a softmax sample for exploration.\n",
        "    During evaluation we select the most visited child.\n",
        "    \"\"\"\n",
        "    visit_counts = [child.visit_count for child in node.children.values()]\n",
        "    actions = [action for action in node.children.keys()]\n",
        "    action = None\n",
        "    if mode == 'softmax':\n",
        "        t = 1.0\n",
        "        action = softmax_sample(visit_counts, actions, t)\n",
        "    elif mode == 'max':\n",
        "        action, _ = max(node.children.items(), key=lambda item: item[1].visit_count)\n",
        "    counts_exp = np.exp(visit_counts)\n",
        "    probs = counts_exp / np.sum(counts_exp, axis=0)    \n",
        "    #return action ,probs,node.value()\n",
        "    return action ,np.array(visit_counts)/sum(visit_counts),node.value()\n",
        "\n",
        "def run_mcts(net, state,prob,root_value,num_simulations,discount = 0.9):\n",
        "    \"\"\"\n",
        "    Core Monte Carlo Tree Search algorithm.\n",
        "    To decide on an action, we run N simulations, always starting at the root of\n",
        "    the search tree and traversing the tree according to the UCB formula until we\n",
        "    reach a leaf node.\n",
        "    \"\"\"\n",
        "    prob, root_value = prob.tolist()[0] ,catts(root_value.numpy().ravel())\n",
        "    to_play = True\n",
        "    action_space=[ i for i in range(len(prob))]#history.action_space()\n",
        "    #print(\"action space\",action_space)\n",
        "    root = Node(0)\n",
        "    expand_node(root, to_play,action_space,state,0.0,prob)#node, to_play, actions_space ,hidden_state,reward,policy\n",
        "    add_exploration_noise( root)\n",
        "\n",
        "\n",
        "    min_max_stats = MinMaxStats()\n",
        "\n",
        "    for _ in range(num_simulations): \n",
        "        node = root\n",
        "        search_path = [node]\n",
        "\n",
        "        while node.expanded():\n",
        "            action, node = select_child( node, min_max_stats)\n",
        "            search_path.append(node)\n",
        "\n",
        "        # Inside the search tree we use the dynamics function to obtain the next\n",
        "        # hidden state given an action and the previous hidden state.\n",
        "        parent = search_path[-2]\n",
        "        \n",
        "        #network_output = network.recurrent_inference(parent.hidden_state, action)\n",
        "        next_state,r,action_probs, value = dynamics(net,parent.hidden_state,onehot(action,len(action_space))) \n",
        "        expand_node(node, to_play, action_space,next_state,r,action_probs)#node, to_play, actions_space ,hidden_state,reward,policy\n",
        "\n",
        "        backpropagate(search_path, value, to_play, discount, min_max_stats)#search_path, value,,discount, min_max_stats\n",
        "    return root    \n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-NpN4lU12kW"
      },
      "source": [
        "import gym\n",
        "class ScalingObservationWrapper(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    Wrapper that apply a min-max scaling of observations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env, low=None, high=None):\n",
        "        super().__init__(env)\n",
        "        assert isinstance(env.observation_space, gym.spaces.Box)\n",
        "\n",
        "        low = np.array(self.observation_space.low if low is None else low)\n",
        "        high = np.array(self.observation_space.high if high is None else high)\n",
        "\n",
        "        self.mean = (high + low) / 2\n",
        "        self.max = high - self.mean\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return (observation - self.mean) / self.max"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waeVGfWytBB1"
      },
      "source": [
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "def onehot(a,n=2):\n",
        "  return np.eye(n)[a]\n",
        "def play_game(env,net,targetnet,n_sim,discount,render,device,n_act,max_steps,td_steps,per):\n",
        "    trajectory=[]\n",
        "    root_values,pred_values,rewards=[],[],[]\n",
        "    state = env.reset() \n",
        "    done = False\n",
        "    r =0 \n",
        "    stp=0\n",
        "    while not done:\n",
        "        if render:\n",
        "          env.render()\n",
        "        stp+=1  \n",
        "        h ,prob,pred_value= net.inference_initial_state(torch.tensor([state]).float().to(device)) \n",
        "        root  = run_mcts(net,h.cpu(),prob.cpu(),pred_value.cpu(),num_simulations=n_sim,discount=discount)\n",
        "        action,action_prob,mcts_val = select_action(root) \n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        r+=reward\n",
        "        if stp>max_steps:\n",
        "          done = True\n",
        "        data = [state,onehot(action,n_act),action_prob,mcts_val,reward,1]#state,onehotaction,action_prob,mcts_val,reward,priority\n",
        "        if targetnet is None:\n",
        "          root_values.append(mcts_val)\n",
        "        else:\n",
        "          ht ,tarprob,target_pred_value= targetnet.inference_initial_state(torch.tensor([state]).float().to(device))\n",
        "          root_values.append(catts(target_pred_value.cpu().numpy().ravel()))\n",
        "        pred_values.append(catts(pred_value.cpu().numpy().ravel()))\n",
        "        rewards.append(reward)\n",
        "        trajectory.append(data)\n",
        "        state = next_state\n",
        "    #calculating priority as z - pred value\n",
        "    if per:  \n",
        "      priorities =get_initial_priorities(root_values,pred_values,rewards,discount=discount, td_steps=td_steps)\n",
        "      #update trajectory priority\n",
        "      assert len(trajectory) == len(priorities)\n",
        "      for i in range(len(trajectory)):\n",
        "        trajectory[i][5]=priorities[i]\n",
        "    print(\"DATA collection:played for \",len(trajectory),\" steps , rewards\",r,\" last state \",state)   \n",
        "    return trajectory    \n",
        "def get_initial_priorities(root_values,pred_values,rewards,discount=0.99, td_steps=10):\n",
        "    z_values = []\n",
        "    alpha = 1\n",
        "    beta = 1 \n",
        "    for current_index in range(len(root_values)):\n",
        "        bootstrap_index = current_index + td_steps\n",
        "        if bootstrap_index < len(root_values):\n",
        "            value = root_values[bootstrap_index] * discount ** td_steps\n",
        "        else:\n",
        "            value = 0\n",
        "\n",
        "        for i, reward in enumerate(rewards[current_index:bootstrap_index]):\n",
        "            value += reward * discount ** i\n",
        "\n",
        "        if current_index < len(root_values):\n",
        "            z_values.append(value)\n",
        "    #print(\"get priorities\",pred_values,z_values)        \n",
        "    p = np.abs(np.array(pred_values)-np.array(z_values))**alpha  + 0.00001\n",
        "    #priority = p /np.sum(p)\n",
        "    #N= len(pred_values) \n",
        "    #weights = (1/(N*priority))**beta\n",
        "    return list(p)#,list(weights)\n",
        "def eval_game(env,net,n_sim,render,device,max_steps):\n",
        "    state = env.reset() \n",
        "    done = False\n",
        "    r = 0\n",
        "    stp=0\n",
        "    while not done:\n",
        "        if render:\n",
        "          env.render()\n",
        "        stp+=1  \n",
        "        h ,prob,value= net.inference_initial_state(torch.tensor([state]).float().to(device)) \n",
        "        root  = run_mcts(net,h.cpu(),prob.cpu(),value.cpu(),num_simulations=n_sim,discount=discount)\n",
        "        action,action_prob,mcts_val = select_action(root,\"max\")\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        if stp>max_steps:\n",
        "          done = True\n",
        "        r+=reward\n",
        "        state = next_state\n",
        "    print(\"Eval:played for \",r ,\" rewards\",\" last state \",state)   \n",
        "    \n",
        "def sample_games(buffer,batch_size):\n",
        "    # Sample game from buffer either uniformly or according to some priority\n",
        "    #print(\"samplig from .\",len(buffer))\n",
        "    return list(np.random.choice(len(buffer),batch_size))\n",
        "\n",
        "def sample_position(trajectory,priority=None):\n",
        "    # Sample position from game either uniformly or according to some priority.\n",
        "    if priority == None:\n",
        "      return np.random.choice(len(trajectory),1)[0]\n",
        "    return np.random.choice(len(trajectory),1,p = priority)[0]\n",
        "    #return np.random.choice(list(range(0, len(trajectory))),1,p = priority)[0]\n",
        "\n",
        "def update_priorites(buffer,indexes,new_priority):\n",
        "    #buffer is a list and is passed as refernce so changes made here will reflect in buffer\n",
        "    for i in range(len(indexes)):\n",
        "      x,y = indexes[i]\n",
        "      #old_state,old_onehot_action,old_action_prob,old_mcts_val,old_reward,old_pred_value = buffer[x][y]\n",
        "      #buffer[x][y]=(old_state,old_onehot_action,old_action_prob,old_mcts_val,old_reward,new_pred_values[i])\n",
        "      buffer[x][y][5]=new_priority[i]\n",
        "\n",
        "\n",
        "def sample_batch(model,targetmodel,action_space_size,buffer,discount,batch_size,num_unroll_steps, td_steps,n_sim,per):\n",
        "    obs_batch, action_batch, reward_batch, value_batch, policy_batch,weights_batch = [], [], [], [], [],[]\n",
        "    indexes=[]\n",
        "    game_idx = sample_games(buffer,batch_size)\n",
        "    for gi in game_idx:\n",
        "      g = buffer[gi]\n",
        "      state,action,action_prob,root_val,reward,priority = zip(*g)\n",
        "      state,action,action_prob,root_val,reward,priority  =list(state),list(action),list(action_prob),list(root_val),list(reward),list(priority)\n",
        "      #print(\"pred val sample batch\",priority)\n",
        "      if per:\n",
        "        #make priority for sampling from root_value and n_step value\n",
        "        ps  = np.array(priority)/np.sum(np.array(priority))\n",
        "        game_pos = sample_position(g,list(ps))#state index sampled using priority\n",
        "        beta =1 \n",
        "        N = len(g)\n",
        "        weight =(1/(N*ps[game_pos]))**beta\n",
        "        #N= len(pred_values) \n",
        "        #weights = (1/(N*priority))**beta\n",
        "      else:  \n",
        "        weight = 1.0\n",
        "        game_pos = sample_position(g)#state index sampled using priority\n",
        "      _actions = action[game_pos:game_pos + num_unroll_steps]\n",
        "      # random action selection to complete num_unroll_steps\n",
        "      _actions += [onehot(np.random.randint(0, action_space_size),action_space_size)for _ in range(num_unroll_steps - len(_actions))]\n",
        "\n",
        "      obs_batch.append(state[game_pos])\n",
        "      action_batch.append(_actions)\n",
        "      value, reward, policy = make_target(model=model,target_model=targetmodel,buffer=buffer,gameindex=gi,states_trajectory=state,child_visits=action_prob ,root_values=root_val,\n",
        "                                          rewards=reward,state_index=game_pos,discount=discount, num_unroll_steps=num_unroll_steps, td_steps=td_steps,n_sim=n_sim)\n",
        "      reward_batch.append(reward)\n",
        "      value_batch.append(value)\n",
        "      policy_batch.append(policy)\n",
        "      weights_batch.append(weight)\n",
        "      indexes.append((gi,game_pos))\n",
        "\n",
        "\n",
        "\n",
        "    obs_batch = torch.tensor(obs_batch).float()\n",
        "    action_batch = torch.tensor(action_batch).long()\n",
        "    reward_batch = torch.tensor(reward_batch).float()\n",
        "    value_batch = torch.tensor(value_batch).float()\n",
        "    policy_batch = torch.tensor(policy_batch).float()\n",
        "    weights_batch = torch.tensor(weights_batch).float()\n",
        "    return obs_batch, action_batch, reward_batch, value_batch, policy_batch,weights_batch,indexes\n",
        "\n",
        "\n",
        "def make_target(model,target_model,buffer,gameindex,states_trajectory,child_visits,root_values,rewards,state_index,discount=0.99, num_unroll_steps=5, td_steps=5,n_sim=50):\n",
        "        # The value target is the discounted root value of the search tree or value by target network N steps into the future, plus\n",
        "        # the discounted sum of all rewards until then.\n",
        "        target_values, target_rewards, target_policies = [], [], []\n",
        "        for current_index in range(state_index, state_index + num_unroll_steps + 1):\n",
        "            bootstrap_index = current_index + td_steps\n",
        "            if bootstrap_index < len(root_values):\n",
        "                if target_model is None:\n",
        "                    value = root_values[bootstrap_index] * discount ** td_steps\n",
        "                else:\n",
        "                    #  a target network  based on recent parameters is used to provide a fresher,\n",
        "                    # stable n-step bootstrapped target for the value function\n",
        "                    obs = states_trajectory[bootstrap_index]\n",
        "                    #ht ,tarprob,target_pred_value= target_model.inference_initial_state(torch.tensor([obs]).float().to(device))\n",
        "                    ht ,tarprob,target_pred_value= model.inference_initial_state(torch.tensor([obs]).float().to(device))#try recent model (should be same as hard target update freq as 1  but this works well and that doesn't )\n",
        "                    value=catts(target_pred_value.cpu().numpy().ravel()) * discount ** td_steps\n",
        "            else:\n",
        "                value = 0\n",
        "\n",
        "            for i, reward in enumerate(rewards[current_index:bootstrap_index]):\n",
        "                value += reward * discount ** i\n",
        "\n",
        "            if current_index < len(root_values):\n",
        "                target_values.append(stcat(value))\n",
        "                target_rewards.append(stcat(rewards[current_index]))\n",
        "                if target_model is not None and np.random.random() <= 0.8: \n",
        "                    #we recompute policy for current_index using latest params model  \n",
        "                    #and then change it in buffer also use it as target policy 80 percent of time to keep labels stable\n",
        "                    obs = states_trajectory[current_index]\n",
        "                    h ,prob,pred_value= model.inference_initial_state(torch.tensor([obs]).float().to(device)) ##############################################\n",
        "                    root  = run_mcts(model,h.cpu(),prob.cpu(),pred_value.cpu(),num_simulations=n_sim,discount=discount)\n",
        "                    action,action_prob,mcts_val = select_action(root) \n",
        "                    buffer[gameindex][current_index][2]=action_prob #only change this rest values depend on the original trajectory\n",
        "                    child_visits[current_index] =action_prob\n",
        "\n",
        "                target_policies.append(child_visits[current_index])\n",
        "\n",
        "            else:\n",
        "                # States past the end of games are treated as absorbing states.\n",
        "                target_values.append(stcat(0))\n",
        "                target_rewards.append(stcat(0))\n",
        "                # Note: Target policy is  set to 0 so that no policy loss is calculated for them\n",
        "                #target_policies.append([0 for _ in range(len(child_visits[0]))])\n",
        "                target_policies.append(child_visits[0]*0.0)\n",
        "\n",
        "        return target_values, target_rewards, target_policies\n",
        "\n",
        "\n",
        "def scalar_reward_loss( prediction, target):\n",
        "        return -(torch.log(prediction) * target).sum(1)\n",
        "\n",
        "def scalar_value_loss( prediction, target):\n",
        "        return -(torch.log(prediction) * target).sum(1)\n",
        "def update_weights(model,targetmodel,action_space_size, optimizer, replay_buffer,discount,batch_size,num_unroll_steps, td_steps,n_sim,per ):\n",
        "    batch = sample_batch(model,targetmodel,action_space_size,replay_buffer,discount,batch_size,num_unroll_steps, td_steps,n_sim,per)\n",
        "    obs_batch, action_batch, target_reward, target_value, target_policy,target_weights,indexes = batch\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    obs_batch = obs_batch.to(device)\n",
        "    action_batch = action_batch.to(device) \n",
        "    target_reward = target_reward.to(device)\n",
        "    target_value = target_value.to(device)\n",
        "    target_policy = target_policy.to(device)\n",
        "    target_weights = target_weights.to(device)\n",
        "    target_reward_phi =target_reward \n",
        "    target_value_phi = target_value\n",
        "\n",
        "    hidden_state, policy_prob,value  = model.initial_state(obs_batch) # initial model_call #\n",
        "    \n",
        "    value_loss = scalar_value_loss(value, target_value_phi[:, 0])\n",
        "    policy_loss = -(torch.log(policy_prob) * target_policy[:, 0]).sum(1)\n",
        "    reward_loss = torch.zeros(batch_size, device=device)\n",
        "    initial_state_values = value.detach()\n",
        "    gradient_scale = 1 / num_unroll_steps\n",
        "    for step_i in range(num_unroll_steps):\n",
        "        hidden_state, reward,policy_prob,value  = model.next_state(hidden_state, action_batch[:, step_i]) \n",
        "        #h,pred_reward,pred_policy,pred_value= net.next_state(h,act)\n",
        "        policy_loss += -(torch.log(policy_prob) * target_policy[:, step_i + 1]).sum(1)\n",
        "        value_loss += scalar_value_loss(value, target_value_phi[:, step_i + 1])\n",
        "        reward_loss += scalar_reward_loss(reward, target_reward_phi[:, step_i])\n",
        "        hidden_state.register_hook(lambda grad: grad * 0.5)\n",
        "\n",
        "    # optimize\n",
        "    if targetmodel is None:\n",
        "      value_loss_coeff = 1\n",
        "    else:  \n",
        "      value_loss_coeff = 0.25 #to reduce value overfiiting due to off policy \n",
        "    loss = (policy_loss + value_loss_coeff * value_loss + reward_loss) # find value loss coefficiet = 1?\n",
        "    weights = target_weights#/target_weights.max()#dividing by max doesnt work\n",
        "    weighted_loss = (weights * loss).mean()#1?\n",
        "    weighted_loss.register_hook(lambda grad: grad * gradient_scale)\n",
        "    loss = loss.mean()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    weighted_loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "    optimizer.step()\n",
        "    if per:\n",
        "      #remvoing 2nd forward pass can do it also should be chill???\n",
        "      #updated_h,updated_prob,updated_pred_value= model.inference_initial_state(obs_batch) \n",
        "      #return indexes,updated_pred_value.cpu().numpy()\n",
        "      return indexes,np.abs(catts(initial_state_values.cpu().numpy())-catts(target_value[:, 0].cpu().numpy())) +0.00001\n",
        "    return None,None  \n",
        "\n",
        "def adjust_lr(optimizer, step_count):\n",
        "\n",
        "    lr_init=0.05\n",
        "    lr_decay_rate=0.01\n",
        "    lr_decay_steps=10000\n",
        "    lr = lr_init * lr_decay_rate ** (step_count / lr_decay_steps)\n",
        "    lr = max(lr, 0.001)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    return lr\n",
        "def soft_update(target, source, tau):\n",
        "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
        "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
        "    return target\n",
        "def get_scalars(new_pred_values):\n",
        "    vals = []\n",
        "    for i in range(new_pred_values.shape[0]):\n",
        "      #print(new_pred_values[i,:].shape)\n",
        "      vals.append(catts(new_pred_values[i,:]))\n",
        "    return vals\n",
        "learning_rate = [0.05]   \n",
        "stp=[0]\n",
        "def net_train(net, targetnet, action_space_size, replay_buffer,discount,batch_size,num_unroll_steps, td_steps,training_steps=1000,target_update=50,tou=1,n_sim=50,per = False):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model =net\n",
        "    target_model = targetnet\n",
        "    #MuZeroNet(input_size=4, action_space_n=2, reward_support_size=5, value_support_size=5).to(device) #training fresh net\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate[0], momentum=0.9,weight_decay=1e-4)\n",
        "    #training_steps=training_steps=500#20000\n",
        "    # wait for replay buffer to be non-empty\n",
        "    while len(replay_buffer) == 0:\n",
        "        pass\n",
        "\n",
        "    for step_count in tqdm(range(training_steps)):\n",
        "        stp[0]+=1\n",
        "        learning_rate[0] = adjust_lr( optimizer, step_count)\n",
        "        indexes,new_priority = update_weights(model,target_model, action_space_size, optimizer, replay_buffer,discount,batch_size,num_unroll_steps, td_steps,n_sim,per)\n",
        "        if target_model is not None:\n",
        "          if stp[0] % target_update==0:\n",
        "            #print(\"softupdate \", tou)\n",
        "            soft_update(target=target_model, source=model, tau=tou)\n",
        "        if per:\n",
        "          #print(\"new pred val net train\",new_pred_values,new_pred_values.shape)\n",
        "          #new_pred_values = get_scalars(new_pred_values)\n",
        "          #print(\"new pred val net train\",new_pred_values)\n",
        "          update_priorites(replay_buffer,indexes,new_priority)\n",
        "\n",
        "    return model,target_model\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZWra51wFVvb",
        "outputId": "282cb5c1-38a5-4f52-e677-640e56ad6089"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "render = False\n",
        "episodes_per_train=30\n",
        "episodes_per_eval =5\n",
        "buffer =[]\n",
        "#buffer = deque(maxlen = episodes_per_train)\n",
        "training_steps=50\n",
        "max_steps=5000\n",
        "n_sim= 25\n",
        "discount = 0.99\n",
        "target_update_frewq = 1\n",
        "update_frac=0.01 # hard update\n",
        "batch_size = 126\n",
        "envs = ['CartPole-v1','MountainCar-v0','LunarLander-v2']\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"training for \",envs[0])\n",
        "env=gym.make(envs[0])\n",
        "#env=env.unwrapped\n",
        "env = ScalingObservationWrapper(env, low=[-2.4, -2.0, -0.42, -3.5], high=[2.4, 2.0, 0.42, 3.5])#only for cartpole\n",
        "\n",
        "s_dim =env.observation_space.shape[0]\n",
        "print(\"s_dim: \",s_dim)\n",
        "a_dim =env.action_space.n\n",
        "print(\"a_dim: \",a_dim)\n",
        "a_bound =1 #env.action_space.high[0]\n",
        "print(\"a_bound: \",a_bound)\n",
        "\n",
        "\n",
        "\n",
        "net = MuZeroNet(input_size=s_dim, action_space_n=a_dim, reward_support_size=15, value_support_size=15).to(device)\n",
        "targetnet = MuZeroNet(input_size=s_dim, action_space_n=a_dim, reward_support_size=15, value_support_size=15).to(device) #None for not using reanalyze\n",
        "targetnet = soft_update(target=targetnet, source=net, tau=1)#make them same\n",
        "for t in range(training_steps):\n",
        "  if t<0:\n",
        "    priority = True \n",
        "    tr_stp=20\n",
        "  else :\n",
        "    tr_stp=300\n",
        "    priority = True \n",
        "  if targetnet is None:\n",
        "    buffer =[] # onpolicy \n",
        "  for _ in range(episodes_per_train):\n",
        "    buffer.append(play_game(env,net,targetnet,n_sim,discount,render,device,a_dim,max_steps,td_steps=5,per=priority))\n",
        "  print(\"training from \",len(buffer),\" games\")  \n",
        "\n",
        "  print(\"training with \",\" priority \",priority,\" training_steps \",tr_stp,\" discount \",discount,\" batch_size \",batch_size)  \n",
        "  net,targetnet = net_train(net,targetnet,action_space_size=a_dim, replay_buffer=buffer,discount=discount,batch_size=batch_size,num_unroll_steps=5, \n",
        "                            td_steps=5,training_steps=tr_stp,target_update=target_update_frewq,tou=update_frac,n_sim=n_sim,per = priority)\n",
        "  if t>5:\n",
        "    for _ in range(episodes_per_eval):\n",
        "      eval_game(env,net,n_sim,render,device,max_steps)\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training for  CartPole-v1\n",
            "s_dim:  4\n",
            "a_dim:  2\n",
            "a_bound:  1\n",
            "DATA collection:played for  14  steps , rewards 14.0  last state  [-0.06145443 -0.5746942   0.57348211  0.57691253]\n",
            "DATA collection:played for  26  steps , rewards 26.0  last state  [-0.03618334 -0.57879118  0.54365153  0.6455156 ]\n",
            "DATA collection:played for  48  steps , rewards 48.0  last state  [ 0.0450806   0.19039865 -0.53910564 -0.34789166]\n",
            "DATA collection:played for  20  steps , rewards 20.0  last state  [ 0.06365533  0.18908547 -0.52870992 -0.28945359]\n",
            "DATA collection:played for  14  steps , rewards 14.0  last state  [-0.07124386 -0.78960839  0.56441739  0.7194434 ]\n",
            "DATA collection:played for  32  steps , rewards 32.0  last state  [ 0.08257008  0.21650297 -0.506793   -0.15345194]\n",
            "DATA collection:played for  28  steps , rewards 28.0  last state  [-0.05330461 -0.4203812   0.55689031  0.50365349]\n",
            "DATA collection:played for  32  steps , rewards 32.0  last state  [ 0.00622982  0.21268304 -0.5111647  -0.34480719]\n",
            "DATA collection:played for  18  steps , rewards 18.0  last state  [ 0.05436683  0.21053978 -0.51683258 -0.34118448]\n",
            "DATA collection:played for  13  steps , rewards 13.0  last state  [-0.02838801 -0.28847946  0.51307636  0.35019962]\n",
            "DATA collection:played for  33  steps , rewards 33.0  last state  [ 0.11387728  0.85225444 -0.51578429 -0.62593842]\n",
            "DATA collection:played for  17  steps , rewards 17.0  last state  [ 0.02764503  0.31612057 -0.52933035 -0.39954124]\n",
            "DATA collection:played for  14  steps , rewards 14.0  last state  [ 0.04153512  0.20073532 -0.50041317 -0.23871849]\n",
            "DATA collection:played for  17  steps , rewards 17.0  last state  [ 0.03297994  0.08702191 -0.50247424 -0.23041598]\n",
            "DATA collection:played for  11  steps , rewards 11.0  last state  [ 0.0636692   0.69172988 -0.56935832 -0.65671096]\n",
            "DATA collection:played for  24  steps , rewards 24.0  last state  [ 0.05598019  0.79408038 -0.53520782 -0.67986787]\n",
            "DATA collection:played for  22  steps , rewards 22.0  last state  [ 0.06119592  0.60512781 -0.53088834 -0.56546137]\n",
            "DATA collection:played for  20  steps , rewards 20.0  last state  [ 0.06724017  0.40689091 -0.50303749 -0.42877982]\n",
            "DATA collection:played for  76  steps , rewards 76.0  last state  [-0.19604675 -0.79340505  0.54201901  0.54648273]\n",
            "DATA collection:played for  15  steps , rewards 15.0  last state  [ 0.07244926  0.50815323 -0.53364027 -0.47394051]\n",
            "DATA collection:played for  14  steps , rewards 14.0  last state  [-0.06714769 -0.22256861  0.50802055  0.24735066]\n",
            "DATA collection:played for  33  steps , rewards 33.0  last state  [-0.12526895 -0.88715488  0.56431491  0.62461838]\n",
            "DATA collection:played for  13  steps , rewards 13.0  last state  [ 0.01986844  0.47099615 -0.50927858 -0.51012327]\n",
            "DATA collection:played for  17  steps , rewards 17.0  last state  [ 0.04209506  0.28032276 -0.53756565 -0.37941908]\n",
            "DATA collection:played for  37  steps , rewards 37.0  last state  [ 0.15046038  0.83571891 -0.59392812 -0.51850245]\n",
            "DATA collection:played for  14  steps , rewards 14.0  last state  [ 0.07141278  0.41240512 -0.53981574 -0.43043963]\n",
            "DATA collection:played for  12  steps , rewards 12.0  last state  [-0.05572701 -0.20796861  0.50043768  0.24113226]\n",
            "DATA collection:played for  16  steps , rewards 16.0  last state  [ 0.02276992 -0.01134843 -0.50613815 -0.18611908]\n",
            "DATA collection:played for  16  steps , rewards 16.0  last state  [ 0.0391699   0.41945469 -0.51288715 -0.40574514]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/300 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  20  steps , rewards 20.0  last state  [ 0.0583267   0.40367529 -0.50340462 -0.43223636]\n",
            "training from  30  games\n",
            "training with   priority  True  training_steps  300  discount  0.99  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [37:26<00:00,  7.49s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.07714393  0.97490656 -0.50255896 -0.84824182]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.08743727  0.96409753 -0.60303482 -0.89066878]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.07007788  0.86917507 -0.5927495  -0.8155901 ]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.08326343  0.9636825  -0.58426967 -0.88573319]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.0676862   0.96966151 -0.52802101 -0.86015457]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.08506863  0.97938436 -0.57354399 -0.86953185]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.07647542  0.96087293 -0.59939423 -0.87594764]\n",
            "DATA collection:played for  8  steps , rewards 8.0  last state  [ 0.05310481  0.7732298  -0.53130673 -0.73490708]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.04447155  0.8625303  -0.58512093 -0.80672873]\n",
            "DATA collection:played for  8  steps , rewards 8.0  last state  [ 0.06065596  0.76224175 -0.50779475 -0.73504294]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.06711315  0.98875208 -0.56758232 -0.8719858 ]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.09663544  0.99717742 -0.58157254 -0.87247573]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.06260246  0.95888596 -0.59797041 -0.87945444]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.06334007  0.89139678 -0.53511494 -0.79960238]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.06221956  0.97695798 -0.51464456 -0.84754026]\n",
            "DATA collection:played for  11  steps , rewards 11.0  last state  [ 0.0776847   1.05140548 -0.6393237  -0.95108209]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.07578002  0.88604054 -0.59595038 -0.8172617 ]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.05781699  0.98033912 -0.58305672 -0.88334674]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.0765534   0.98004328 -0.51950663 -0.86447732]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.0872526   0.98913093 -0.53934691 -0.87251962]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.04588634  0.89211182 -0.60622564 -0.82804947]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.05128101  0.96782916 -0.58295288 -0.8776138 ]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.07319788  0.90564999 -0.62047792 -0.83151569]\n",
            "DATA collection:played for  11  steps , rewards 11.0  last state  [ 0.10074686  1.08150006 -0.62638895 -0.94032897]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.08149241  0.96333063 -0.59193217 -0.87867305]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.07309712  0.8920228  -0.55491878 -0.80765698]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.07026332  0.99221324 -0.57479909 -0.87817409]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.05042124  0.86646092 -0.5405064  -0.79795082]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/300 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  10  steps , rewards 10.0  last state  [ 0.08826755  0.9779832  -0.52866589 -0.85535759]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [ 0.06155716  0.85984037 -0.56141202 -0.79969331]\n",
            "training from  60  games\n",
            "training with   priority  True  training_steps  300  discount  0.99  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [31:52<00:00,  6.37s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.07116491 -0.9606189   0.55430245  0.86238971]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.07226547 -0.96661599  0.56112593  0.85925911]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.08987286 -0.98902862  0.60857922  0.89048086]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.07626768 -0.96175547  0.52060011  0.85718509]\n",
            "DATA collection:played for  8  steps , rewards 8.0  last state  [-0.03095244 -0.78499065  0.54994613  0.74349692]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.06597959 -0.98955091  0.55084103  0.87410678]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.0879562  -0.9685832   0.53060665  0.85636345]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.08513525 -0.96548917  0.50444985  0.84892129]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.0801762  -1.00110213  0.52691698  0.85138886]\n",
            "DATA collection:played for  8  steps , rewards 8.0  last state  [-0.02470162 -0.77208879  0.54968538  0.7456731 ]\n",
            "DATA collection:played for  8  steps , rewards 8.0  last state  [-0.03755594 -0.79723232  0.53166238  0.73859667]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.0750983  -0.98553725  0.59960327  0.87170541]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.03951993 -0.88401064  0.53207461  0.78968426]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.06467036 -0.89344689  0.61218619  0.81548058]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.07407471 -0.99333806  0.50185663  0.84288561]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.04453706 -0.89699668  0.58926222  0.82424503]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.04996361 -0.86665693  0.61321451  0.81723059]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.04422736 -0.85877406  0.51594671  0.79269962]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.0769602  -0.95410347  0.52078527  0.85532589]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.06668704 -0.88135977  0.53647783  0.79987977]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.05444636 -0.97599017  0.60782448  0.87873041]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.06814034 -0.89009784  0.55192865  0.80310936]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.08788153 -0.95260907  0.51896632  0.8539958 ]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.06935284 -0.87747032  0.57462832  0.81703395]\n",
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.08040252 -0.89444301  0.51445508  0.78849612]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.09569887 -0.99502652  0.53203152  0.87053121]\n",
            "DATA collection:played for  10  steps , rewards 10.0  last state  [-0.07840866 -0.99018695  0.54261181  0.86511165]\n",
            "DATA collection:played for  8  steps , rewards 8.0  last state  [-0.03956609 -0.75980127  0.52815128  0.72602814]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/300 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  9  steps , rewards 9.0  last state  [-0.07173226 -0.8957186   0.54690878  0.7971703 ]\n",
            "DATA collection:played for  8  steps , rewards 8.0  last state  [-0.05668442 -0.80577136  0.50938258  0.71392537]\n",
            "training from  90  games\n",
            "training with   priority  True  training_steps  300  discount  0.99  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [29:46<00:00,  5.96s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  74  steps , rewards 74.0  last state  [0.22495214 0.53495874 0.50066385 0.04118511]\n",
            "DATA collection:played for  89  steps , rewards 89.0  last state  [0.21815935 0.45351489 0.5016483  0.14783018]\n",
            "DATA collection:played for  97  steps , rewards 97.0  last state  [0.22705973 0.4386713  0.51390553 0.20208032]\n",
            "DATA collection:played for  67  steps , rewards 67.0  last state  [0.18527692 0.61462516 0.50924819 0.00554831]\n",
            "DATA collection:played for  80  steps , rewards 80.0  last state  [0.21180824 0.36308525 0.506082   0.23373334]\n",
            "DATA collection:played for  83  steps , rewards 83.0  last state  [0.22471739 0.62975009 0.50313792 0.00796669]\n",
            "DATA collection:played for  103  steps , rewards 103.0  last state  [0.16874726 0.45287858 0.50585312 0.09050702]\n",
            "DATA collection:played for  64  steps , rewards 64.0  last state  [0.25736497 0.55803229 0.52821502 0.1400701 ]\n",
            "DATA collection:played for  105  steps , rewards 105.0  last state  [ 1.99165938e-01  6.11803075e-01  5.00407197e-01 -5.29099629e-05]\n",
            "DATA collection:played for  112  steps , rewards 112.0  last state  [0.09061223 0.53515618 0.50025936 0.02530552]\n",
            "DATA collection:played for  80  steps , rewards 80.0  last state  [0.24388086 0.52785578 0.50047528 0.14119184]\n",
            "DATA collection:played for  84  steps , rewards 84.0  last state  [0.1910406  0.53490236 0.49931649 0.06697371]\n",
            "DATA collection:played for  94  steps , rewards 94.0  last state  [0.26320414 0.52151438 0.50011801 0.15737369]\n",
            "DATA collection:played for  80  steps , rewards 80.0  last state  [0.20630282 0.52146953 0.49994509 0.11156417]\n",
            "DATA collection:played for  95  steps , rewards 95.0  last state  [0.25907106 0.6371842  0.50610917 0.00435582]\n",
            "DATA collection:played for  78  steps , rewards 78.0  last state  [0.27229394 0.55576606 0.50761204 0.12228198]\n",
            "DATA collection:played for  79  steps , rewards 79.0  last state  [0.19951398 0.46292345 0.50718885 0.16438473]\n",
            "DATA collection:played for  111  steps , rewards 111.0  last state  [0.25617305 0.6268779  0.50053645 0.07284188]\n",
            "DATA collection:played for  115  steps , rewards 115.0  last state  [0.241646   0.61638285 0.51370218 0.05315258]\n",
            "DATA collection:played for  84  steps , rewards 84.0  last state  [0.20559453 0.55332218 0.51573786 0.09949365]\n",
            "DATA collection:played for  85  steps , rewards 85.0  last state  [0.18767082 0.65841815 0.50698175 0.00310105]\n",
            "DATA collection:played for  71  steps , rewards 71.0  last state  [0.20534547 0.47003752 0.51801337 0.12375225]\n",
            "DATA collection:played for  103  steps , rewards 103.0  last state  [0.28437214 0.60455997 0.5159386  0.12389011]\n",
            "DATA collection:played for  72  steps , rewards 72.0  last state  [0.17143646 0.54117668 0.52136137 0.0776581 ]\n",
            "DATA collection:played for  84  steps , rewards 84.0  last state  [0.18808768 0.34943542 0.50306044 0.21547628]\n",
            "DATA collection:played for  135  steps , rewards 135.0  last state  [0.1872081  0.63790833 0.50905609 0.00263194]\n",
            "DATA collection:played for  76  steps , rewards 76.0  last state  [0.17563331 0.35619242 0.51090927 0.20668599]\n",
            "DATA collection:played for  72  steps , rewards 72.0  last state  [0.23410737 0.56185877 0.52277873 0.09275659]\n",
            "DATA collection:played for  85  steps , rewards 85.0  last state  [0.1859379  0.63057588 0.50390972 0.00942782]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/300 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  103  steps , rewards 103.0  last state  [0.23064003 0.62829437 0.5012131  0.0414467 ]\n",
            "training from  120  games\n",
            "training with   priority  True  training_steps  300  discount  0.99  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [35:14<00:00,  7.05s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  133  steps , rewards 133.0  last state  [1.0107071  1.01416674 0.22029396 0.04734531]\n",
            "DATA collection:played for  142  steps , rewards 142.0  last state  [1.01216939 1.09736025 0.25412125 0.0088543 ]\n",
            "DATA collection:played for  133  steps , rewards 133.0  last state  [ 1.01449291  1.16682735  0.28660973 -0.07024686]\n",
            "DATA collection:played for  135  steps , rewards 135.0  last state  [ 1.0147418   1.18318757  0.31019597 -0.04197557]\n",
            "DATA collection:played for  146  steps , rewards 146.0  last state  [1.01480494 1.06613463 0.25216245 0.01123302]\n",
            "DATA collection:played for  142  steps , rewards 142.0  last state  [1.0107561  1.07201638 0.24572885 0.0465788 ]\n",
            "DATA collection:played for  134  steps , rewards 134.0  last state  [1.01429813 1.10862529 0.28269719 0.00699123]\n",
            "DATA collection:played for  134  steps , rewards 134.0  last state  [ 1.01516563  1.26587787  0.29341397 -0.09647253]\n",
            "DATA collection:played for  138  steps , rewards 138.0  last state  [ 1.00496538  1.07983707  0.26021928 -0.01449167]\n",
            "DATA collection:played for  147  steps , rewards 147.0  last state  [ 1.01473349  1.1698661   0.26447703 -0.05702275]\n",
            "DATA collection:played for  124  steps , rewards 124.0  last state  [1.01410257 1.08035895 0.27003479 0.07619078]\n",
            "DATA collection:played for  134  steps , rewards 134.0  last state  [ 1.00299186  1.08925933  0.26908468 -0.01092621]\n",
            "DATA collection:played for  147  steps , rewards 147.0  last state  [1.00690288 0.97726987 0.2353369  0.05035517]\n",
            "DATA collection:played for  144  steps , rewards 144.0  last state  [ 1.01342464  1.07126414  0.21582721 -0.03210143]\n",
            "DATA collection:played for  142  steps , rewards 142.0  last state  [1.00955426 1.08006744 0.27018533 0.01645262]\n",
            "DATA collection:played for  155  steps , rewards 155.0  last state  [1.00672762 0.96893056 0.26270071 0.10117702]\n",
            "DATA collection:played for  142  steps , rewards 142.0  last state  [1.00968324 1.07284371 0.2597578  0.00650746]\n",
            "DATA collection:played for  143  steps , rewards 143.0  last state  [ 1.00148409  1.2000018   0.2587241  -0.05644989]\n",
            "DATA collection:played for  136  steps , rewards 136.0  last state  [1.00401534 1.07852352 0.29509331 0.03483767]\n",
            "DATA collection:played for  147  steps , rewards 147.0  last state  [ 1.00669994  1.1694695   0.30277399 -0.03445774]\n",
            "DATA collection:played for  149  steps , rewards 149.0  last state  [1.01215428 0.99285039 0.2612079  0.10972657]\n",
            "DATA collection:played for  130  steps , rewards 130.0  last state  [1.01180835 1.0999996  0.29956349 0.03473313]\n",
            "DATA collection:played for  126  steps , rewards 126.0  last state  [1.0037722  1.10443402 0.25236412 0.03204032]\n",
            "DATA collection:played for  145  steps , rewards 145.0  last state  [1.0086126  0.97854223 0.24028077 0.04876437]\n",
            "DATA collection:played for  137  steps , rewards 137.0  last state  [1.00798974 1.00122769 0.26320635 0.0918066 ]\n",
            "DATA collection:played for  143  steps , rewards 143.0  last state  [ 1.0018      1.17363734  0.27062532 -0.08769798]\n",
            "DATA collection:played for  142  steps , rewards 142.0  last state  [1.0131466  1.06483356 0.27973581 0.05052703]\n",
            "DATA collection:played for  134  steps , rewards 134.0  last state  [1.01437846 1.09212441 0.25576731 0.01503851]\n",
            "DATA collection:played for  142  steps , rewards 142.0  last state  [ 1.00130578  1.09087158  0.24143383 -0.00814766]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/300 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  138  steps , rewards 138.0  last state  [1.0072197  1.08371418 0.24370753 0.0036811 ]\n",
            "training from  150  games\n",
            "training with   priority  True  training_steps  300  discount  0.99  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [37:09<00:00,  7.43s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.28242896  0.55075895  0.23239925  0.05916297]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.38344573  0.52688167  0.26082367 -0.00170789]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-4.11027608e-01  5.33349336e-01  2.94709868e-01 -1.10685065e-04]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.23734125  0.93386289  0.27431329 -0.10995987]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.32349594  0.55205925  0.2970811   0.08856572]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.23451983  0.9179481   0.36571616 -0.03888524]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.29422487  0.55649122  0.30777002  0.07111036]\n",
            "DATA collection:played for  216  steps , rewards 216.0  last state  [ 1.00104033  0.19621063 -0.16352007 -0.04723985]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.40878596  0.53610113  0.2869575   0.09716943]\n",
            "DATA collection:played for  231  steps , rewards 231.0  last state  [ 1.00172134  0.07835567 -0.18070132  0.04114208]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.22126121  0.718928    0.301279   -0.04613951]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.23943661  0.71639301  0.26672851 -0.03417386]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.17817593  0.73542653  0.28199076  0.0521985 ]\n",
            "DATA collection:played for  195  steps , rewards 195.0  last state  [ 1.00150395  0.09814287 -0.18046484  0.0825607 ]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.23549763  0.72200801  0.26558107 -0.04835315]\n",
            "DATA collection:played for  226  steps , rewards 226.0  last state  [ 1.0008461   0.16213318 -0.20328063 -0.03683963]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.20897968  0.5734622   0.20331594 -0.05157939]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.28529562  0.72588987  0.30923671 -0.00961401]\n",
            "DATA collection:played for  217  steps , rewards 217.0  last state  [ 1.00002288  0.09388106 -0.20128545  0.00187702]\n",
            "DATA collection:played for  220  steps , rewards 220.0  last state  [ 1.00060524  0.16464033 -0.17036679 -0.01607601]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-1.87372637e-01  7.32862892e-01  3.01834899e-01 -4.07986281e-04]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.24766187  0.51852319  0.26489688  0.05103048]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.39658314  0.5734838   0.29808977 -0.07269979]\n",
            "DATA collection:played for  207  steps , rewards 207.0  last state  [ 1.00237265  0.09413457 -0.20637561  0.02483369]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.44152167  0.54044076  0.35828517 -0.01464137]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.46777211  0.34782761  0.28096074  0.06709844]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.33200423  0.53634222  0.30694732 -0.01421973]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.34788464  0.54604012  0.29565132 -0.07131874]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.12371932  0.92901079  0.23972647 -0.10941258]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/300 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.23661723  0.33428806  0.25399506  0.10335626]\n",
            "training from  180  games\n",
            "training with   priority  True  training_steps  300  discount  0.99  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [38:37<00:00,  7.72s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  169  steps , rewards 169.0  last state  [ 0.85223155  1.18143419  0.50078701 -0.03362801]\n",
            "DATA collection:played for  140  steps , rewards 140.0  last state  [0.82577476 1.09075991 0.49894012 0.0406008 ]\n",
            "DATA collection:played for  203  steps , rewards 203.0  last state  [0.9215422  1.1791694  0.50465549 0.02355622]\n",
            "DATA collection:played for  217  steps , rewards 217.0  last state  [0.88024666 1.18648748 0.49891006 0.00687849]\n",
            "DATA collection:played for  191  steps , rewards 191.0  last state  [0.88613831 1.17308893 0.50503768 0.03650258]\n",
            "DATA collection:played for  223  steps , rewards 223.0  last state  [0.99795175 1.17772285 0.50495108 0.03233256]\n",
            "DATA collection:played for  297  steps , rewards 297.0  last state  [ 0.85308802  1.16257013  0.50169315 -0.01394571]\n",
            "DATA collection:played for  285  steps , rewards 285.0  last state  [ 0.87377611  1.17603055  0.50560014 -0.01994252]\n",
            "DATA collection:played for  189  steps , rewards 189.0  last state  [0.93521991 1.16289896 0.50558566 0.01115483]\n",
            "DATA collection:played for  163  steps , rewards 163.0  last state  [ 0.87209328  1.1860905   0.50147018 -0.034463  ]\n",
            "DATA collection:played for  194  steps , rewards 194.0  last state  [ 0.98533982  1.28167167  0.50225285 -0.03784373]\n",
            "DATA collection:played for  137  steps , rewards 137.0  last state  [ 0.87569327  1.20631138  0.50462573 -0.02572692]\n",
            "DATA collection:played for  215  steps , rewards 215.0  last state  [0.87072268 1.18021371 0.5004959  0.00581909]\n",
            "DATA collection:played for  209  steps , rewards 209.0  last state  [1.02080379 1.16018289 0.4870542  0.05938108]\n",
            "DATA collection:played for  201  steps , rewards 201.0  last state  [0.96606314 1.17314064 0.49878584 0.03494494]\n",
            "DATA collection:played for  228  steps , rewards 228.0  last state  [0.95431902 1.07669123 0.4990829  0.12283877]\n",
            "DATA collection:played for  191  steps , rewards 191.0  last state  [0.9235094  1.15767142 0.50808975 0.03334   ]\n",
            "DATA collection:played for  221  steps , rewards 221.0  last state  [ 0.85739767  1.1669725   0.50023292 -0.01959573]\n",
            "DATA collection:played for  208  steps , rewards 208.0  last state  [0.8298725  1.08758316 0.50054089 0.02297193]\n",
            "DATA collection:played for  226  steps , rewards 226.0  last state  [1.0069554  1.0903942  0.48125295 0.09904171]\n",
            "DATA collection:played for  202  steps , rewards 202.0  last state  [ 1.01806389e+00  1.28229189e+00  5.02385338e-01 -8.59761891e-04]\n",
            "DATA collection:played for  211  steps , rewards 211.0  last state  [0.90768421 1.17540183 0.51395331 0.03369248]\n",
            "DATA collection:played for  255  steps , rewards 255.0  last state  [-1.0005052  -0.97644095 -0.45760663 -0.07915162]\n",
            "DATA collection:played for  186  steps , rewards 186.0  last state  [ 1.00579804  1.29701107  0.49811717 -0.03643675]\n",
            "DATA collection:played for  195  steps , rewards 195.0  last state  [0.94677193 1.18108504 0.50511747 0.01210841]\n",
            "DATA collection:played for  182  steps , rewards 182.0  last state  [ 1.00339014  1.28674105  0.49641644 -0.04190214]\n",
            "DATA collection:played for  177  steps , rewards 177.0  last state  [0.96317866 1.19306793 0.50566762 0.03125498]\n",
            "DATA collection:played for  215  steps , rewards 215.0  last state  [ 0.87781775  1.2019579   0.50203882 -0.03405806]\n",
            "DATA collection:played for  177  steps , rewards 177.0  last state  [-1.00078116 -1.00592673 -0.46729633 -0.08865336]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/300 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  200  steps , rewards 200.0  last state  [ 1.00999451  1.27362371  0.49814111 -0.01127744]\n",
            "training from  210  games\n",
            "training with   priority  True  training_steps  300  discount  0.99  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [39:03<00:00,  7.81s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  243.0  rewards  last state  [1.00300271 0.78880332 0.25732053 0.07239702]\n",
            "Eval:played for  255.0  rewards  last state  [1.00198698 0.8265093  0.25568599 0.06998368]\n",
            "Eval:played for  287.0  rewards  last state  [1.0031707  0.80961856 0.24004708 0.03716936]\n",
            "Eval:played for  206.0  rewards  last state  [ 1.00255218  0.93615246  0.25854404 -0.05249778]\n",
            "Eval:played for  257.0  rewards  last state  [1.0070167  0.79537533 0.27716672 0.09730089]\n",
            "DATA collection:played for  250  steps , rewards 250.0  last state  [ 1.01069096  0.90179201  0.26542703 -0.01434071]\n",
            "DATA collection:played for  223  steps , rewards 223.0  last state  [1.01513125 0.81133821 0.24324349 0.06337843]\n",
            "DATA collection:played for  236  steps , rewards 236.0  last state  [ 1.01351407  0.92851644  0.26219012 -0.03757852]\n",
            "DATA collection:played for  253  steps , rewards 253.0  last state  [1.01504461 0.819756   0.26518424 0.07984251]\n",
            "DATA collection:played for  261  steps , rewards 261.0  last state  [ 1.00906564  0.99198161  0.26048713 -0.0514178 ]\n",
            "DATA collection:played for  286  steps , rewards 286.0  last state  [ 1.00467858  0.90787599  0.25536935 -0.02389343]\n",
            "DATA collection:played for  249  steps , rewards 249.0  last state  [1.00809263 0.80252026 0.23433509 0.04278465]\n",
            "DATA collection:played for  247  steps , rewards 247.0  last state  [1.00719513 0.80963699 0.25081393 0.05122643]\n",
            "DATA collection:played for  231  steps , rewards 231.0  last state  [1.01375268 0.82187939 0.25506375 0.04848323]\n",
            "DATA collection:played for  181  steps , rewards 181.0  last state  [1.00559953 0.81708122 0.26405515 0.08207653]\n",
            "DATA collection:played for  277  steps , rewards 277.0  last state  [ 1.00517422  0.98797289  0.27208126 -0.04726033]\n",
            "DATA collection:played for  271  steps , rewards 271.0  last state  [1.01278684 0.81233851 0.26317949 0.07846384]\n",
            "DATA collection:played for  219  steps , rewards 219.0  last state  [1.00528154 0.8353113  0.25204004 0.01489133]\n",
            "DATA collection:played for  275  steps , rewards 275.0  last state  [1.00303328 0.81523329 0.22850026 0.05610824]\n",
            "DATA collection:played for  236  steps , rewards 236.0  last state  [1.00577295 0.73853542 0.22613149 0.05195671]\n",
            "DATA collection:played for  192  steps , rewards 192.0  last state  [ 1.01185075  0.92924812  0.26417503 -0.02540373]\n",
            "DATA collection:played for  219  steps , rewards 219.0  last state  [ 1.01061571  0.83291491  0.225711   -0.00976254]\n",
            "DATA collection:played for  248  steps , rewards 248.0  last state  [ 1.00672808  0.93326102  0.25449429 -0.06435804]\n",
            "DATA collection:played for  255  steps , rewards 255.0  last state  [1.01276653 0.79671261 0.2561686  0.07643639]\n",
            "DATA collection:played for  269  steps , rewards 269.0  last state  [ 1.01189703  0.99323341  0.26403207 -0.07072774]\n",
            "DATA collection:played for  253  steps , rewards 253.0  last state  [1.00791717 0.79530552 0.23274177 0.07304147]\n",
            "DATA collection:played for  228  steps , rewards 228.0  last state  [1.00306801 0.91051549 0.2901463  0.00958771]\n",
            "DATA collection:played for  233  steps , rewards 233.0  last state  [1.01308874 0.80671534 0.2615817  0.06706242]\n",
            "DATA collection:played for  271  steps , rewards 271.0  last state  [1.00076468 0.83711446 0.26303332 0.06901454]\n",
            "DATA collection:played for  234  steps , rewards 234.0  last state  [1.0033922  0.72736018 0.2178742  0.06586991]\n",
            "DATA collection:played for  225  steps , rewards 225.0  last state  [1.01445442 0.82577207 0.24579228 0.03329416]\n",
            "DATA collection:played for  261  steps , rewards 261.0  last state  [1.01414068 0.7940663  0.22499776 0.03103091]\n",
            "DATA collection:played for  275  steps , rewards 275.0  last state  [1.01263474 0.81146271 0.23405642 0.01603229]\n",
            "DATA collection:played for  191  steps , rewards 191.0  last state  [1.00949066 0.80469448 0.24822447 0.08000245]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/300 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  254  steps , rewards 254.0  last state  [1.00959308 0.74428566 0.22315488 0.0748016 ]\n",
            "training from  240  games\n",
            "training with   priority  True  training_steps  300  discount  0.99  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [40:06<00:00,  8.02s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  190.0  rewards  last state  [ 0.74267848  1.10152803  0.50023632 -0.04509647]\n",
            "Eval:played for  500.0  rewards  last state  [-0.89035161 -0.00237573  0.01150435 -0.08384898]\n",
            "Eval:played for  263.0  rewards  last state  [0.81470553 0.98991192 0.49884151 0.10697603]\n",
            "Eval:played for  168.0  rewards  last state  [0.82134386 1.10426296 0.50787726 0.03251296]\n",
            "Eval:played for  151.0  rewards  last state  [0.74405088 1.01064673 0.50642077 0.0507861 ]\n",
            "DATA collection:played for  178  steps , rewards 178.0  last state  [0.78333119 1.10811827 0.50355197 0.00238893]\n",
            "DATA collection:played for  350  steps , rewards 350.0  last state  [0.843853   1.08349454 0.50367554 0.01819454]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.97629378 -0.00762963  0.01809107 -0.07675235]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.70101608  0.01579044 -0.00079685 -0.07977105]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.65391836 -0.17722417 -0.00946936  0.09445924]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.08746319 -0.01783278  0.00547494  0.03124657]\n",
            "DATA collection:played for  233  steps , rewards 233.0  last state  [0.74861265 0.99859451 0.50463301 0.05143472]\n",
            "DATA collection:played for  242  steps , rewards 242.0  last state  [ 0.81644825  1.08797918  0.49921797 -0.00388293]\n",
            "DATA collection:played for  260  steps , rewards 260.0  last state  [0.80560899 1.0892226  0.49897927 0.00475895]\n",
            "DATA collection:played for  148  steps , rewards 148.0  last state  [0.78197988 1.10353393 0.50310163 0.00979343]\n",
            "DATA collection:played for  147  steps , rewards 147.0  last state  [0.79016773 0.97175069 0.50458281 0.13972312]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.19139386 -0.01453483 -0.01551901  0.03052743]\n",
            "DATA collection:played for  214  steps , rewards 214.0  last state  [0.86836955 1.09792898 0.51535803 0.03980582]\n",
            "DATA collection:played for  171  steps , rewards 171.0  last state  [0.75608609 1.01996954 0.50905193 0.05103835]\n",
            "DATA collection:played for  246  steps , rewards 246.0  last state  [0.88018425 1.08883173 0.50099269 0.03364601]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [0.51922756 0.53558174 0.29322685 0.04218136]\n",
            "DATA collection:played for  202  steps , rewards 202.0  last state  [0.79237619 0.91016733 0.50250159 0.12979395]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [0.53156795 0.53301283 0.33903905 0.09186458]\n",
            "DATA collection:played for  153  steps , rewards 153.0  last state  [0.81045867 1.01655861 0.5116809  0.0794415 ]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.52363439  0.02962422  0.01497431 -0.07587969]\n",
            "DATA collection:played for  189  steps , rewards 189.0  last state  [0.75116346 1.0048162  0.512686   0.05874103]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.00772909 -0.02462609  0.00735074  0.02082606]\n",
            "DATA collection:played for  186  steps , rewards 186.0  last state  [0.87911801 1.10506103 0.5105209  0.0465648 ]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.58110086  0.02438335  0.00103769 -0.0759373 ]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [-0.73807585  0.01103509  0.00288047 -0.07572078]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.00780392 -0.02594197 -0.00375133  0.02263497]\n",
            "DATA collection:played for  183  steps , rewards 183.0  last state  [0.74807217 1.00795037 0.50285123 0.05182382]\n",
            "DATA collection:played for  203  steps , rewards 203.0  last state  [0.76618392 1.0200085  0.5145664  0.05998531]\n",
            "DATA collection:played for  183  steps , rewards 183.0  last state  [0.7979709  1.01225298 0.49912222 0.06252265]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/300 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.13081442 -0.01670612 -0.00473851  0.03089188]\n",
            "training from  270  games\n",
            "training with   priority  True  training_steps  300  discount  0.99  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [39:23<00:00,  7.88s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  116.0  rewards  last state  [-0.75499519 -1.09766485 -0.51240417 -0.06408163]\n",
            "Eval:played for  124.0  rewards  last state  [-0.75466789 -1.0857188  -0.49979674 -0.07807447]\n",
            "Eval:played for  112.0  rewards  last state  [-0.76280649 -1.10798329 -0.51386314 -0.07350155]\n",
            "Eval:played for  118.0  rewards  last state  [-0.76034418 -1.08728801 -0.50238015 -0.0586083 ]\n",
            "Eval:played for  128.0  rewards  last state  [-0.75548857 -1.06102243 -0.49947875 -0.10938937]\n",
            "DATA collection:played for  110  steps , rewards 110.0  last state  [-0.71738118 -1.09325265 -0.5036032  -0.04196963]\n",
            "DATA collection:played for  113  steps , rewards 113.0  last state  [-0.62425987 -0.98342553 -0.49937604 -0.05440216]\n",
            "DATA collection:played for  123  steps , rewards 123.0  last state  [-0.77165064 -1.16867968 -0.51109993 -0.0251122 ]\n",
            "DATA collection:played for  121  steps , rewards 121.0  last state  [-0.75689514 -1.16989282 -0.50116649  0.01338513]\n",
            "DATA collection:played for  122  steps , rewards 122.0  last state  [-0.69897688 -1.07271611 -0.50900637 -0.06751046]\n",
            "DATA collection:played for  104  steps , rewards 104.0  last state  [-0.70740949 -1.10123084 -0.50909803 -0.04868336]\n",
            "DATA collection:played for  114  steps , rewards 114.0  last state  [-0.67414163 -1.07962249 -0.50420609 -0.04372657]\n",
            "DATA collection:played for  127  steps , rewards 127.0  last state  [-0.72344723 -1.00617512 -0.50132825 -0.13046501]\n",
            "DATA collection:played for  127  steps , rewards 127.0  last state  [-0.77805766 -1.19365085 -0.50175478 -0.02231452]\n",
            "DATA collection:played for  139  steps , rewards 139.0  last state  [-0.76737935 -1.16566722 -0.51433024 -0.0704177 ]\n",
            "DATA collection:played for  124  steps , rewards 124.0  last state  [-0.62216893 -1.08879405 -0.50329651  0.01326889]\n",
            "DATA collection:played for  126  steps , rewards 126.0  last state  [-0.64571913 -1.07996542 -0.50965886 -0.02588813]\n",
            "DATA collection:played for  135  steps , rewards 135.0  last state  [-0.73473748 -1.15817693 -0.50656436 -0.03224157]\n",
            "DATA collection:played for  117  steps , rewards 117.0  last state  [-0.80775521 -1.16314155 -0.51886538 -0.06535167]\n",
            "DATA collection:played for  143  steps , rewards 143.0  last state  [-0.75068318 -1.16489325 -0.50749613 -0.00286343]\n",
            "DATA collection:played for  123  steps , rewards 123.0  last state  [-0.68053279 -1.16194915 -0.50063072  0.02683449]\n",
            "DATA collection:played for  137  steps , rewards 137.0  last state  [-0.79421233 -1.20198795 -0.50856758 -0.02725174]\n",
            "DATA collection:played for  120  steps , rewards 120.0  last state  [-0.6993979  -1.11232608 -0.51395237 -0.04157155]\n",
            "DATA collection:played for  125  steps , rewards 125.0  last state  [-0.72373141 -1.16083576 -0.50454327 -0.01582604]\n",
            "DATA collection:played for  127  steps , rewards 127.0  last state  [-0.55294385 -0.98073864 -0.49936151 -0.0470201 ]\n",
            "DATA collection:played for  118  steps , rewards 118.0  last state  [-0.71847894 -1.07382385 -0.49954334 -0.0418276 ]\n",
            "DATA collection:played for  132  steps , rewards 132.0  last state  [-0.80720966 -1.06328191 -0.50157509 -0.12511405]\n",
            "DATA collection:played for  117  steps , rewards 117.0  last state  [-0.75956386 -1.1655633  -0.49950867 -0.02349655]\n",
            "DATA collection:played for  120  steps , rewards 120.0  last state  [-0.66024583 -1.09427275 -0.50107604 -0.01727368]\n",
            "DATA collection:played for  133  steps , rewards 133.0  last state  [-0.77731475 -1.19467967 -0.50826722 -0.03389565]\n",
            "DATA collection:played for  117  steps , rewards 117.0  last state  [-0.77883309 -1.18385044 -0.50190263 -0.01439454]\n",
            "DATA collection:played for  131  steps , rewards 131.0  last state  [-0.72602693 -1.16090435 -0.50237252 -0.00878289]\n",
            "DATA collection:played for  143  steps , rewards 143.0  last state  [-7.63624698e-01 -1.17089807e+00 -5.03757996e-01  8.25941757e-04]\n",
            "DATA collection:played for  120  steps , rewards 120.0  last state  [-0.70862742 -1.10278056 -0.50924877 -0.04147513]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/300 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  125  steps , rewards 125.0  last state  [-0.80035624 -1.18899172 -0.50004162 -0.02119233]\n",
            "training from  300  games\n",
            "training with   priority  True  training_steps  300  discount  0.99  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [39:31<00:00,  7.90s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  173.0  rewards  last state  [-1.01154971 -1.18326853 -0.4339288  -0.00941023]\n",
            "Eval:played for  500.0  rewards  last state  [ 0.50476108 -0.02111064  0.00456205  0.07626672]\n",
            "Eval:played for  151.0  rewards  last state  [-1.00555737 -1.16901787 -0.42388321 -0.03288265]\n",
            "Eval:played for  194.0  rewards  last state  [-1.00681886 -1.07250382 -0.42334195 -0.09893381]\n",
            "Eval:played for  199.0  rewards  last state  [-1.00219598e+00 -1.16247288e+00 -4.19126160e-01  2.36435692e-04]\n",
            "DATA collection:played for  209  steps , rewards 209.0  last state  [-1.00337589 -1.15821196 -0.41032333  0.00266363]\n",
            "DATA collection:played for  188  steps , rewards 188.0  last state  [-1.00970544 -1.09174987 -0.42071403 -0.09133945]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.67397474 -0.01441378 -0.00952778  0.08703726]\n",
            "DATA collection:played for  181  steps , rewards 181.0  last state  [-1.00379915 -1.16728715 -0.47163651 -0.05326372]\n",
            "DATA collection:played for  171  steps , rewards 171.0  last state  [-1.00853083 -1.20425355 -0.43984743 -0.02131971]\n",
            "DATA collection:played for  145  steps , rewards 145.0  last state  [-1.0001735  -1.2070929  -0.44664592 -0.00362657]\n",
            "DATA collection:played for  472  steps , rewards 472.0  last state  [1.00104834 0.1963203  0.08355356 0.01891114]\n",
            "DATA collection:played for  152  steps , rewards 152.0  last state  [-1.00616759 -1.1002779  -0.39038615 -0.05785525]\n",
            "DATA collection:played for  424  steps , rewards 424.0  last state  [1.00157531 0.19691488 0.11708862 0.09051224]\n",
            "DATA collection:played for  142  steps , rewards 142.0  last state  [-1.011656   -1.10463986 -0.41379705 -0.05116107]\n",
            "DATA collection:played for  177  steps , rewards 177.0  last state  [-1.00147263 -1.17864748 -0.40301828  0.00967245]\n",
            "DATA collection:played for  196  steps , rewards 196.0  last state  [-1.00504701 -1.06710676 -0.42573743 -0.10746106]\n",
            "DATA collection:played for  188  steps , rewards 188.0  last state  [-1.01061673 -1.09102077 -0.435093   -0.05920744]\n",
            "DATA collection:played for  249  steps , rewards 249.0  last state  [-1.00959234 -1.16611029 -0.43447879 -0.08613331]\n",
            "DATA collection:played for  189  steps , rewards 189.0  last state  [-1.00115442 -1.18065923 -0.46676689 -0.02965759]\n",
            "DATA collection:played for  170  steps , rewards 170.0  last state  [-1.01857663 -1.10203455 -0.41965092 -0.09546339]\n",
            "DATA collection:played for  151  steps , rewards 151.0  last state  [-1.00665755 -1.16356254 -0.44191371 -0.03581413]\n",
            "DATA collection:played for  500  steps , rewards 500.0  last state  [ 0.75938309  0.18518652  0.00515345 -0.06363157]\n",
            "DATA collection:played for  160  steps , rewards 160.0  last state  [-1.00901461 -1.08974219 -0.43417676 -0.10305439]\n",
            "DATA collection:played for  168  steps , rewards 168.0  last state  [-1.00887875 -1.09191423 -0.39732135 -0.08879259]\n",
            "DATA collection:played for  459  steps , rewards 459.0  last state  [ 1.00280595  0.29599295  0.07990187 -0.06332609]\n",
            "DATA collection:played for  165  steps , rewards 165.0  last state  [-1.00234981 -1.203018   -0.44285521  0.02566416]\n",
            "DATA collection:played for  151  steps , rewards 151.0  last state  [-1.01413671 -1.20090353 -0.42282317  0.00794038]\n",
            "DATA collection:played for  194  steps , rewards 194.0  last state  [-1.00352894 -1.06167127 -0.40067427 -0.09287915]\n",
            "DATA collection:played for  177  steps , rewards 177.0  last state  [-1.00642901e+00 -1.18580472e+00 -4.38756119e-01  5.13761271e-04]\n",
            "DATA collection:played for  160  steps , rewards 160.0  last state  [-1.00691198 -1.06633734 -0.40773835 -0.10375997]\n",
            "DATA collection:played for  164  steps , rewards 164.0  last state  [-1.01635351 -1.09521998 -0.41922419 -0.08322393]\n",
            "DATA collection:played for  201  steps , rewards 201.0  last state  [-1.00653819 -1.16660629 -0.42296563 -0.00368231]\n",
            "DATA collection:played for  150  steps , rewards 150.0  last state  [-1.01410972 -1.10781867 -0.39768424 -0.05240412]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/300 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  158  steps , rewards 158.0  last state  [-1.01039063 -1.08660947 -0.42029907 -0.08896607]\n",
            "training from  330  games\n",
            "training with   priority  True  training_steps  300  discount  0.99  batch_size  126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 89%|████████▉ | 267/300 [35:32<04:29,  8.16s/it]"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}