{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "muzero_simple_but_better.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPa6VKAThxm3F1D0Kp7eEOs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rootAkash/reinforcement_learning/blob/master/muzero_simple_but_better.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Y_uk-bvSKg6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQRen3PlNkiB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "227cf806-4a8a-420b-e611-727b3ffa81dd"
      },
      "source": [
        "import numpy as np\n",
        "def stcat(x,support=5):\n",
        "  x = np.sign(x) * ((abs(x) + 1)**0.5 - 1) + 0.001 * x\n",
        "  x = np.clip(x, -support, support)\n",
        "  floor = np.floor(x)\n",
        "  prob = x - floor\n",
        "  logits = np.zeros( 2 * support + 1)\n",
        "  first_index = int(floor + support)\n",
        "  second_index = int(floor + support+1)\n",
        "  logits[first_index] = 1-prob\n",
        "  if prob>0:\n",
        "    logits[second_index] = prob\n",
        "  return logits\n",
        "def catts(x,support=5):\n",
        "  support = np.arange(-support, support+1, 1)\n",
        "  x = np.sum(support*x)\n",
        "  x = np.sign(x) * ((((1 + 4 * 0.001 * (abs(x) + 1 + 0.001))**0.5 - 1) / (2 * 0.001))** 2- 1)\n",
        "  return x  \n",
        "\n",
        "#cat = stcat(58705)\n",
        "#print(cat)\n",
        "#scalar = catts(cat)\n",
        "#print(scalar)\n",
        "print(\"done\")        \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhf3dyDa2GYq",
        "outputId": "1cb2387b-e8cd-4fc9-91aa-d734eddfe537"
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MuZeroNet(nn.Module):\n",
        "    def __init__(self, input_size, action_space_n, reward_support_size, value_support_size):\n",
        "        super().__init__()\n",
        "        self.hx_size = 32\n",
        "        self._representation = nn.Sequential(nn.Linear(input_size, self.hx_size),\n",
        "                                             nn.Tanh())\n",
        "        self._dynamics_state = nn.Sequential(nn.Linear(self.hx_size + action_space_n, 64),\n",
        "                                             nn.Tanh(),\n",
        "                                             nn.Linear(64, self.hx_size),\n",
        "                                             nn.Tanh())\n",
        "        self._dynamics_reward = nn.Sequential(nn.Linear(self.hx_size + action_space_n, 64),\n",
        "                                              nn.LeakyReLU(),\n",
        "                                              nn.Linear(64, 2*reward_support_size+1))\n",
        "        self._prediction_actor = nn.Sequential(nn.Linear(self.hx_size, 64),\n",
        "                                               nn.LeakyReLU(),\n",
        "                                               nn.Linear(64, action_space_n))\n",
        "        self._prediction_value = nn.Sequential(nn.Linear(self.hx_size, 64),\n",
        "                                               nn.LeakyReLU(),\n",
        "                                               nn.Linear(64, 2*value_support_size+1))\n",
        "        self.action_space_n = action_space_n\n",
        "\n",
        "        self._prediction_value[-1].weight.data.fill_(0)\n",
        "        self._prediction_value[-1].bias.data.fill_(0)\n",
        "        self._dynamics_reward[-1].weight.data.fill_(0)\n",
        "        self._dynamics_reward[-1].bias.data.fill_(0)\n",
        "\n",
        "    def p(self, state):\n",
        "        actor_logit = torch.softmax(self._prediction_actor(state),dim=1)\n",
        "        value = torch.softmax(self._prediction_value(state),dim=1)\n",
        "        return actor_logit, value\n",
        "\n",
        "    def h(self, obs_history):\n",
        "        return self._representation(obs_history)\n",
        "\n",
        "    def g(self, state, action):\n",
        "        x = torch.cat((state, action), dim=1)\n",
        "        next_state = self._dynamics_state(x)\n",
        "        reward = torch.softmax(self._dynamics_reward(x),dim=1)\n",
        "        return next_state, reward     \n",
        "\n",
        "    def initial_state(self, x):\n",
        "        hout = self.h(x)\n",
        "        prob,v= self.p(hout)\n",
        "        return hout,prob,v\n",
        "    def next_state(self,hin,a):\n",
        "        hout,r = self.g(hin,a)\n",
        "        prob,v= self.p(hout)\n",
        "        return hout,r,prob,v\n",
        "    def inference_initial_state(self, x):\n",
        "        with torch.no_grad():\n",
        "          hout = self.h(x)\n",
        "          prob,v=self.p(hout)\n",
        "\n",
        "          return hout,prob,v\n",
        "    def inference_next_state(self,hin,a):\n",
        "        with torch.no_grad():\n",
        "          hout,r = self.g(hin,a)\n",
        "          prob,v=self.p(hout)\n",
        "          return hout,r,prob,v     \n",
        "\n",
        "\n",
        "print(\"done\")                                      "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkKMTKa6wYtR"
      },
      "source": [
        "\n",
        "#MTCS    MUzero modified for intermeditate rewards settings and using predicted rewards\n",
        "#accepts policy as a list\n",
        "import torch\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "def dynamics(net,state,action):\n",
        "    #print(state,action) \n",
        "    next_state,reward,prob,value = net.inference_next_state(state,torch.tensor([action]).float())\n",
        "    reward = catts(reward.numpy().ravel())\n",
        "    value = catts(value.numpy().ravel())\n",
        "    prob = prob.tolist()[0]\n",
        "    #print(\"dynamics\",prob)\n",
        "    return next_state,reward,prob,value\n",
        "\n",
        "\n",
        "class MinMaxStats:\n",
        "    \"\"\"A class that holds the min-max values of the tree.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.MAXIMUM_FLOAT_VALUE = float('inf')       \n",
        "        self.maximum =  -self.MAXIMUM_FLOAT_VALUE\n",
        "        self.minimum =  self.MAXIMUM_FLOAT_VALUE\n",
        "\n",
        "    def update(self, value: float):\n",
        "        if value is None:\n",
        "            raise ValueError\n",
        "\n",
        "        self.maximum = max(self.maximum, value)\n",
        "        self.minimum = min(self.minimum, value)\n",
        "\n",
        "    def normalize(self, value: float) -> float:\n",
        "        # If the value is unknow, by default we set it to the minimum possible value\n",
        "        if value is None:\n",
        "            return 0.0\n",
        "\n",
        "        if self.maximum > self.minimum:\n",
        "            # We normalize only when we have set the maximum and minimum values.\n",
        "            return (value - self.minimum) / (self.maximum - self.minimum)\n",
        "        return value\n",
        "\n",
        "\n",
        "class Node:\n",
        "    \"\"\"A class that represent nodes inside the MCTS tree\"\"\"\n",
        "\n",
        "    def __init__(self, prior: float):\n",
        "        self.visit_count = 0\n",
        "        self.to_play = -1\n",
        "        self.prior = prior\n",
        "        self.value_sum = 0\n",
        "        self.children = {}\n",
        "        self.hidden_state = None\n",
        "        self.reward = 0\n",
        "\n",
        "    def expanded(self):\n",
        "        return len(self.children) > 0\n",
        "\n",
        "    def value(self):\n",
        "        if self.visit_count == 0:\n",
        "            return None\n",
        "        return self.value_sum / self.visit_count\n",
        "\n",
        "\n",
        "def softmax_sample(visit_counts, actions, t):\n",
        "    counts_exp = np.exp(visit_counts) * (1 / t)\n",
        "    probs = counts_exp / np.sum(counts_exp, axis=0)\n",
        "    action_idx = np.random.choice(len(actions), p=probs)\n",
        "    return actions[action_idx]\n",
        "\n",
        "\n",
        "\"\"\"MCTS module: where MuZero thinks inside the tree.\"\"\"\n",
        "\n",
        "\n",
        "def add_exploration_noise( node):\n",
        "    \"\"\"\n",
        "    At the start of each search, we add dirichlet noise to the prior of the root\n",
        "    to encourage the search to explore new actions.\n",
        "    \"\"\"\n",
        "    actions = list(node.children.keys())\n",
        "    noise = np.random.dirichlet([0.25] * len(actions)) # config.root_dirichlet_alpha\n",
        "    frac = 0.25#config.root_exploration_fraction\n",
        "    for a, n in zip(actions, noise):\n",
        "        node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac\n",
        "\n",
        "\n",
        "\n",
        "def ucb_score(parent, child,min_max_stats):\n",
        "    \"\"\"\n",
        "    The score for a node is based on its value, plus an exploration bonus based on\n",
        "    the prior.\n",
        "\n",
        "    \"\"\"\n",
        "    pb_c_base = 19652\n",
        "    pb_c_init = 1.25\n",
        "    pb_c = math.log((parent.visit_count + pb_c_base + 1) / pb_c_base) + pb_c_init\n",
        "    pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
        "\n",
        "    prior_score = pb_c * child.prior\n",
        "    value_score = min_max_stats.normalize(child.value())\n",
        "    return  value_score + prior_score \n",
        "\n",
        "def select_child(node, min_max_stats):\n",
        "    \"\"\"\n",
        "    Select the child with the highest UCB score.\n",
        "    \"\"\"\n",
        "    # When the parent visit count is zero, all ucb scores are zeros, therefore we return a random child\n",
        "    if node.visit_count == 0:\n",
        "        return random.sample(node.children.items(), 1)[0]\n",
        "\n",
        "    _, action, child = max(\n",
        "        (ucb_score(node, child, min_max_stats), action,\n",
        "         child) for action, child in node.children.items())\n",
        "    return action, child\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def expand_node(node, to_play, actions_space,hidden_state,reward,policy):\n",
        "    \"\"\"\n",
        "    We expand a node using the value, reward and policy prediction obtained from\n",
        "    the neural networks.\n",
        "    \"\"\"\n",
        "    node.to_play = to_play\n",
        "    node.hidden_state = hidden_state\n",
        "    node.reward = reward\n",
        "    policy = {a:policy[a] for a in actions_space}\n",
        "    policy_sum = sum(policy.values())\n",
        "    for action, p in policy.items():\n",
        "        node.children[action] = Node(p / policy_sum) # not needed since mine are already softmax but its fine \n",
        "\n",
        "\n",
        "def backpropagate(search_path, value,to_play,discount, min_max_stats):\n",
        "    \"\"\"\n",
        "    At the end of a simulation, we propagate the evaluation all the way up the\n",
        "    tree to the root.\n",
        "    \"\"\"\n",
        "    for node in search_path[::-1]: #[::-1] means reversed\n",
        "        node.value_sum += value \n",
        "        node.visit_count += 1\n",
        "        min_max_stats.update(node.value())\n",
        "\n",
        "        value = node.reward + discount * value\n",
        "\n",
        "\n",
        "def select_action(node, mode ='softmax'):\n",
        "    \"\"\"\n",
        "    After running simulations inside in MCTS, we select an action based on the root's children visit counts.\n",
        "    During training we use a softmax sample for exploration.\n",
        "    During evaluation we select the most visited child.\n",
        "    \"\"\"\n",
        "    visit_counts = [child.visit_count for child in node.children.values()]\n",
        "    actions = [action for action in node.children.keys()]\n",
        "    action = None\n",
        "    if mode == 'softmax':\n",
        "        t = 1.0\n",
        "        action = softmax_sample(visit_counts, actions, t)\n",
        "    elif mode == 'max':\n",
        "        action, _ = max(node.children.items(), key=lambda item: item[1].visit_count)\n",
        "    counts_exp = np.exp(visit_counts)\n",
        "    probs = counts_exp / np.sum(counts_exp, axis=0)    \n",
        "    #return action ,probs,node.value()\n",
        "    return action ,np.array(visit_counts)/sum(visit_counts),node.value()\n",
        "\n",
        "def run_mcts(net, state,prob,root_value,num_simulations,discount = 0.9):\n",
        "    \"\"\"\n",
        "    Core Monte Carlo Tree Search algorithm.\n",
        "    To decide on an action, we run N simulations, always starting at the root of\n",
        "    the search tree and traversing the tree according to the UCB formula until we\n",
        "    reach a leaf node.\n",
        "    \"\"\"\n",
        "    prob, root_value = prob.tolist()[0] ,catts(root_value.numpy().ravel())\n",
        "    to_play = True\n",
        "    action_space=[ i for i in range(len(prob))]#history.action_space()\n",
        "    #print(\"action space\",action_space)\n",
        "    root = Node(0)\n",
        "    expand_node(root, to_play,action_space,state,0.0,prob)#node, to_play, actions_space ,hidden_state,reward,policy\n",
        "    add_exploration_noise( root)\n",
        "\n",
        "\n",
        "    min_max_stats = MinMaxStats()\n",
        "\n",
        "    for _ in range(num_simulations): \n",
        "        node = root\n",
        "        search_path = [node]\n",
        "\n",
        "        while node.expanded():\n",
        "            action, node = select_child( node, min_max_stats)\n",
        "            search_path.append(node)\n",
        "\n",
        "        # Inside the search tree we use the dynamics function to obtain the next\n",
        "        # hidden state given an action and the previous hidden state.\n",
        "        parent = search_path[-2]\n",
        "        \n",
        "        #network_output = network.recurrent_inference(parent.hidden_state, action)\n",
        "        next_state,r,action_probs, value = dynamics(net,parent.hidden_state,onehot(action,len(action_space))) \n",
        "        expand_node(node, to_play, action_space,next_state,r,action_probs)#node, to_play, actions_space ,hidden_state,reward,policy\n",
        "\n",
        "        backpropagate(search_path, value, to_play, discount, min_max_stats)#search_path, value,,discount, min_max_stats\n",
        "    return root    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-NpN4lU12kW"
      },
      "source": [
        "import gym\n",
        "class ScalingObservationWrapper(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    Wrapper that apply a min-max scaling of observations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env, low=None, high=None):\n",
        "        super().__init__(env)\n",
        "        assert isinstance(env.observation_space, gym.spaces.Box)\n",
        "\n",
        "        low = np.array(self.observation_space.low if low is None else low)\n",
        "        high = np.array(self.observation_space.high if high is None else high)\n",
        "\n",
        "        self.mean = (high + low) / 2\n",
        "        self.max = high - self.mean\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return (observation - self.mean) / self.max"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waeVGfWytBB1"
      },
      "source": [
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "def onehot(a,n=2):\n",
        "  return np.eye(n)[a]\n",
        "def play_game(env,net,n_sim,discount,render):\n",
        "    trajectory=[]\n",
        "    state = env.reset() \n",
        "    done = False\n",
        "    while not done:\n",
        "        if render:\n",
        "          env.render()\n",
        "        h ,prob,pred_value= net.inference_initial_state(torch.tensor([state]).float()) \n",
        "        root  = run_mcts(net,h,prob,pred_value,num_simulations=n_sim,discount=discount)\n",
        "        action,action_prob,mcts_val = select_action(root) \n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        data = (state,onehot(action),action_prob,mcts_val,reward,pred_value)\n",
        "        trajectory.append(data)\n",
        "        state = next_state\n",
        "    print(\"DATA collection:played for \",len(trajectory),\" steps\")   \n",
        "    return trajectory    \n",
        "def eval_game(env,net,n_sim,render):\n",
        "    state = env.reset() \n",
        "    done = False\n",
        "    r = 0\n",
        "    while not done:\n",
        "        if render:\n",
        "          env.render()\n",
        "        h ,prob,value= net.inference_initial_state(torch.tensor([state]).float()) \n",
        "        root  = run_mcts(net,h,prob,value,num_simulations=n_sim,discount=discount)\n",
        "        action,action_prob,mcts_val = select_action(root,\"max\")\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        r+=reward\n",
        "        state = next_state\n",
        "    print(\"Eval:played for \",r ,\" rewards\")   \n",
        "    \n",
        "def sample_games(buffer,batch_size):\n",
        "    # Sample game from buffer either uniformly or according to some priority\n",
        "    #print(\"samplig from .\",len(buffer))\n",
        "    return random.choices(buffer, k=batch_size)\n",
        "\n",
        "def sample_position(trajectory):\n",
        "    # Sample position from game either uniformly or according to some priority.\n",
        "    return np.random.randint(0, len(trajectory))\n",
        "\n",
        "\n",
        "def sample_batch(action_space_size,buffer,discount,batch_size,num_unroll_steps, td_steps):\n",
        "    obs_batch, action_batch, reward_batch, value_batch, policy_batch,weights = [], [], [], [], [],[]\n",
        "    games = sample_games(buffer,batch_size)\n",
        "    for g in games:\n",
        "      state,action,action_prob,root_val,reward,pred_val = zip(*g)\n",
        "      #make priority for sampling\n",
        "\n",
        "      game_pos = sample_position(g)#state index sampled using priority\n",
        "      state,action,action_prob,root_val,reward =list(state),list(action),list(action_prob),list(root_val),list(reward)\n",
        "\n",
        "      _actions = action[game_pos:game_pos + num_unroll_steps]\n",
        "      # random action selection to complete num_unroll_steps\n",
        "      _actions += [onehot(np.random.randint(0, action_space_size))for _ in range(num_unroll_steps - len(_actions))]\n",
        "\n",
        "      obs_batch.append(state[game_pos])\n",
        "      action_batch.append(_actions)\n",
        "      value, reward, policy = make_target(child_visits=action_prob ,root_values=root_val,rewards=reward,state_index=game_pos,discount=discount, num_unroll_steps=num_unroll_steps, td_steps=td_steps)\n",
        "      reward_batch.append(reward)\n",
        "      value_batch.append(value)\n",
        "      policy_batch.append(policy)\n",
        "\n",
        "\n",
        "\n",
        "    obs_batch = torch.tensor(obs_batch).float()\n",
        "    action_batch = torch.tensor(action_batch).long()\n",
        "    reward_batch = torch.tensor(reward_batch).float()\n",
        "    value_batch = torch.tensor(value_batch).float()\n",
        "    policy_batch = torch.tensor(policy_batch).float()\n",
        "    return obs_batch, action_batch, reward_batch, value_batch, policy_batch\n",
        "\n",
        "\n",
        "def make_target(child_visits,root_values,rewards,state_index,discount=0.99, num_unroll_steps=5, td_steps=10):\n",
        "        # The value target is the discounted root value of the search tree N steps into the future, plus\n",
        "        # the discounted sum of all rewards until then.\n",
        "        target_values, target_rewards, target_policies = [], [], []\n",
        "        for current_index in range(state_index, state_index + num_unroll_steps + 1):\n",
        "            bootstrap_index = current_index + td_steps\n",
        "            if bootstrap_index < len(root_values):\n",
        "                value = root_values[bootstrap_index] * discount ** td_steps\n",
        "            else:\n",
        "                value = 0\n",
        "\n",
        "            for i, reward in enumerate(rewards[current_index:bootstrap_index]):\n",
        "                value += reward * discount ** i\n",
        "\n",
        "            if current_index < len(root_values):\n",
        "                target_values.append(stcat(value))\n",
        "                target_rewards.append(stcat(rewards[current_index]))\n",
        "                target_policies.append(child_visits[current_index])\n",
        "\n",
        "            else:\n",
        "                # States past the end of games are treated as absorbing states.\n",
        "                target_values.append(stcat(0))\n",
        "                target_rewards.append(stcat(0))\n",
        "                # Note: Target policy is  set to 0 so that no policy loss is calculated for them\n",
        "                #target_policies.append([0 for _ in range(len(child_visits[0]))])\n",
        "                target_policies.append(child_visits[0]*0.0)\n",
        "\n",
        "        return target_values, target_rewards, target_policies\n",
        "\n",
        "\n",
        "def scalar_reward_loss( prediction, target):\n",
        "        return -(torch.log(prediction) * target).sum(1)\n",
        "\n",
        "def scalar_value_loss( prediction, target):\n",
        "        return -(torch.log(prediction) * target).sum(1)\n",
        "def update_weights(model, action_space_size, optimizer, replay_buffer,discount,batch_size,num_unroll_steps, td_steps ):\n",
        "    batch = sample_batch(action_space_size,replay_buffer,discount,batch_size,num_unroll_steps, td_steps)\n",
        "    obs_batch, action_batch, target_reward, target_value, target_policy = batch\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    obs_batch = obs_batch.to(device)\n",
        "    action_batch = action_batch.to(device)#.unsqueeze(-1) # its not onehot yet \n",
        "    target_reward = target_reward.to(device)\n",
        "    target_value = target_value.to(device)\n",
        "    target_policy = target_policy.to(device)\n",
        "\n",
        "    # transform targets to categorical representation # its already done\n",
        "    # Reference:  Appendix F\n",
        "    #transformed_target_reward = config.scalar_transform(target_reward)\n",
        "    target_reward_phi =target_reward #config.reward_phi(transformed_target_reward)\n",
        "    #transformed_target_value = config.scalar_transform(target_value)\n",
        "    target_value_phi = target_value#config.value_phi(transformed_target_value)\n",
        "\n",
        "    hidden_state, policy_prob,value  = model.initial_state(obs_batch) # initial model_call ###################################### make changes\n",
        "    #h,init_pred_p,init_pred_v = net.initial_state(in_s)\n",
        "\n",
        "    value_loss = scalar_value_loss(value, target_value_phi[:, 0])\n",
        "    policy_loss = -(torch.log(policy_prob) * target_policy[:, 0]).sum(1)\n",
        "    reward_loss = torch.zeros(batch_size, device=device)\n",
        "\n",
        "    gradient_scale = 1 / num_unroll_steps\n",
        "    for step_i in range(num_unroll_steps):\n",
        "        hidden_state, reward,policy_prob,value  = model.next_state(hidden_state, action_batch[:, step_i]) ######################### make changes\n",
        "        #h,pred_reward,pred_policy,pred_value= net.next_state(h,act)\n",
        "        policy_loss += -(torch.log(policy_prob) * target_policy[:, step_i + 1]).sum(1)\n",
        "        value_loss += scalar_value_loss(value, target_value_phi[:, step_i + 1])\n",
        "        reward_loss += scalar_reward_loss(reward, target_reward_phi[:, step_i])\n",
        "        hidden_state.register_hook(lambda grad: grad * 0.5)\n",
        "\n",
        "    # optimize\n",
        "    value_loss_coeff = 1\n",
        "    loss = (policy_loss + value_loss_coeff * value_loss + reward_loss) # find value loss coefficiet = 1?\n",
        "    weights = 1\n",
        "    weighted_loss = (weights * loss).mean()#1?\n",
        "    weighted_loss.register_hook(lambda grad: grad * gradient_scale)\n",
        "    loss = loss.mean()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    weighted_loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 5)#5?\n",
        "    optimizer.step()\n",
        "\n",
        "def adjust_lr(optimizer, step_count):\n",
        "\n",
        "    lr_init=0.05\n",
        "    lr_decay_rate=0.01\n",
        "    lr_decay_steps=10000\n",
        "    lr = lr_init * lr_decay_rate ** (step_count / lr_decay_steps)\n",
        "    lr = max(lr, 0.001)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    return lr\n",
        "\n",
        "\n",
        "learning_rate = [0.05]   \n",
        "def net_train(net,  action_space_size, replay_buffer,discount,batch_size,num_unroll_steps, td_steps):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model =net\n",
        "    #MuZeroNet(input_size=4, action_space_n=2, reward_support_size=5, value_support_size=5).to(device) #training fresh net\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate[0], momentum=0.9,weight_decay=1e-4)\n",
        "    training_steps=2000#20000\n",
        "    # wait for replay buffer to be non-empty\n",
        "    while len(replay_buffer) == 0:\n",
        "        pass\n",
        "\n",
        "    for step_count in tqdm(range(training_steps)):\n",
        "        learning_rate[0] = adjust_lr( optimizer, step_count)\n",
        "        update_weights(model, action_space_size, optimizer, replay_buffer,discount,batch_size,num_unroll_steps, td_steps)\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZWra51wFVvb",
        "outputId": "05ca0dd8-f9a8-482b-8a2a-7e1da26d637f"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "render = False\n",
        "episodes_per_train=30\n",
        "episodes_per_eval = 10\n",
        "#buffer =[]\n",
        "buffer = deque(maxlen = episodes_per_train)\n",
        "training_steps=50\n",
        "\n",
        "n_sim= 50\n",
        "discount = 0.99\n",
        "batch_size = 512\n",
        "envs = ['CartPole-v1']\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "env=gym.make(envs[0])\n",
        "#env=env.unwrapped\n",
        "env = ScalingObservationWrapper(env, low=[-2.4, -2.0, -0.42, -3.5], high=[2.4, 2.0, 0.42, 3.5])\n",
        "\n",
        "s_dim =env.observation_space.shape[0]\n",
        "print(\"s_dim: \",s_dim)\n",
        "a_dim =env.action_space.n\n",
        "print(\"a_dim: \",a_dim)\n",
        "a_bound =1 #env.action_space.high[0]\n",
        "print(\"a_bound: \",a_bound)\n",
        "\n",
        "\n",
        "\n",
        "net = MuZeroNet(input_size=4, action_space_n=2, reward_support_size=5, value_support_size=5).to(device)\n",
        "\n",
        "for t in range(training_steps):\n",
        "  for _ in range(episodes_per_train):\n",
        "    buffer.append(play_game(env,net,n_sim,discount,render))\n",
        "  print(\"training from \",len(buffer),\" games\")  \n",
        "  net = net_train(net,  action_space_size=a_dim, replay_buffer=buffer,discount=discount,batch_size=batch_size,num_unroll_steps=5, td_steps=10)\n",
        "  for _ in range(episodes_per_eval):\n",
        "    eval_game(env,net,n_sim,render)\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "s_dim:  4\n",
            "a_dim:  2\n",
            "a_bound:  1\n",
            "DATA collection:played for  24  steps\n",
            "DATA collection:played for  22  steps\n",
            "DATA collection:played for  14  steps\n",
            "DATA collection:played for  25  steps\n",
            "DATA collection:played for  18  steps\n",
            "DATA collection:played for  18  steps\n",
            "DATA collection:played for  26  steps\n",
            "DATA collection:played for  15  steps\n",
            "DATA collection:played for  16  steps\n",
            "DATA collection:played for  14  steps\n",
            "DATA collection:played for  20  steps\n",
            "DATA collection:played for  21  steps\n",
            "DATA collection:played for  12  steps\n",
            "DATA collection:played for  38  steps\n",
            "DATA collection:played for  12  steps\n",
            "DATA collection:played for  37  steps\n",
            "DATA collection:played for  25  steps\n",
            "DATA collection:played for  31  steps\n",
            "DATA collection:played for  32  steps\n",
            "DATA collection:played for  9  steps\n",
            "DATA collection:played for  17  steps\n",
            "DATA collection:played for  16  steps\n",
            "DATA collection:played for  15  steps\n",
            "DATA collection:played for  23  steps\n",
            "DATA collection:played for  31  steps\n",
            "DATA collection:played for  17  steps\n",
            "DATA collection:played for  35  steps\n",
            "DATA collection:played for  13  steps\n",
            "DATA collection:played for  15  steps\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  30  steps\n",
            "training from  30  games\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [08:38<00:00,  3.86it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  236.0  rewards\n",
            "Eval:played for  284.0  rewards\n",
            "Eval:played for  215.0  rewards\n",
            "Eval:played for  439.0  rewards\n",
            "Eval:played for  250.0  rewards\n",
            "Eval:played for  276.0  rewards\n",
            "Eval:played for  328.0  rewards\n",
            "Eval:played for  275.0  rewards\n",
            "Eval:played for  319.0  rewards\n",
            "Eval:played for  275.0  rewards\n",
            "DATA collection:played for  388  steps\n",
            "DATA collection:played for  289  steps\n",
            "DATA collection:played for  381  steps\n",
            "DATA collection:played for  459  steps\n",
            "DATA collection:played for  198  steps\n",
            "DATA collection:played for  266  steps\n",
            "DATA collection:played for  334  steps\n",
            "DATA collection:played for  368  steps\n",
            "DATA collection:played for  226  steps\n",
            "DATA collection:played for  433  steps\n",
            "DATA collection:played for  268  steps\n",
            "DATA collection:played for  259  steps\n",
            "DATA collection:played for  334  steps\n",
            "DATA collection:played for  309  steps\n",
            "DATA collection:played for  267  steps\n",
            "DATA collection:played for  274  steps\n",
            "DATA collection:played for  216  steps\n",
            "DATA collection:played for  225  steps\n",
            "DATA collection:played for  255  steps\n",
            "DATA collection:played for  280  steps\n",
            "DATA collection:played for  316  steps\n",
            "DATA collection:played for  240  steps\n",
            "DATA collection:played for  269  steps\n",
            "DATA collection:played for  188  steps\n",
            "DATA collection:played for  263  steps\n",
            "DATA collection:played for  291  steps\n",
            "DATA collection:played for  288  steps\n",
            "DATA collection:played for  243  steps\n",
            "DATA collection:played for  239  steps\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  313  steps\n",
            "training from  30  games\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [10:15<00:00,  3.25it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  312.0  rewards\n",
            "Eval:played for  260.0  rewards\n",
            "Eval:played for  224.0  rewards\n",
            "Eval:played for  218.0  rewards\n",
            "Eval:played for  272.0  rewards\n",
            "Eval:played for  314.0  rewards\n",
            "Eval:played for  307.0  rewards\n",
            "Eval:played for  226.0  rewards\n",
            "Eval:played for  268.0  rewards\n",
            "Eval:played for  345.0  rewards\n",
            "DATA collection:played for  245  steps\n",
            "DATA collection:played for  221  steps\n",
            "DATA collection:played for  279  steps\n",
            "DATA collection:played for  297  steps\n",
            "DATA collection:played for  247  steps\n",
            "DATA collection:played for  268  steps\n",
            "DATA collection:played for  259  steps\n",
            "DATA collection:played for  273  steps\n",
            "DATA collection:played for  200  steps\n",
            "DATA collection:played for  196  steps\n",
            "DATA collection:played for  243  steps\n",
            "DATA collection:played for  357  steps\n",
            "DATA collection:played for  265  steps\n",
            "DATA collection:played for  201  steps\n",
            "DATA collection:played for  266  steps\n",
            "DATA collection:played for  207  steps\n",
            "DATA collection:played for  224  steps\n",
            "DATA collection:played for  234  steps\n",
            "DATA collection:played for  267  steps\n",
            "DATA collection:played for  205  steps\n",
            "DATA collection:played for  273  steps\n",
            "DATA collection:played for  269  steps\n",
            "DATA collection:played for  219  steps\n",
            "DATA collection:played for  271  steps\n",
            "DATA collection:played for  250  steps\n",
            "DATA collection:played for  270  steps\n",
            "DATA collection:played for  230  steps\n",
            "DATA collection:played for  221  steps\n",
            "DATA collection:played for  328  steps\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  322  steps\n",
            "training from  30  games\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [09:51<00:00,  3.38it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  294.0  rewards\n",
            "Eval:played for  260.0  rewards\n",
            "Eval:played for  240.0  rewards\n",
            "Eval:played for  258.0  rewards\n",
            "Eval:played for  266.0  rewards\n",
            "Eval:played for  295.0  rewards\n",
            "Eval:played for  254.0  rewards\n",
            "Eval:played for  328.0  rewards\n",
            "Eval:played for  297.0  rewards\n",
            "Eval:played for  239.0  rewards\n",
            "DATA collection:played for  296  steps\n",
            "DATA collection:played for  405  steps\n",
            "DATA collection:played for  293  steps\n",
            "DATA collection:played for  293  steps\n",
            "DATA collection:played for  221  steps\n",
            "DATA collection:played for  378  steps\n",
            "DATA collection:played for  278  steps\n",
            "DATA collection:played for  290  steps\n",
            "DATA collection:played for  368  steps\n",
            "DATA collection:played for  268  steps\n",
            "DATA collection:played for  251  steps\n",
            "DATA collection:played for  272  steps\n",
            "DATA collection:played for  323  steps\n",
            "DATA collection:played for  260  steps\n",
            "DATA collection:played for  264  steps\n",
            "DATA collection:played for  329  steps\n",
            "DATA collection:played for  263  steps\n",
            "DATA collection:played for  350  steps\n",
            "DATA collection:played for  291  steps\n",
            "DATA collection:played for  421  steps\n",
            "DATA collection:played for  305  steps\n",
            "DATA collection:played for  302  steps\n",
            "DATA collection:played for  234  steps\n",
            "DATA collection:played for  410  steps\n",
            "DATA collection:played for  283  steps\n",
            "DATA collection:played for  308  steps\n",
            "DATA collection:played for  233  steps\n",
            "DATA collection:played for  360  steps\n",
            "DATA collection:played for  247  steps\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  312  steps\n",
            "training from  30  games\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [10:10<00:00,  3.27it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  244.0  rewards\n",
            "Eval:played for  249.0  rewards\n",
            "Eval:played for  378.0  rewards\n",
            "Eval:played for  380.0  rewards\n",
            "Eval:played for  324.0  rewards\n",
            "Eval:played for  283.0  rewards\n",
            "Eval:played for  298.0  rewards\n",
            "Eval:played for  333.0  rewards\n",
            "Eval:played for  244.0  rewards\n",
            "Eval:played for  268.0  rewards\n",
            "DATA collection:played for  267  steps\n",
            "DATA collection:played for  310  steps\n",
            "DATA collection:played for  270  steps\n",
            "DATA collection:played for  294  steps\n",
            "DATA collection:played for  316  steps\n",
            "DATA collection:played for  272  steps\n",
            "DATA collection:played for  284  steps\n",
            "DATA collection:played for  269  steps\n",
            "DATA collection:played for  265  steps\n",
            "DATA collection:played for  268  steps\n",
            "DATA collection:played for  261  steps\n",
            "DATA collection:played for  400  steps\n",
            "DATA collection:played for  357  steps\n",
            "DATA collection:played for  257  steps\n",
            "DATA collection:played for  218  steps\n",
            "DATA collection:played for  327  steps\n",
            "DATA collection:played for  339  steps\n",
            "DATA collection:played for  258  steps\n",
            "DATA collection:played for  266  steps\n",
            "DATA collection:played for  286  steps\n",
            "DATA collection:played for  364  steps\n",
            "DATA collection:played for  325  steps\n",
            "DATA collection:played for  295  steps\n",
            "DATA collection:played for  370  steps\n",
            "DATA collection:played for  266  steps\n",
            "DATA collection:played for  297  steps\n",
            "DATA collection:played for  249  steps\n",
            "DATA collection:played for  270  steps\n",
            "DATA collection:played for  355  steps\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  266  steps\n",
            "training from  30  games\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [10:22<00:00,  3.21it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  318.0  rewards\n",
            "Eval:played for  339.0  rewards\n",
            "Eval:played for  391.0  rewards\n",
            "Eval:played for  360.0  rewards\n",
            "Eval:played for  280.0  rewards\n",
            "Eval:played for  306.0  rewards\n",
            "Eval:played for  296.0  rewards\n",
            "Eval:played for  306.0  rewards\n",
            "Eval:played for  279.0  rewards\n",
            "Eval:played for  295.0  rewards\n",
            "DATA collection:played for  275  steps\n",
            "DATA collection:played for  305  steps\n",
            "DATA collection:played for  305  steps\n",
            "DATA collection:played for  432  steps\n",
            "DATA collection:played for  270  steps\n",
            "DATA collection:played for  373  steps\n",
            "DATA collection:played for  297  steps\n",
            "DATA collection:played for  314  steps\n",
            "DATA collection:played for  350  steps\n",
            "DATA collection:played for  326  steps\n",
            "DATA collection:played for  317  steps\n",
            "DATA collection:played for  340  steps\n",
            "DATA collection:played for  285  steps\n",
            "DATA collection:played for  319  steps\n",
            "DATA collection:played for  292  steps\n",
            "DATA collection:played for  283  steps\n",
            "DATA collection:played for  289  steps\n",
            "DATA collection:played for  401  steps\n",
            "DATA collection:played for  357  steps\n",
            "DATA collection:played for  290  steps\n",
            "DATA collection:played for  246  steps\n",
            "DATA collection:played for  348  steps\n",
            "DATA collection:played for  326  steps\n",
            "DATA collection:played for  296  steps\n",
            "DATA collection:played for  285  steps\n",
            "DATA collection:played for  296  steps\n",
            "DATA collection:played for  319  steps\n",
            "DATA collection:played for  333  steps\n",
            "DATA collection:played for  335  steps\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  279  steps\n",
            "training from  30  games\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [11:04<00:00,  3.01it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  310.0  rewards\n",
            "Eval:played for  549.0  rewards\n",
            "Eval:played for  251.0  rewards\n",
            "Eval:played for  521.0  rewards\n",
            "Eval:played for  402.0  rewards\n",
            "Eval:played for  428.0  rewards\n",
            "Eval:played for  335.0  rewards\n",
            "Eval:played for  1075.0  rewards\n",
            "Eval:played for  412.0  rewards\n",
            "Eval:played for  312.0  rewards\n",
            "DATA collection:played for  372  steps\n",
            "DATA collection:played for  1007  steps\n",
            "DATA collection:played for  312  steps\n",
            "DATA collection:played for  540  steps\n",
            "DATA collection:played for  405  steps\n",
            "DATA collection:played for  485  steps\n",
            "DATA collection:played for  243  steps\n",
            "DATA collection:played for  427  steps\n",
            "DATA collection:played for  234  steps\n",
            "DATA collection:played for  581  steps\n",
            "DATA collection:played for  351  steps\n",
            "DATA collection:played for  363  steps\n",
            "DATA collection:played for  398  steps\n",
            "DATA collection:played for  496  steps\n",
            "DATA collection:played for  363  steps\n",
            "DATA collection:played for  419  steps\n",
            "DATA collection:played for  230  steps\n",
            "DATA collection:played for  266  steps\n",
            "DATA collection:played for  317  steps\n",
            "DATA collection:played for  732  steps\n",
            "DATA collection:played for  286  steps\n",
            "DATA collection:played for  417  steps\n",
            "DATA collection:played for  1144  steps\n",
            "DATA collection:played for  439  steps\n",
            "DATA collection:played for  378  steps\n",
            "DATA collection:played for  244  steps\n",
            "DATA collection:played for  379  steps\n",
            "DATA collection:played for  494  steps\n",
            "DATA collection:played for  647  steps\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DATA collection:played for  501  steps\n",
            "training from  30  games\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [11:09<00:00,  2.99it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval:played for  1299.0  rewards\n",
            "Eval:played for  959.0  rewards\n",
            "Eval:played for  1349.0  rewards\n",
            "Eval:played for  1003.0  rewards\n",
            "Eval:played for  1528.0  rewards\n",
            "Eval:played for  298.0  rewards\n",
            "Eval:played for  2508.0  rewards\n",
            "Eval:played for  1488.0  rewards\n",
            "Eval:played for  838.0  rewards\n",
            "Eval:played for  1555.0  rewards\n",
            "DATA collection:played for  2264  steps\n",
            "DATA collection:played for  1057  steps\n",
            "DATA collection:played for  768  steps\n",
            "DATA collection:played for  7775  steps\n",
            "DATA collection:played for  2200  steps\n",
            "DATA collection:played for  1404  steps\n",
            "DATA collection:played for  975  steps\n",
            "DATA collection:played for  467  steps\n",
            "DATA collection:played for  735  steps\n",
            "DATA collection:played for  884  steps\n",
            "DATA collection:played for  2098  steps\n",
            "DATA collection:played for  704  steps\n",
            "DATA collection:played for  681  steps\n",
            "DATA collection:played for  1600  steps\n",
            "DATA collection:played for  743  steps\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}