{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SAC2_torch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOsFL7Di33G4NBM7eMslDSn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rootAkash/reinforcement_learning/blob/master/Soft%20AC/SAC2_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yFxID47gl5L"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plLsCJOlQYC3"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "class Policy(nn.Module):\n",
        "\n",
        "    def __init__(self,observation_size,action_size,hidden_units):\n",
        "        super().__init__()\n",
        "        self.observation_size=observation_size\n",
        "        self.hidden_units=hidden_units\n",
        "        self.action_size = action_size\n",
        "        self.h1 = nn.Linear(self.observation_size, self.hidden_units)  \n",
        "        self.h2 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        self.h3 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        self.h4 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        self.h5 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        self.h6 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        self.mu = nn.Linear(self.hidden_units,self.action_size)\n",
        "        self.sigma = nn.Linear(self.hidden_units,self.action_size)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.h1(x))\n",
        "        x = F.relu(self.h2(x))\n",
        "        x = F.relu(self.h3(x))\n",
        "        x = F.relu(self.h4(x))\n",
        "        x = F.relu(self.h5(x))\n",
        "        x = F.relu(self.h6(x))\n",
        "        mus = torch.tanh(self.mu(x))\n",
        "        sigs= F.softplus(self.sigma(x))\n",
        "        sigs= torch.clamp(sigs, min=0.001, max=100)#1e-22 , 1e+02\n",
        "        return mus , sigs\n",
        "    def predict(self, x):\n",
        "        with torch.no_grad():\n",
        "          output=self.forward(x)\n",
        "        return output  \n",
        "\n",
        "class Value(nn.Module):\n",
        "    def __init__(self,observation_size,hidden_units):\n",
        "        super().__init__()\n",
        "        self.observation_size=observation_size\n",
        "        self.hidden_units=hidden_units\n",
        "        self.h1 = nn.Linear(self.observation_size, self.hidden_units)  \n",
        "        self.h2 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        self.h3 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        self.h4 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        self.h5 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        self.h6 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        self.v = nn.Linear(self.hidden_units,1)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.h1(x))\n",
        "        x = F.relu(self.h2(x))\n",
        "        x = F.relu(self.h3(x))\n",
        "        x = F.relu(self.h4(x))\n",
        "        x = F.relu(self.h5(x))\n",
        "        x = F.relu(self.h6(x))\n",
        "        vout = self.v(x)\n",
        "        return vout\n",
        "    def predict(self, x):\n",
        "        with torch.no_grad():\n",
        "          output=self.forward(x)\n",
        "        return output    \n",
        "class Q_net(nn.Module):\n",
        "\n",
        "    def __init__(self,observation_size,action_size,hidden_units):\n",
        "        super().__init__()\n",
        "        self.observation_size=observation_size\n",
        "        self.hidden_units=hidden_units\n",
        "        self.action_size = action_size\n",
        "        self.h1 = nn.Linear(self.observation_size, self.hidden_units) \n",
        "        self.a1 = nn.Linear(self.action_size, self.hidden_units) \n",
        "        self.h2 = nn.Linear(self.hidden_units*2,self.hidden_units)\n",
        "        self.h3 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        self.h4 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        self.h5 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        self.h6 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        self.q = nn.Linear(self.hidden_units,1)\n",
        "    def forward(self, x,a):\n",
        "        x = F.relu(self.h1(x))\n",
        "        a = F.relu(self.a1(a))\n",
        "        x = torch.cat([x,a], dim=1)\n",
        "        x = F.relu(self.h2(x))\n",
        "        x = F.relu(self.h3(x))\n",
        "        x = F.relu(self.h4(x))\n",
        "        x = F.relu(self.h5(x))\n",
        "        x = F.relu(self.h6(x))\n",
        "        qout = self.q(x)\n",
        "        return qout\n",
        "    def predict(self, x,a):\n",
        "        with torch.no_grad():\n",
        "          output=self.forward(x,a)\n",
        "        return output\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3bHlG__7Dft"
      },
      "source": [
        "def remember(s,a,r,ns,d):\n",
        "  #s=s.ravel()\n",
        "  #ns=ns.ravel()\n",
        "  memory.append([s,a,np.array([r]),ns,np.array([d])])\n",
        "def sample_games(buffer,batch_size):\n",
        "  # Sample game from buffer either uniformly or according to some priority\n",
        "  #print(\"samplig from .\",len(buffer))\n",
        "  return list(np.random.choice(len(buffer),batch_size))\n",
        "def soft_update(target, source, tau):\n",
        "  for target_param, param in zip(target.parameters(), source.parameters()):\n",
        "    target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
        "  return target\n",
        "def get_log_pdf_multi(x,mean,std):\n",
        "  k= x.shape[1] #action dim\n",
        "  pi = 3.1415926\n",
        "  cov = std**2\n",
        "  det = torch.prod(cov,dim=1,keepdim=True)\n",
        "  norm_const = 1.0/ ( np.power(2*pi,k/2) * torch.pow(det,0.5) )\n",
        "  prod  = (1/cov)*torch.square(x - mean)\n",
        "  prod2 =torch.sum(prod,dim=1,keepdim=True) \n",
        "  pdf = norm_const * torch.exp( -0.5 *prod2)\n",
        "  final_log_pdf = torch.log(pdf+1e-07)\n",
        "  return final_log_pdf\n",
        "\n",
        "def get_entropy_multi(x, mean, std,Act):\n",
        "  #log pdf (squashed guassian) = log pdfguassian(mupolicy,sigma_policy) - sum of  log(1-A**2) ; where each A is component of tanh squahed action vector \n",
        "  log_pdf_final = get_log_pdf_multi(x,mean,std) - torch.sum(torch.log(1- torch.square(Act) +1e-07),dim=1,keepdim=True)\n",
        "  return -log_pdf_final\n",
        "\n",
        "\n",
        "\n",
        "def replay_and_train(policy,value,t_value,q_1,q_2,popt,vopt,qopt,size=128):\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  gamma=0.99\n",
        "  mse = nn.MSELoss()\n",
        "  alpha = 1.0\n",
        "  sample_size=size\n",
        "  if len(memory) < sample_size:\n",
        "    return\n",
        "  samples=random.sample(memory,sample_size)\n",
        "  s,a,r,ns,d=zip(*samples)\n",
        "  #s,a,r,ns,d = list(s),list(a),list(r),list(ns),list(d)\n",
        "  #print(s,a,r,ns,d)\n",
        "  s= torch.tensor(s).float().to(device)\n",
        "  a= torch.tensor(a).float().to(device)\n",
        "  r= torch.tensor(r).float().to(device)\n",
        "  ns= torch.tensor(ns).float().to(device)\n",
        "  d= torch.tensor(d).float().to(device)\n",
        "\n",
        "  yq = r + gamma*(1-d)*t_value.predict(ns)\n",
        "  new_mu,new_sig = policy(s)\n",
        "  E =np.random.multivariate_normal(np.zeros_like(new_mu[0].detach().numpy()),np.diag(np.ones_like(new_mu[0].detach().numpy())),size) \n",
        "  E = torch.tensor(E).float().to(device)\n",
        "  Act_guassian = new_mu + E*new_sig # guassian action using reparametrisation\n",
        "  new_Action  =torch.tanh(Act_guassian)  #final squashed guassian action\n",
        "  entropy = get_entropy_multi(Act_guassian,new_mu,new_sig,new_Action)\n",
        "  yv = torch.minimum(q_1.predict(s,new_Action),q_2.predict(s,new_Action)) + alpha*entropy.detach() # value target should be bootstrapped to current policy\n",
        "  #training nets \n",
        "  #train q nets\n",
        "  qloss = mse(q_1(s,a),yq) + mse(q_2(s,a),yq)\n",
        "  qopt.zero_grad()                                                                                                          #    \n",
        "  qloss.backward()                                                                                                         #\n",
        "  qopt.step() \n",
        "  #train v net\n",
        "  vloss = mse(value(s),yv)\n",
        "  vopt.zero_grad()                                                                                                          #    \n",
        "  vloss.backward()                                                                                                         #\n",
        "  vopt.step() \n",
        "  #train policy \n",
        "  policy_objective = q_1(s,new_Action)#state from buffer action from recent policy and not from buffer to train to maximise q\n",
        "  final_policy_objective = policy_objective + alpha*entropy # maximise this\n",
        "  final_policy_loss = - torch.mean(final_policy_objective)  #therefore minimise this\n",
        "  popt.zero_grad()                                                                                                          #    \n",
        "  final_policy_loss.backward()                                                                                                         #\n",
        "  popt.step() \n",
        "  \n",
        "  #train t_value\n",
        "  soft_update(target=t_value, source=value, tau=0.01)\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QI4onnTUyPAn",
        "outputId": "b37712fc-684e-4b15-c7a0-5036d015b005"
      },
      "source": [
        "import gym\n",
        "\n",
        "memory=deque(maxlen=5000000)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "#env,networks ad optimisers\n",
        "env=gym.make('Pendulum-v0')\n",
        "#env=gym.make('MountainCarContinuous-v0')\n",
        "\n",
        "env=env.unwrapped\n",
        "\n",
        "s_dim = env.observation_space.shape[0]\n",
        "print(s_dim)\n",
        "a_dim = env.action_space.shape[0]\n",
        "print(a_dim)\n",
        "a_bound = env.action_space.high[0]\n",
        "print(a_bound)\n",
        "\n",
        "policy = Policy(s_dim,a_dim,100).to(device)\n",
        "value = Value(s_dim,100).to(device)\n",
        "t_value = Value(s_dim,100).to(device)\n",
        "soft_update(target=t_value, source=value, tau=1)\n",
        "q_1 = Q_net(s_dim,a_dim,100).to(device)\n",
        "q_2 = Q_net(s_dim,a_dim,100).to(device)\n",
        "qopt = optim.Adam(list(q_1.parameters()) + list(q_2.parameters()),lr=0.001)\n",
        "popt = optim.Adam(policy.parameters(),lr=0.001)\n",
        "vopt = optim.Adam(value.parameters(),lr=0.001)        \n",
        "max_steps=5000\n",
        "episodes = 500\n",
        "train_iter = 5000  # 1000, batch of 8/16 works best for now\n",
        "for e in range(episodes):\n",
        "  done = False\n",
        "  s = env.reset()\n",
        "  rew = 0 \n",
        "  stp=0\n",
        "  while not done:\n",
        "    new_mu,new_sig = policy.predict(torch.tensor([s]).float().to(device))\n",
        "    new_mu,new_sig=new_mu.cpu().numpy(),new_sig.cpu().numpy()\n",
        "    #print(new_mu,new_sig,new_mu[0],new_sig[0])\n",
        "    E =np.random.multivariate_normal(np.zeros_like(new_mu[0]),np.diag(np.ones_like(new_mu[0])))#np.random.normal(mu=0,std_dev=1) diag cov of 1 is same as std dev of 1\n",
        "    Act_guassian = new_mu[0] + new_sig[0]*E # guassian action using reparametrisation\n",
        "    act  =np.tanh(Act_guassian)\n",
        "    s_,r,done,_=env.step(act*a_bound)\n",
        "    if stp>max_steps:\n",
        "      done = True  \n",
        "    remember(s,act,r,s_,done)\n",
        "    s=s_\n",
        "    rew+=r\n",
        "    stp+=1\n",
        "  print(e,rew)\n",
        "  if e>0:\n",
        "    print(\"training\")\n",
        "    for i in  tqdm(range(train_iter)):\n",
        "      replay_and_train(policy,value,t_value,q_1,q_2,popt,vopt,qopt,size=8)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "1\n",
            "2.0\n",
            "0 -37123.447086588305\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 5/5000 [00:00<01:44, 47.65it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 -27819.903587189565\n",
            "training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [01:44<00:00, 47.93it/s]\n",
            "  0%|          | 5/5000 [00:00<02:06, 39.41it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2 -38596.47850594417\n",
            "training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [01:46<00:00, 46.85it/s]\n",
            "  0%|          | 5/5000 [00:00<01:49, 45.70it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "3 -31738.923493831062\n",
            "training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [01:49<00:00, 45.57it/s]\n",
            "  0%|          | 5/5000 [00:00<01:52, 44.23it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4 -17144.314777526182\n",
            "training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [01:49<00:00, 45.49it/s]\n",
            "  0%|          | 5/5000 [00:00<01:56, 42.85it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "5 -25140.598987865826\n",
            "training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [01:49<00:00, 45.66it/s]\n",
            "  0%|          | 5/5000 [00:00<01:58, 42.21it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "6 -1055.7201506140902\n",
            "training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [01:49<00:00, 45.61it/s]\n",
            "  0%|          | 6/5000 [00:00<01:29, 55.97it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "7 -158.49423783744794\n",
            "training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [01:48<00:00, 46.07it/s]\n",
            "  0%|          | 5/5000 [00:00<01:53, 44.06it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "8 -166.3775650373026\n",
            "training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 4994/5000 [01:49<00:00, 48.61it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5rB3haEtjPkU",
        "outputId": "10b7bfce-94fe-4cd6-c6c8-e468b1c86818"
      },
      "source": [
        "import gym\n",
        "\n",
        "memory=deque(maxlen=5000000)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "#env,networks ad optimisers\n",
        "#env=gym.make('Pendulum-v0')\n",
        "env=gym.make('MountainCarContinuous-v0')\n",
        "\n",
        "env=env.unwrapped\n",
        "\n",
        "s_dim = env.observation_space.shape[0]\n",
        "print(s_dim)\n",
        "a_dim = env.action_space.shape[0]\n",
        "print(a_dim)\n",
        "a_bound = env.action_space.high[0]\n",
        "print(a_bound)\n",
        "\n",
        "policy = Policy(s_dim,a_dim,100).to(device)\n",
        "value = Value(s_dim,100).to(device)\n",
        "t_value = Value(s_dim,100).to(device)\n",
        "soft_update(target=t_value, source=value, tau=1)\n",
        "q_1 = Q_net(s_dim,a_dim,100).to(device)\n",
        "q_2 = Q_net(s_dim,a_dim,100).to(device)\n",
        "qopt = optim.Adam(list(q_1.parameters()) + list(q_2.parameters()),lr=0.001,betas=(0.5, 0.999))\n",
        "popt = optim.Adam(policy.parameters(),lr=0.001,betas=(0.5, 0.999))\n",
        "vopt = optim.Adam(value.parameters(),lr=0.001,betas=(0.5, 0.999))        \n",
        "max_steps=5000\n",
        "episodes = 5000\n",
        "steps = 3500 \n",
        "ctr = 0\n",
        "render =False\n",
        "train_iter = 1000  # 1000, batch of 8/16 works best for now\n",
        "\n",
        "for ep in range(episodes):\n",
        "\ts = env.reset()\n",
        "\tdone=False\n",
        "\trews=0\n",
        "\tif ep>1500:\n",
        "\t\trender=1\n",
        "\tfor step in range(steps):\n",
        "\t\tif done:\n",
        "\t\t\ts = env.reset()\n",
        "\t\tif render:\n",
        "\t\t\tenv.render()\t\n",
        "   \n",
        "\t\tnew_mu,new_sig = policy.predict(torch.tensor([s]).float().to(device))\n",
        "\t\t\n",
        "\t\tmu,sig=new_mu.cpu().numpy(),new_sig.cpu().numpy()\n",
        "\t\tE =np.random.multivariate_normal(np.zeros_like(mu[0]),np.diag(np.ones_like(sig[0])))#np.random.normal(0,1)\n",
        "\n",
        "\t\tAction  =np.tanh(mu[0] + sig[0]*E)\n",
        "\t\tif ep < 10 :\n",
        "\t\t\t# for some additional exploration not necesserily needed\n",
        "\t\t\tif E > 0.5:\n",
        "\t\t\t\tAction = np.clip(a_bound*E,-a_bound,a_bound)\t\t\n",
        "\t\ts_,r,done,_=env.step(Action*a_bound)\n",
        "\t\tif done :\n",
        "\t\t\tr =r+10000\t# to encourage reaching target more\n",
        "\t\t\tprint(\"reached\")\n",
        "\t\tremember(s,Action,r,s_,done)\n",
        "\t\trews+=r\t\n",
        "\t\tctr+=1\n",
        "\t\ts=s_\n",
        "\tprint(\"episode: \"+str(ep)+ \" rews: \"+str(rews))\t\t\n",
        "\tprint(\"training\")\n",
        "\tfor i in  range(train_iter):\n",
        "\t\treplay_and_train(policy,value,t_value,q_1,q_2,popt,vopt,qopt,size=16)\n",
        "\t\tif i % (train_iter//10)==0:\n",
        "\t\t\tprint('.',end='')#tqdm sometimes has issues in colab so did this \n",
        "\tprint('|')\t"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "1\n",
            "1.0\n",
            "reached\n",
            "episode: 0 rews: 9960.348283908465\n",
            "training\n",
            "..........|\n",
            "episode: 1 rews: -149.48956947605896\n",
            "training\n",
            "..........|\n",
            "episode: 2 rews: -150.34566090881796\n",
            "training\n",
            "..........|\n",
            "episode: 3 rews: -214.8940358008331\n",
            "training\n",
            "..........|\n",
            "episode: 4 rews: -162.43216157926918\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 5 rews: 9935.953813575446\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 6 rews: 9959.867563747235\n",
            "training\n",
            "..........|\n",
            "episode: 7 rews: -153.62652802219037\n",
            "training\n",
            "..........|\n",
            "episode: 8 rews: -199.9902604543084\n",
            "training\n",
            "..........|\n",
            "episode: 9 rews: -154.51613932437454\n",
            "training\n",
            "..........|\n",
            "episode: 10 rews: -193.8129400390111\n",
            "training\n",
            "..........|\n",
            "episode: 11 rews: -196.23978337629833\n",
            "training\n",
            "..........|\n",
            "episode: 12 rews: -188.7714696405506\n",
            "training\n",
            "..........|\n",
            "episode: 13 rews: -185.67991707265128\n",
            "training\n",
            "..........|\n",
            "episode: 14 rews: -165.0688008852172\n",
            "training\n",
            "..........|\n",
            "episode: 15 rews: -178.15057243338563\n",
            "training\n",
            "..........|\n",
            "episode: 16 rews: -167.71689517726006\n",
            "training\n",
            "..........|\n",
            "episode: 17 rews: -178.13104649588425\n",
            "training\n",
            "..........|\n",
            "episode: 18 rews: -174.81484943357538\n",
            "training\n",
            "..........|\n",
            "episode: 19 rews: -133.84149022174185\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 20 rews: 9929.742798217643\n",
            "training\n",
            "..........|\n",
            "episode: 21 rews: -155.4979680569285\n",
            "training\n",
            "..........|\n",
            "episode: 22 rews: -108.99475475343918\n",
            "training\n",
            "..........|\n",
            "episode: 23 rews: -121.49079957953457\n",
            "training\n",
            "..........|\n",
            "episode: 24 rews: -137.32519518266056\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 25 rews: 9930.243347958693\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 26 rews: 9904.707489465563\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 27 rews: 9907.0930431023\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 28 rews: 9907.67580470072\n",
            "training\n",
            "..........|\n",
            "episode: 29 rews: -191.60336890933155\n",
            "training\n",
            "..........|\n",
            "episode: 30 rews: -115.73510774011154\n",
            "training\n",
            "..........|\n",
            "episode: 31 rews: -141.3655003631493\n",
            "training\n",
            "..........|\n",
            "episode: 32 rews: -128.46489276266678\n",
            "training\n",
            "..........|\n",
            "episode: 33 rews: -123.42070866850392\n",
            "training\n",
            "..........|\n",
            "episode: 34 rews: -191.8768635517278\n",
            "training\n",
            "..........|\n",
            "episode: 35 rews: -193.02290467692856\n",
            "training\n",
            "..........|\n",
            "episode: 36 rews: -121.11468892467349\n",
            "training\n",
            "..........|\n",
            "episode: 37 rews: -111.0187319258716\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "episode: 38 rews: 20058.926024488886\n",
            "training\n",
            "..........|\n",
            "episode: 39 rews: -109.09994749790128\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 40 rews: 9990.551270151453\n",
            "training\n",
            "..........|\n",
            "episode: 41 rews: -109.98370207413276\n",
            "training\n",
            "..........|\n",
            "episode: 42 rews: -90.04365104269107\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 43 rews: 10006.761647538913\n",
            "training\n",
            "..........|\n",
            "episode: 44 rews: -86.36404434511962\n",
            "training\n",
            "..........|\n",
            "episode: 45 rews: -70.61730656583926\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 46 rews: 10019.105631528131\n",
            "training\n",
            "..........|\n",
            "episode: 47 rews: -84.34318380411017\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 48 rews: 10017.317526864881\n",
            "training\n",
            "..........|\n",
            "episode: 49 rews: -93.29734337657699\n",
            "training\n",
            "..........|\n",
            "episode: 50 rews: -103.06357621440921\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 51 rews: 9986.279995209166\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 52 rews: 9974.968622209144\n",
            "training\n",
            "..........|\n",
            "episode: 53 rews: -128.10451376699635\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 54 rews: 9981.031322416458\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 55 rews: 9966.52467614783\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 56 rews: 9972.474119105764\n",
            "training\n",
            "..........|\n",
            "episode: 57 rews: -130.8414730531171\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 58 rews: 9970.454525479183\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 59 rews: 30161.27764228012\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 60 rews: 9973.136046453548\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "episode: 61 rews: 20082.646232230345\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 62 rews: 9982.565507509107\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "episode: 63 rews: 20069.34723062779\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 64 rews: 60492.57493889446\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 65 rews: 30164.272430355155\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "episode: 66 rews: 20057.64926326763\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 67 rews: 110954.65465987388\n",
            "training\n",
            "...."
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2c99abe2e45e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0mreplay_and_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpopt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvopt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mqopt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#tqdm sometimes has issues in colab so did this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-4c9ebc530e8e>\u001b[0m in \u001b[0;36mreplay_and_train\u001b[0;34m(policy, value, t_value, q_1, q_2, popt, vopt, qopt, size)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;31m#train v net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m   \u001b[0mvloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m   \u001b[0mvopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                                                                                                          \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m   \u001b[0mvloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                                                                                                         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m   \u001b[0mvopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    215\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/profiler.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_callbacks_on_exit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_exit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_end_callbacks_on_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFuture\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mFuture\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2dIRKWRgt3b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}