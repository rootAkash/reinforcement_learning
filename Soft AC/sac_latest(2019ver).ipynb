{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sac_latest(2019ver).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN9QpsDqwzRwXod4+CmjR6s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rootAkash/reinforcement_learning/blob/master/Soft%20AC/sac_latest(2019ver).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHHNKndmNqg_"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "class Policy(nn.Module):\n",
        "\n",
        "    def __init__(self,observation_size,action_size,hidden_units):\n",
        "        super().__init__()\n",
        "        self.observation_size=observation_size\n",
        "        self.hidden_units=hidden_units\n",
        "        self.action_size = action_size\n",
        "        self.h1 = nn.Linear(self.observation_size, self.hidden_units)  \n",
        "        self.h2 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        self.h3 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        self.h4 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        self.h5 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        self.h6 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        self.mu = nn.Linear(self.hidden_units,self.action_size)\n",
        "        self.sigma = nn.Linear(self.hidden_units,self.action_size)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.h1(x))\n",
        "        x = F.relu(self.h2(x))\n",
        "        x = F.relu(self.h3(x))\n",
        "        x = F.relu(self.h4(x))\n",
        "        x = F.relu(self.h5(x))\n",
        "        x = F.relu(self.h6(x))\n",
        "        mus = torch.tanh(self.mu(x))\n",
        "        sigs= F.softplus(self.sigma(x))\n",
        "        sigs= torch.clamp(sigs, min=0.001, max=100)#1e-22 , 1e+02\n",
        "        return mus , sigs\n",
        "    def predict(self, x):\n",
        "        with torch.no_grad():\n",
        "          output=self.forward(x)\n",
        "        return output  \n",
        "\n",
        "class Q_net(nn.Module):\n",
        "\n",
        "    def __init__(self,observation_size,action_size,hidden_units):\n",
        "        super().__init__()\n",
        "        self.observation_size=observation_size\n",
        "        self.hidden_units=hidden_units\n",
        "        self.action_size = action_size\n",
        "        self.h1 = nn.Linear(self.observation_size, self.hidden_units) \n",
        "        self.a1 = nn.Linear(self.action_size, self.hidden_units) \n",
        "        self.h2 = nn.Linear(self.hidden_units*2,self.hidden_units)\n",
        "        self.h3 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        self.h4 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        self.h5 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        self.h6 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        self.q = nn.Linear(self.hidden_units,1)\n",
        "    def forward(self, x,a):\n",
        "        x = F.relu(self.h1(x))\n",
        "        a = F.relu(self.a1(a))\n",
        "        x = torch.cat([x,a], dim=1)\n",
        "        x = F.relu(self.h2(x))\n",
        "        x = F.relu(self.h3(x))\n",
        "        x = F.relu(self.h4(x))\n",
        "        x = F.relu(self.h5(x))\n",
        "        x = F.relu(self.h6(x))\n",
        "        qout = self.q(x)\n",
        "        return qout\n",
        "    def predict(self, x,a):\n",
        "        with torch.no_grad():\n",
        "          output=self.forward(x,a)\n",
        "        return output\n",
        "\n",
        "def remember(s,a,r,ns,d):\n",
        "  #s=s.ravel()\n",
        "  #ns=ns.ravel()\n",
        "  memory.append([s,a,np.array([r]),ns,np.array([d])])\n",
        "def sample_games(buffer,batch_size):\n",
        "  # Sample game from buffer either uniformly or according to some priority\n",
        "  #print(\"samplig from .\",len(buffer))\n",
        "  return list(np.random.choice(len(buffer),batch_size))\n",
        "def soft_update(target, source, tau):\n",
        "  for target_param, param in zip(target.parameters(), source.parameters()):\n",
        "    target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
        "  return target\n",
        "def get_log_pdf_multi(x,mean,std):\n",
        "  k= x.shape[1] #action dim\n",
        "  pi = 3.1415926\n",
        "  cov = std**2\n",
        "  det = torch.prod(cov,dim=1,keepdim=True)\n",
        "  norm_const = 1.0/ ( np.power(2*pi,k/2) * torch.pow(det,0.5) )\n",
        "  prod  = (1/cov)*torch.square(x - mean)\n",
        "  prod2 =torch.sum(prod,dim=1,keepdim=True) \n",
        "  pdf = norm_const * torch.exp( -0.5 *prod2)\n",
        "  final_log_pdf = torch.log(pdf+1e-07)\n",
        "  return final_log_pdf\n",
        "\n",
        "def get_entropy_multi(x, mean, std,Act):\n",
        "  #log pdf (squashed guassian) = log pdfguassian(mupolicy,sigma_policy) - sum of  log(1-A**2) ; where each A is component of tanh squahed action vector \n",
        "  log_pdf_final = get_log_pdf_multi(x,mean,std) - torch.sum(torch.log(1- torch.square(Act) +1e-07),dim=1,keepdim=True)\n",
        "  return -log_pdf_final\n",
        "\n",
        "\n",
        "\n",
        "def replay_and_train(target_entropy,log_alpha,policy,q_1,q_2,t_q_1,t_q_2,popt,qopt,log_alpha_opt,size=128):\n",
        "  #target entropy is a constant so no need to make it into torch tensor\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  gamma=0.99\n",
        "  mse = nn.MSELoss()\n",
        "  alpha = np.exp(log_alpha.item())#get new alpha value\n",
        "  sample_size=size\n",
        "  if len(memory) < sample_size:\n",
        "    return\n",
        "  samples=random.sample(memory,sample_size)\n",
        "  s,a,r,ns,d=zip(*samples)\n",
        "\n",
        "  #s,a,r,ns,d = list(s),list(a),list(r),list(ns),list(d)\n",
        "  #print(s,a,r,ns,d)\n",
        "  s= torch.tensor(s).float().to(device)\n",
        "  a= torch.tensor(a).float().to(device)\n",
        "  r= torch.tensor(r).float().to(device)\n",
        "  ns= torch.tensor(ns).float().to(device)\n",
        "  d= torch.tensor(d).float().to(device)\n",
        "\n",
        "  new_n_mu,new_n_sig = policy.predict(ns)\n",
        "  n_E =np.random.multivariate_normal(np.zeros_like(new_n_mu[0].numpy()),np.diag(np.ones_like(new_n_mu[0].numpy())),size) \n",
        "  n_E = torch.tensor(n_E).float().to(device)\n",
        "  n_Act_guassian = new_n_mu + n_E*new_n_sig # guassian action using reparametrisation\n",
        "  new_next_Action  =torch.tanh(n_Act_guassian)  #final squashed guassian action\n",
        "  n_entropy = get_entropy_multi(n_Act_guassian,new_n_mu,new_n_sig,new_next_Action)\n",
        "  yq = r + gamma*(1-d)*torch.minimum(t_q_1.predict(ns,new_next_Action),t_q_2.predict(ns,new_next_Action)) + alpha*n_entropy # value target should be bootstrapped to current policy\n",
        "  #training nets \n",
        "  #train q nets\n",
        "  qloss = mse(q_1(s,a),yq) + mse(q_2(s,a),yq)\n",
        "  qopt.zero_grad()                                                                                                          #    \n",
        "  qloss.backward()                                                                                                         #\n",
        "  qopt.step() \n",
        "  #train policy\n",
        "  new_mu,new_sig = policy(s)\n",
        "  E =np.random.multivariate_normal(np.zeros_like(new_mu[0].detach().numpy()),np.diag(np.ones_like(new_mu[0].detach().numpy())),size) \n",
        "  E = torch.tensor(E).float().to(device)\n",
        "  Act_guassian = new_mu + E*new_sig # guassian action using reparametrisation\n",
        "  new_Action  =torch.tanh(Act_guassian)  #final squashed guassian action\n",
        "  entropy = get_entropy_multi(Act_guassian,new_mu,new_sig,new_Action)\n",
        "\n",
        "  policy_objective = torch.minimum(q_1(s,new_Action),q_2(s,new_Action))#state from buffer action from recent policy and not from buffer to train to maximise q\n",
        "  final_policy_objective = policy_objective + alpha*entropy # maximise this\n",
        "  final_policy_loss = - torch.mean(final_policy_objective)  #therefore minimise this\n",
        "  popt.zero_grad()                                                                                                          #    \n",
        "  final_policy_loss.backward()                                                                                                         #\n",
        "  popt.step() \n",
        "  #train alpha\n",
        "  log_alpha_loss = torch.mean(log_alpha*(entropy.detach() - target_entropy))\n",
        "  log_alpha_opt.zero_grad()                                                                                                          #    \n",
        "  log_alpha_loss.backward()                                                                                                         #\n",
        "  log_alpha_opt.step() \n",
        "  #train t_value\n",
        "  soft_update(target=t_q_1, source=q_1, tau=0.001)\n",
        "  soft_update(target=t_q_2, source=q_2, tau=0.001)\n",
        "  return qloss.item(),final_policy_loss.item(),log_alpha_loss.item(),torch.mean(entropy).item()\n",
        "\n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsDWkdlVhsGY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dbc21ecb-2e71-4f0b-e5a0-f9670942a9ec"
      },
      "source": [
        "\n",
        "import gym\n",
        "\n",
        "memory=deque(maxlen=5000000)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "#env,networks ad optimisers\n",
        "env=gym.make('Pendulum-v0')\n",
        "#env=gym.make('MountainCarContinuous-v0')\n",
        "#env=gym.make('LunarLanderContinuous-v2')\n",
        "env=env.unwrapped\n",
        "\n",
        "s_dim = env.observation_space.shape[0]\n",
        "print(s_dim)\n",
        "a_dim = env.action_space.shape[0]\n",
        "print(a_dim)\n",
        "a_bound = env.action_space.high[0]\n",
        "print(a_bound)\n",
        "#########################################################################\n",
        "target_entropy = -a_dim# -dim(A)\n",
        "log_alpha = torch.tensor([0.0], requires_grad=True).to(device)\n",
        "policy = Policy(s_dim,a_dim,100).to(device)\n",
        "q_1 = Q_net(s_dim,a_dim,100).to(device)\n",
        "q_2 = Q_net(s_dim,a_dim,100).to(device)\n",
        "t_q_1 = Q_net(s_dim,a_dim,100).to(device)\n",
        "t_q_2 = Q_net(s_dim,a_dim,100).to(device)\n",
        "soft_update(target=t_q_1, source=q_1, tau=1)\n",
        "soft_update(target=t_q_2, source=q_2, tau=1)\n",
        "qopt = optim.Adam(list(q_1.parameters()) + list(q_2.parameters()),lr=0.001)\n",
        "popt = optim.Adam(policy.parameters(),lr=0.001)      \n",
        "log_alpha_opt =  optim.Adam(params=[log_alpha],lr=0.001)\n",
        "#########################################################################\n",
        "max_steps=5000\n",
        "train_iter = 2000  # 1000, batch of 8/16 works best for now\n",
        "test_eps = 335#after this no training\n",
        "warmup = 20 # before this only buffer is filled without training\n",
        "episodes = test_eps+200\n",
        "for e in range(episodes):\n",
        "  done = False\n",
        "  s = env.reset()\n",
        "  rew = 0 \n",
        "  stp=0\n",
        "  while not done:\n",
        "    new_mu,new_sig = policy.predict(torch.tensor([s]).float().to(device))\n",
        "    new_mu,new_sig=new_mu.cpu().numpy(),new_sig.cpu().numpy()\n",
        "    #print(new_mu,new_sig,new_mu[0],new_sig[0])\n",
        "    E =np.random.multivariate_normal(np.zeros_like(new_mu[0]),np.diag(np.ones_like(new_mu[0])))#np.random.normal(mu=0,std_dev=1) diag cov of 1 is same as std dev of 1\n",
        "    Act_guassian = new_mu[0] + new_sig[0]*E # guassian action using reparametrisation\n",
        "    act  =np.tanh(Act_guassian)\n",
        "    if e>test_eps-10:\n",
        "      env.render()\n",
        "      #act = new_mu[0] # testing\n",
        "\n",
        "    s_,r,done,_=env.step(act*a_bound)\n",
        "    #if done:\n",
        "    #  r = 10000\n",
        "    if stp>max_steps:\n",
        "      done = True\n",
        "    if e<=test_eps:    \n",
        "      remember(s,act,r,s_,done)\n",
        "    s=s_\n",
        "    rew+=r\n",
        "    stp+=1\n",
        "  print(e,rew)\n",
        "  if e>warmup and e <= test_eps:\n",
        "    print(\"training\")\n",
        "    p=0\n",
        "    q=0\n",
        "    a=0\n",
        "    e=0\n",
        "    for i in  tqdm(range(train_iter)):\n",
        "      ql,pl,al,ee=replay_and_train(target_entropy,log_alpha,policy,q_1,q_2,t_q_1,t_q_2,popt,qopt,log_alpha_opt,size=8)\n",
        "      p+=pl\n",
        "      q+=ql\n",
        "      a+=al\n",
        "      e+=ee\n",
        "    print(\"p loss\",p/train_iter,\"|q loss \",q/train_iter,\"|alpha loss \",a/train_iter,\"|avg entropy \",e/train_iter)  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "1\n",
            "2.0\n",
            "0 -25222.842255454816\n",
            "1 -30296.520427303873\n",
            "2 -29253.245579475373\n",
            "3 -26590.32169724874\n",
            "4 -29588.22246068339\n",
            "5 -28997.538767979346\n",
            "6 -30976.4101496262\n",
            "7 -30611.770735596507\n",
            "8 -35826.86704074124\n",
            "9 -27615.34144057221\n",
            "10 -34340.218415652686\n",
            "11 -28772.62722366432\n",
            "12 -38849.47271567901\n",
            "13 -27188.860613252553\n",
            "14 -33812.262474530355\n",
            "15 -32594.59622314251\n",
            "16 -31784.288941319403\n",
            "17 -28061.169291971408\n",
            "18 -29517.562886011136\n",
            "19 -28322.741513295015\n",
            "20 -33584.5164193784\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 6/2000 [00:00<00:33, 59.13it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21 -31818.416831428232\n",
            "training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [00:34<00:00, 58.07it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "p loss 49.090103847935794 |q loss  10.571634257160127 |alpha loss  -0.8737437579840771 |avg entropy  0.1898439302444458\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 6/2000 [00:00<00:34, 58.62it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "22 -36960.26437465789\n",
            "training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [00:36<00:00, 55.50it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "p loss 128.39814170074462 |q loss  30.53578739571571 |alpha loss  -0.5615698036830872 |avg entropy  -0.7039721604967489\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 6/2000 [00:00<00:36, 54.41it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "23 -974.9117581646091\n",
            "training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [00:35<00:00, 56.56it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "p loss 178.71576988983153 |q loss  69.91868237662315 |alpha loss  0.1376227659068536 |avg entropy  -1.0630990459807217\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 6/2000 [00:00<00:33, 59.19it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "24 -143.16392843652514\n",
            "training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [00:35<00:00, 56.20it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "p loss 192.55946698379518 |q loss  134.2798135100603 |alpha loss  0.21478118851268663 |avg entropy  -1.1244336513821036\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 7/2000 [00:00<00:32, 60.80it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "25 -377.8528850298341\n",
            "training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [00:37<00:00, 53.95it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "p loss 182.28119353103637 |q loss  127.05148928344249 |alpha loss  -0.02751391660317313 |avg entropy  -0.9808705130694434\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 6/2000 [00:00<00:38, 51.66it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "26 -273.1238226245109\n",
            "training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [00:38<00:00, 52.34it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "p loss 171.2845327720642 |q loss  106.0044349297285 |alpha loss  -0.05509544974437449 |avg entropy  -0.9656449092403054\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 6/2000 [00:00<00:33, 59.32it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "27 -371.3599918267318\n",
            "training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [00:36<00:00, 55.38it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "p loss 158.32802932739258 |q loss  128.59446131634712 |alpha loss  -0.10067165455641225 |avg entropy  -0.9456548450575211\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 5/2000 [00:00<00:41, 48.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "28 -138.22867454163492\n",
            "training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [00:36<00:00, 55.09it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "p loss 151.8316500148773 |q loss  103.68605345988274 |alpha loss  -0.0711063454770483 |avg entropy  -0.9660456793010235\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 7/2000 [00:00<00:32, 61.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "29 -136.8365280874623\n",
            "training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [00:35<00:00, 55.76it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "p loss 138.61200984477998 |q loss  89.79790043902398 |alpha loss  -0.18637724330369382 |avg entropy  -0.9223771478638518\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 6/2000 [00:00<00:37, 52.88it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "30 -197.16265849508642\n",
            "training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [00:35<00:00, 56.33it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "p loss 137.35090455245972 |q loss  91.52525483083726 |alpha loss  -0.006236843514256179 |avg entropy  -0.9971406557522714\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 6/2000 [00:00<00:35, 56.55it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "31 -142.09145325038804\n",
            "training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [00:35<00:00, 56.54it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "p loss 130.2245040359497 |q loss  71.0233547487259 |alpha loss  0.06762262192182243 |avg entropy  -1.0261079611834139\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 6/2000 [00:00<00:34, 57.29it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "32 -257.1995406745323\n",
            "training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [00:36<00:00, 55.30it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "p loss 124.51532713222504 |q loss  77.9536374475956 |alpha loss  0.07189268835261464 |avg entropy  -1.0293394185164944\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 6/2000 [00:00<00:35, 56.89it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "33 -247.52639395704784\n",
            "training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [00:35<00:00, 55.95it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "p loss 117.1976464176178 |q loss  63.31087055319548 |alpha loss  -0.14882694161869586 |avg entropy  -0.9403860767222941\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 6/2000 [00:00<00:38, 51.40it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "34 -378.7351438432642\n",
            "training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 26%|██▋       | 528/2000 [00:09<00:26, 55.15it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-07bd93d6a9f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0me\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m       \u001b[0mql\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mee\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreplay_and_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_entropy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_alpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_q_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_q_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpopt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mqopt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_alpha_opt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m       \u001b[0mp\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m       \u001b[0mq\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mql\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-07bd93d6a9f6>\u001b[0m in \u001b[0;36mreplay_and_train\u001b[0;34m(target_entropy, log_alpha, policy, q_1, q_2, t_q_1, t_q_2, popt, qopt, log_alpha_opt, size)\u001b[0m\n\u001b[1;32m    145\u001b[0m   \u001b[0mfinal_policy_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_policy_objective\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#therefore minimise this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m   \u001b[0mpopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                                                                                                          \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m   \u001b[0mfinal_policy_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                                                                                                         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m   \u001b[0mpopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m   \u001b[0;31m#train alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2BmDArNKTC1",
        "outputId": "d36d56a1-887f-4cb5-c624-b3f71242e35d"
      },
      "source": [
        "\n",
        "import gym\n",
        "\n",
        "memory=deque(maxlen=5000000)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "#env,networks ad optimisers\n",
        "#env=gym.make('Pendulum-v0')\n",
        "env=gym.make('MountainCarContinuous-v0')\n",
        "#env=gym.make('LunarLanderContinuous-v2')\n",
        "env=env.unwrapped\n",
        "\n",
        "s_dim = env.observation_space.shape[0]\n",
        "print(s_dim)\n",
        "a_dim = env.action_space.shape[0]\n",
        "print(a_dim)\n",
        "a_bound = env.action_space.high[0]\n",
        "print(a_bound)\n",
        "#########################################################################\n",
        "target_entropy = -a_dim# -dim(A)\n",
        "log_alpha = torch.tensor([0.0], requires_grad=True).to(device)\n",
        "policy = Policy(s_dim,a_dim,256).to(device)\n",
        "q_1 = Q_net(s_dim,a_dim,256).to(device)\n",
        "q_2 = Q_net(s_dim,a_dim,256).to(device)\n",
        "t_q_1 = Q_net(s_dim,a_dim,256).to(device)\n",
        "t_q_2 = Q_net(s_dim,a_dim,256).to(device)\n",
        "soft_update(target=t_q_1, source=q_1, tau=1)\n",
        "soft_update(target=t_q_2, source=q_2, tau=1)\n",
        "qopt = optim.Adam(list(q_1.parameters()) + list(q_2.parameters()),lr=0.001)\n",
        "popt = optim.Adam(policy.parameters(),lr=0.001)      \n",
        "log_alpha_opt =  optim.Adam(params=[log_alpha],lr=0.001)\n",
        "#########################################################################\n",
        "max_steps=5000\n",
        "train_iter = 1000  # 1000, batch of 8/16 works best for now\n",
        "test_eps = 335#after this no training\n",
        "warmup = 20 # before this only buffer is filled without training\n",
        "episodes = test_eps+200\n",
        "for e in range(episodes):\n",
        "  done = False\n",
        "  s = env.reset()\n",
        "  rew = 0 \n",
        "  stp=0\n",
        "  for step in range(max_steps):\n",
        "    if done:\n",
        "      s = env.reset()    \n",
        "    new_mu,new_sig = policy.predict(torch.tensor([s]).float().to(device))\n",
        "    new_mu,new_sig=new_mu.cpu().numpy(),new_sig.cpu().numpy()\n",
        "    #print(new_mu,new_sig,new_mu[0],new_sig[0])\n",
        "    E =np.random.multivariate_normal(np.zeros_like(new_mu[0]),np.diag(np.ones_like(new_mu[0])))#np.random.normal(mu=0,std_dev=1) diag cov of 1 is same as std dev of 1\n",
        "    Act_guassian = new_mu[0] + new_sig[0]*E # guassian action using reparametrisation\n",
        "    act  =np.tanh(Act_guassian)\n",
        "    if e>test_eps-10:\n",
        "      env.render()\n",
        "      #act = new_mu[0] # testing\n",
        "    if e < warmup :\n",
        "      # for some additional exploration not necesserily needed\n",
        "      if E > 0.5:\n",
        "        act = np.clip(a_bound*E,-a_bound,a_bound)\n",
        "    s_,r,done,_=env.step(act*a_bound)\n",
        "    if done :\n",
        "      r =r+1000 # to encourage reaching target more\n",
        "      print(\"reached\")\n",
        "\n",
        "    if e<=test_eps:    \n",
        "      remember(s,act,r,s_,done)\n",
        "    s=s_\n",
        "    rew+=r\n",
        "    stp+=1\n",
        "  print(e,rew)\n",
        "  if e>0 and e <= test_eps:\n",
        "    print(\"training\")\n",
        "    p=0\n",
        "    q=0\n",
        "    a=0\n",
        "    e=0\n",
        "    for i in  range(train_iter):\n",
        "      ql,pl,al,ee=replay_and_train(target_entropy,log_alpha,policy,q_1,q_2,t_q_1,t_q_2,popt,qopt,log_alpha_opt,size=16)\n",
        "      p+=pl\n",
        "      q+=ql\n",
        "      a+=al\n",
        "      e+=ee\n",
        "      if i % (train_iter//10)==0:\n",
        "        print('.',end='')#tqdm sometimes has issues in colab so did this \n",
        "    print('|')        \n",
        "    print(\"p loss\",p/train_iter,\"|q loss \",q/train_iter,\"|alpha loss \",a/train_iter,\"|avg entropy \",e/train_iter)  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "1\n",
            "1.0\n",
            "reached\n",
            "0 913.99809764594\n",
            "reached\n",
            "1 916.1790499573405\n",
            "training\n",
            "..........|\n",
            "p loss -0.9757406290769577 |q loss  302.5312634554472 |alpha loss  -0.8413645657507004 |avg entropy  0.6812703840136528\n",
            "2 -199.42869662691916\n",
            "training\n",
            "..........|\n",
            "p loss -0.6142752554416656 |q loss  151.1950949425447 |alpha loss  -2.510945767402649 |avg entropy  0.675104011297226\n",
            "3 -194.32615789042165\n",
            "training\n",
            "..........|\n",
            "p loss -0.494650959700346 |q loss  302.3434837314311 |alpha loss  -4.109535607814789 |avg entropy  0.6494722853600979\n",
            "4 -174.4450626007392\n",
            "training\n",
            "..........|\n",
            "p loss -0.42509693949669597 |q loss  302.2682649302877 |alpha loss  -5.422665097236633 |avg entropy  0.5657113275080919\n",
            "5 -148.54560327173942\n",
            "training\n",
            "..........|\n",
            "p loss -0.20876643967628478 |q loss  0.001610347646928858 |alpha loss  -4.468893990039826 |avg entropy  0.04021861305925995\n",
            "6 -133.44223360142158\n",
            "training\n",
            "..........|\n",
            "p loss -0.17914595772325992 |q loss  9.0638146870333e-05 |alpha loss  -3.5103050490617753 |avg entropy  -0.2949856579229236\n",
            "reached\n",
            "7 969.4936677546283\n",
            "training\n",
            "..........|\n",
            "p loss -0.238434774113819 |q loss  151.20286409323472 |alpha loss  -2.4982062837667764 |avg entropy  -0.5513430338110775\n",
            "8 -123.38698875567262\n",
            "training\n",
            "..........|\n",
            "p loss -0.16182816416025161 |q loss  302.3873367632235 |alpha loss  -0.7508050512708724 |avg entropy  -0.8737678366228938\n",
            "reached\n",
            "9 978.9023446506571\n",
            "training\n",
            "..........|\n",
            "p loss -0.18111412192881107 |q loss  0.001432475816422084 |alpha loss  0.34722038565576074 |avg entropy  -1.0579515388607978\n",
            "reached\n",
            "10 976.0158874564369\n",
            "training\n",
            "..........|\n",
            "p loss -0.1930236172680743 |q loss  302.2546499847497 |alpha loss  0.023627417227253318 |avg entropy  -1.003978070691228\n",
            "reached\n",
            "11 978.472520829578\n",
            "training\n",
            "..........|\n",
            "p loss -0.2418320822864771 |q loss  0.000766649532250085 |alpha loss  -0.09439580670744181 |avg entropy  -0.9840350786298513\n",
            "12 -117.8399773937209\n",
            "training\n",
            "..........|\n",
            "p loss -0.2397327541038394 |q loss  151.0166968793313 |alpha loss  0.03403338317200542 |avg entropy  -1.0056865400969983\n",
            "13 -123.75615619422312\n",
            "training\n",
            "..........|\n",
            "p loss -0.22178078766167164 |q loss  0.0092500562092755 |alpha loss  0.028662147134542466 |avg entropy  -1.004812039345503\n",
            "14 -127.33803179484184\n",
            "training\n",
            "..........|\n",
            "p loss -0.16583096599322744 |q loss  301.13420557246945 |alpha loss  -0.010962422017008066 |avg entropy  -0.9980107670724392\n",
            "reached\n",
            "15 974.7952015307169\n",
            "training\n",
            "..........|\n",
            "p loss -0.014273019646876491 |q loss  148.53378695123342 |alpha loss  -0.02765606415271759 |avg entropy  -0.9951194878071546\n",
            "16 -151.01610161522646\n",
            "training\n",
            "..........|\n",
            "p loss -0.009729820228647441 |q loss  566.0607192371609 |alpha loss  0.9092143151164055 |avg entropy  -1.1590247817337513\n",
            "17 -187.26934001490807\n",
            "training\n",
            "..........|\n",
            "p loss -0.06916263798065483 |q loss  390.1180604737627 |alpha loss  0.5318628981411457 |avg entropy  -1.0976827631201596\n",
            "18 -186.0989854350337\n",
            "training\n",
            "..........|\n",
            "p loss 0.1718774225693196 |q loss  9.682969813202508 |alpha loss  0.9319585432820022 |avg entropy  -1.1814754337221385\n",
            "19 -143.189580740975\n",
            "training\n",
            "..........|\n",
            "p loss 0.5721900093629956 |q loss  269.14304411570754 |alpha loss  0.11097560236416758 |avg entropy  -1.0221961681693792\n",
            "20 -77.12848690499649\n",
            "training\n",
            "..........|\n",
            "p loss 0.9914834379106761 |q loss  95.57092454080586 |alpha loss  0.5828034703433513 |avg entropy  -1.1225596179664135\n",
            "21 -97.7861521751773\n",
            "training\n",
            "..........|\n",
            "p loss 1.1337301501333714 |q loss  103.6469518993902 |alpha loss  0.2829118815921247 |avg entropy  -1.0631162949800492\n",
            "22 -138.231959552364\n",
            "training\n",
            "..........|\n",
            "p loss 1.0470431663244963 |q loss  132.8169833602053 |alpha loss  0.11514012352190912 |avg entropy  -1.0265227357447146\n",
            "23 -151.04088742898185\n",
            "training\n",
            "..........|\n",
            "p loss 0.9482173046916723 |q loss  31.396463222278282 |alpha loss  0.3389387476965785 |avg entropy  -1.0823504870980978\n",
            "24 -179.0208398557651\n",
            "training\n",
            "..........|\n",
            "p loss 0.8954556315541268 |q loss  53.65099469881505 |alpha loss  -0.10707819590903819 |avg entropy  -0.9731438704505563\n",
            "25 -203.75759126531153\n",
            "training\n",
            "..........|\n",
            "p loss 1.224184288531542 |q loss  69.68347117716074 |alpha loss  -0.03169415395334363 |avg entropy  -0.9921193727403879\n",
            "26 -158.69524693064653\n",
            "training\n",
            "..........|\n",
            "p loss 0.9105762338638306 |q loss  87.93658714867011 |alpha loss  0.0880799564383924 |avg entropy  -1.021691043049097\n",
            "27 -147.11486183445464\n",
            "training\n",
            "..........|\n",
            "p loss 1.2822831408381463 |q loss  46.37645193696115 |alpha loss  -0.056678998210467395 |avg entropy  -0.9857755007147789\n",
            "28 -220.69921583373647\n",
            "training\n",
            "..........|\n",
            "p loss 1.1608390572071074 |q loss  166.32859922017903 |alpha loss  -0.35011473582684993 |avg entropy  -0.9171594612672925\n",
            "29 -208.37621524006659\n",
            "training\n",
            "..........|\n",
            "p loss 1.6949474795609714 |q loss  23.307852914582938 |alpha loss  -0.06525565826147794 |avg entropy  -0.9852322709262371\n",
            "30 -156.7754780219516\n",
            "training\n",
            "..........|\n",
            "p loss 1.300927185446024 |q loss  64.09980385885387 |alpha loss  0.26320868843235073 |avg entropy  -1.0601640338227152\n",
            "31 -199.75573432940712\n",
            "training\n",
            "..........|\n",
            "p loss 1.4555286236703395 |q loss  31.923009111233057 |alpha loss  -0.22057645641639828 |avg entropy  -0.9490886371806264\n",
            "32 -214.60505551655353\n",
            "training\n",
            "..........|\n",
            "p loss 1.794650239855051 |q loss  55.5990705938153 |alpha loss  -0.057728573413565756 |avg entropy  -0.9869171938449144\n",
            "reached\n",
            "33 989.3343078658489\n",
            "training\n",
            "..........|\n",
            "p loss 1.4914542674422264 |q loss  74.40765029102285 |alpha loss  -0.05582725449465215 |avg entropy  -0.9871227690316736\n",
            "reached\n",
            "34 955.1149572168162\n",
            "training\n",
            "..........|\n",
            "p loss 1.3946505379080771 |q loss  35.78134137668368 |alpha loss  -0.42460486900061367 |avg entropy  -0.9100773516334594\n",
            "35 -129.15432718706862\n",
            "training\n",
            "..........|\n",
            "p loss 1.2327495981752872 |q loss  22.506241778131574 |alpha loss  -0.3182366025559604 |avg entropy  -0.9363233545422553\n",
            "reached\n",
            "36 946.6413458270752\n",
            "training\n",
            "..........|\n",
            "p loss 1.1714346630871295 |q loss  40.65282099818997 |alpha loss  0.893841097239405 |avg entropy  -1.1831752317696809\n",
            "37 -167.6659059203939\n",
            "training\n",
            "..........|\n",
            "p loss 1.559068526029587 |q loss  50.67382230148837 |alpha loss  0.7615087347831577 |avg entropy  -1.1724403840303421\n",
            "reached\n",
            "38 955.048729585245\n",
            "training\n",
            "..........|\n",
            "p loss 1.9798989064097405 |q loss  58.33154932009429 |alpha loss  0.17950944030284882 |avg entropy  -1.0432979145236314\n",
            "39 -190.27230463020516\n",
            "training\n",
            "..........|\n",
            "p loss 2.2873614404797555 |q loss  32.12022340753116 |alpha loss  -0.3453227640539408 |avg entropy  -0.9172860163711012\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "40 5352.32290364848\n",
            "training\n",
            "..........|\n",
            "p loss 1.6391657637953758 |q loss  70.09288887778297 |alpha loss  -0.11783809463307261 |avg entropy  -0.9729824826866388\n",
            "41 -149.4239065109241\n",
            "training\n",
            "..........|\n",
            "p loss 2.1208409066200256 |q loss  38.30031706943363 |alpha loss  0.1335952316224575 |avg entropy  -1.030510252557695\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "42 5352.335984296275\n",
            "training\n",
            "..........|\n",
            "p loss 1.566754737019539 |q loss  40.89208612368628 |alpha loss  0.11954742628708481 |avg entropy  -1.0280743110179902\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "43 5377.9078472146775\n",
            "training\n",
            "..........|\n",
            "p loss 1.9771033396720887 |q loss  49.81071458629332 |alpha loss  -0.06664925547689199 |avg entropy  -0.9840733914375305\n",
            "reached\n",
            "reached\n",
            "44 2057.512860198617\n",
            "training\n",
            "..........|\n",
            "p loss 1.206182782769203 |q loss  57.29985915608797 |alpha loss  0.005880367772653699 |avg entropy  -1.0013027760237456\n",
            "reached\n",
            "45 1004.432005774573\n",
            "training\n",
            "..........|\n",
            "p loss 0.9796988134384155 |q loss  47.24290036745369 |alpha loss  -0.33827408363856376 |avg entropy  -0.9229262631833554\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "46 4291.4253877335395\n",
            "training\n",
            "..........|\n",
            "p loss 1.8392952065467834 |q loss  55.087063182543034 |alpha loss  0.3952453573383391 |avg entropy  -1.0896623049266636\n",
            "47 -89.58729433700529\n",
            "training\n",
            "..........|\n",
            "p loss 1.610577041745186 |q loss  36.672249054474754 |alpha loss  -0.38090217288769784 |avg entropy  -0.9130951403677463\n",
            "reached\n",
            "reached\n",
            "48 2156.3299293723626\n",
            "training\n",
            "..........|\n",
            "p loss 1.4613108298182487 |q loss  31.40900961248204 |alpha loss  -0.15535904906131326 |avg entropy  -0.9660336139053106\n",
            "49 -42.344585604374544\n",
            "training\n",
            "..........|\n",
            "p loss 1.7422150485515595 |q loss  55.85479224495217 |alpha loss  0.3448838906548917 |avg entropy  -1.0764379267692565\n",
            "50 -49.284854074517554\n",
            "training\n",
            "..........|\n",
            "p loss 1.4998539668023587 |q loss  21.207417215842753 |alpha loss  0.5038169915433973 |avg entropy  -1.1209025613963604\n",
            "reached\n",
            "51 1029.5599054924571\n",
            "training\n",
            "..........|\n",
            "p loss 2.229233254015446 |q loss  72.94492058383301 |alpha loss  -0.4391853486075997 |avg entropy  -0.8935762244164944\n",
            "52 -30.900983137306703\n",
            "training\n",
            "..........|\n",
            "p loss 1.0509944871068 |q loss  77.66104814406485 |alpha loss  0.27272927766665817 |avg entropy  -1.0641592339202761\n",
            "53 -52.83749035028144\n",
            "training\n",
            "..........|\n",
            "p loss 2.0299349407553673 |q loss  52.89113629772142 |alpha loss  -0.48500513352826236 |avg entropy  -0.8862735242266208\n",
            "54 -167.64088217908463\n",
            "training\n",
            "..........|\n",
            "p loss 2.418254526555538 |q loss  25.620394860785453 |alpha loss  -0.14607580363005399 |avg entropy  -0.9672494760900736\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "55 7571.519149327798\n",
            "training\n",
            "..........|\n",
            "p loss 2.6520495924949645 |q loss  151.3245054933764 |alpha loss  -0.2226305700838566 |avg entropy  -0.9485177801996469\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "56 15199.22920687648\n",
            "training\n",
            "..........|\n",
            "p loss 1.0822309450507164 |q loss  35.065383912108835 |alpha loss  -0.13730723163112998 |avg entropy  -0.969756409805268\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "57 7536.065492572979\n",
            "training\n",
            "..........|\n",
            "p loss 0.6732853929996491 |q loss  94.7180248701414 |alpha loss  0.23794320729374885 |avg entropy  -1.0526617147624493\n",
            "58 -116.1643660348922\n",
            "training\n",
            "...."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSBe5zUsNvBQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}