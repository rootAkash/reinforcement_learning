{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SAC_v2_torch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP6g3zvkPQZuBlNvjPhTq/Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rootAkash/reinforcement_learning/blob/master/Soft%20AC/SAC_v2_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "324AvlanSTJH"
      },
      "source": [
        "#much better version can be created where we dont use .predict but while generating labels \n",
        "#but we create the graph while generating labels for policy and value and qs and then the process becomes more efficient\n",
        "#also instead of looping thru samples do vectorisation"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plLsCJOlQYC3"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "class Policy(nn.Module):\n",
        "\n",
        "    def __init__(self,observation_size,action_size,hidden_units):\n",
        "        super().__init__()\n",
        "        self.observation_size=observation_size\n",
        "        self.hidden_units=hidden_units\n",
        "        self.action_size = action_size\n",
        "        self.h1 = nn.Linear(self.observation_size, 2*self.hidden_units)  \n",
        "        #self.h2 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        #self.h3 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        #self.h4 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        #self.h5 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        self.h6 = nn.Linear(2*self.hidden_units,self.hidden_units)\n",
        "        self.mu = nn.Linear(self.hidden_units,self.action_size)\n",
        "        self.sigma = nn.Linear(self.hidden_units,self.action_size)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.h1(x))\n",
        "        #x = F.relu(self.h2(x))\n",
        "        #x = F.relu(self.h3(x))\n",
        "        #x = F.relu(self.h4(x))\n",
        "        #x = F.relu(self.h5(x))\n",
        "        x = F.relu(self.h6(x))\n",
        "        mus = torch.tanh(self.mu(x))\n",
        "        sigs= F.softplus(self.sigma(x))\n",
        "        sigs= torch.clamp(sigs, min=0.001, max=100)#1e-22 , 1e+02\n",
        "        return mus , sigs\n",
        "    def predict(self, x):\n",
        "        with torch.no_grad():\n",
        "          output=self.forward(x)\n",
        "        return output  \n",
        "\n",
        "class Value(nn.Module):\n",
        "    def __init__(self,observation_size,hidden_units):\n",
        "        super().__init__()\n",
        "        self.observation_size=observation_size\n",
        "        self.hidden_units=hidden_units\n",
        "        self.h1 = nn.Linear(self.observation_size, 2*self.hidden_units)  \n",
        "        #self.h2 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        #self.h3 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        #self.h4 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        #self.h5 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        self.h6 = nn.Linear(2*self.hidden_units,self.hidden_units)\n",
        "        self.v = nn.Linear(self.hidden_units,1)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.h1(x))\n",
        "        #x = F.relu(self.h2(x))\n",
        "        #x = F.relu(self.h3(x))\n",
        "        #x = F.relu(self.h4(x))\n",
        "        #x = F.relu(self.h5(x))\n",
        "        x = F.relu(self.h6(x))\n",
        "        vout = self.v(x)\n",
        "        return vout\n",
        "    def predict(self, x):\n",
        "        with torch.no_grad():\n",
        "          output=self.forward(x)\n",
        "        return output    \n",
        "class Q_net(nn.Module):\n",
        "\n",
        "    def __init__(self,observation_size,action_size,hidden_units):\n",
        "        super().__init__()\n",
        "        self.observation_size=observation_size\n",
        "        self.hidden_units=hidden_units\n",
        "        self.action_size = action_size\n",
        "        self.h1 = nn.Linear(self.observation_size, self.hidden_units) \n",
        "        self.a1 = nn.Linear(self.action_size, self.hidden_units) \n",
        "        self.h2 = nn.Linear(self.hidden_units*2,self.hidden_units)\n",
        "        #self.h3 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        #self.h4 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        #self.h5 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        #self.h6 = nn.Linear(self.hidden_units,self.hidden_units)\n",
        "        self.q = nn.Linear(self.hidden_units,1)\n",
        "    def forward(self, x,a):\n",
        "        x = F.relu(self.h1(x))\n",
        "        a = F.relu(self.a1(a))\n",
        "        x = torch.cat([x,a], dim=1)\n",
        "        x = F.relu(self.h2(x))\n",
        "        #x = F.relu(self.h3(x))\n",
        "        #x = F.relu(self.h4(x))\n",
        "        #x = F.relu(self.h5(x))\n",
        "        #x = F.relu(self.h6(x))\n",
        "        qout = self.q(x)\n",
        "        return qout\n",
        "    def predict(self, x,a):\n",
        "        with torch.no_grad():\n",
        "          output=self.forward(x,a)\n",
        "        return output\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3bHlG__7Dft"
      },
      "source": [
        "def remember(s,a,r,ns,d):\n",
        "  #s=s.ravel()\n",
        "  #ns=ns.ravel()\n",
        "  memory.append([s,a,r,ns,d])\n",
        "def sample_games(buffer,batch_size):\n",
        "  # Sample game from buffer either uniformly or according to some priority\n",
        "  #print(\"samplig from .\",len(buffer))\n",
        "  return list(np.random.choice(len(buffer),batch_size))\n",
        "def soft_update(target, source, tau):\n",
        "  for target_param, param in zip(target.parameters(), source.parameters()):\n",
        "    target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
        "  return target\n",
        "def get_log_pdf(x, mean, std):\n",
        "  sigma_sq = (std)**2\n",
        "  pdf= 1./(2*3.1415926*sigma_sq)**0.5 * math.exp(-(float(x)-float(mean))**2/(2.*sigma_sq))\n",
        "  log_pdf = np.log(pdf) \n",
        "  return log_pdf\n",
        "def get_entropy(x, mean, std,Act):\n",
        "  sigma_sq = (std)**2 #variace\n",
        "  pdf= 1./(2*3.1415926*sigma_sq)**0.5 * math.exp(-(float(x)-float(mean))**2/(2.*sigma_sq))\n",
        "  ent = np.log(pdf) - np.log((1- (Act)**2 )+ 1e-07) # pdf of squashed guassian Act is after squashing  by tanh and x is before that\n",
        "  #log a - log b = log a/b\n",
        "  return -ent\n",
        "def get_log_pdf_multi(x,mean,std):\n",
        "  k= x.shape[0] #actio dim\n",
        "  pi = 3.1415926\n",
        "  cov = std**2\n",
        "  det = np.prod(cov)\n",
        "  norm_const = 1.0/ ( np.power(2*pi,k/2) * np.power(det,0.5) )\n",
        "  prod  = (1/cov)*np.square(x - mean)\n",
        "  prod2 =np.sum(prod) \n",
        "  pdf = norm_const * np.exp( -0.5 *prod2)\n",
        "  return np.log(pdf+1e-07)\n",
        "\n",
        "def get_entropy_multi(x, mean, std,Act):\n",
        "  #log pdf (squashed guassian) = log pdfguassian(mupolicy,sigma_policy) - sum of  log(1-A**2) ; where each A is component of tanh squahed action vector \n",
        "  log_pdf_final = get_log_pdf_multi(x,mean,std) - np.sum(np.log(1- (Act)**2 +1e-07))\n",
        "  return -log_pdf_final\n",
        "\n",
        "\n",
        "\n",
        "def replay_and_train(policy,value,t_value,q_1,q_2,popt,vopt,qopt,size=128):\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  gamma=0.95\n",
        "  mse = nn.MSELoss()\n",
        "  alpha = 1.0\n",
        "  sample_size=size\n",
        "  if len(memory) < sample_size:\n",
        "    return\n",
        "  samples=random.sample(memory,sample_size)\n",
        "  y_q=[]\n",
        "  states =[]\n",
        "  y_v=[]\n",
        "  Es=[]\n",
        "  Acts = []\n",
        "  for sample in samples:\n",
        "    s,a,r,ns,d=sample\n",
        "    yq = r + gamma*(1-d)*t_value.predict(torch.tensor([ns]).float().to(device)).cpu().numpy()[0]\n",
        "    new_mu,new_sig = policy.predict(torch.tensor([s]).float().to(device))\n",
        "    new_mu,new_sig = new_mu.cpu().numpy(),new_sig.cpu().numpy()\n",
        "    #print(new_mu,new_sig,new_mu[0],new_sig[0])\n",
        "    E =np.random.multivariate_normal(np.zeros_like(new_mu[0]),np.diag(np.ones_like(new_mu[0])))#np.random.normal(mu=0,std_dev=1) diag cov of 1 is same as std dev of 1\n",
        "    #print(E)\n",
        "    Act_guassian = new_mu[0] + new_sig[0]*E # guassian action using reparametrisation\n",
        "    new_Action  =np.tanh(Act_guassian)  #final squashed guassian action\n",
        "    entropy = get_entropy_multi(Act_guassian,new_mu,new_sig,new_Action)\n",
        "    #print(s,new_Action,s.shape,new_Action.shape,torch.tensor([s]).float(),torch.tensor([s]).float().shape)\n",
        "    yv = min(q_1.predict(torch.tensor([s]).float().to(device),torch.tensor([new_Action]).float().to(device)).cpu().numpy()[0],\n",
        "             q_2.predict(torch.tensor([s]).float().to(device),torch.tensor([new_Action]).float().to(device)).cpu().numpy()[0]) + alpha*entropy\n",
        "    #print(yv,yq)\n",
        "    Es.append(E)#for policy loss and reparameterisation\n",
        "    y_q.append(yq)\n",
        "    y_v.append(yv)\n",
        "    states.append(s)\n",
        "    Acts.append(a)\n",
        "  Es = torch.tensor(Es).float().to(device)\n",
        "  y_q = torch.tensor(y_q).float().to(device)\n",
        "  y_v = torch.tensor(y_v).float().to(device)\n",
        "  states = torch.tensor(states).float().to(device)\n",
        "  Acts = torch.tensor(Acts).float().to(device)\n",
        "  #training nets \n",
        "  #train q nets\n",
        "  qloss = mse(q_1(states,Acts),y_q) + mse(q_2(states,Acts),y_q)\n",
        "  qopt.zero_grad()                                                                                                          #    \n",
        "  qloss.backward()                                                                                                         #\n",
        "  qopt.step() \n",
        "  #train v net\n",
        "  vloss = mse(value(states),y_v)\n",
        "  vopt.zero_grad()                                                                                                          #    \n",
        "  vloss.backward()                                                                                                         #\n",
        "  vopt.step() \n",
        "  #train policy \n",
        "  #we dont have to do this additional computation actually since its done before but that was in loop\n",
        "  #when vectorised no need to do this \n",
        "  mus,sigs = policy(states)\n",
        "  #print(mus.shape,Es.shape,sigs.shape,(Es*sigs).shape)\n",
        "  x= mus + Es*sigs\n",
        "  actions = torch.tanh(x)\n",
        "  k= x.shape[1] #action dim\n",
        "  pi = 3.1415926\n",
        "  cov = sigs**2\n",
        "  det = torch.prod(cov,dim=1,keepdim=True)\n",
        "  norm_const = 1.0/ ( np.power(2*pi,k/2) * torch.pow(det,0.5) )\n",
        "  prod  = (1/cov)*torch.square(x - mus)\n",
        "  prod2 =torch.sum(prod,dim=1,keepdim=True) \n",
        "  pdf = norm_const * torch.exp( -0.5 *prod2)\n",
        "  final_log_pdf = torch.log(pdf+1e-07)  - torch.sum(torch.log(1- torch.square(actions) +1e-07),dim=1,keepdim=True)\n",
        "  entropy = - final_log_pdf\n",
        "  policy_objective = q_1(states,actions)#state from buffer action from recent policy and not from buffer to train to maximise q\n",
        "  final_policy_objective = policy_objective + alpha*entropy # maximise this\n",
        "  final_policy_loss = - torch.mean(final_policy_objective)  #therefore minimise this\n",
        "  popt.zero_grad()                                                                                                          #    \n",
        "  final_policy_loss.backward()                                                                                                         #\n",
        "  popt.step() \n",
        "  \n",
        "  #train t_value\n",
        "  soft_update(target=t_value, source=value, tau=0.01)\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rB3haEtjPkU",
        "outputId": "7aa97b9e-c986-4928-a993-294592f11c0c"
      },
      "source": [
        "import gym\n",
        "\n",
        "memory=deque(maxlen=5000000)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "#env,networks ad optimisers\n",
        "#env=gym.make('Pendulum-v0')\n",
        "env=gym.make('MountainCarContinuous-v0')\n",
        "\n",
        "env=env.unwrapped\n",
        "\n",
        "s_dim = env.observation_space.shape[0]\n",
        "print(s_dim)\n",
        "a_dim = env.action_space.shape[0]\n",
        "print(a_dim)\n",
        "a_bound = env.action_space.high[0]\n",
        "print(a_bound)\n",
        "\n",
        "policy = Policy(s_dim,a_dim,100).to(device)\n",
        "value = Value(s_dim,100).to(device)\n",
        "t_value = Value(s_dim,100).to(device)\n",
        "soft_update(target=t_value, source=value, tau=1)\n",
        "q_1 = Q_net(s_dim,a_dim,100).to(device)\n",
        "q_2 = Q_net(s_dim,a_dim,100).to(device)\n",
        "qopt = optim.Adam(list(q_1.parameters()) + list(q_2.parameters()),lr=0.001,betas=(0.5, 0.999))\n",
        "popt = optim.Adam(policy.parameters(),lr=0.001,betas=(0.5, 0.999))\n",
        "vopt = optim.Adam(value.parameters(),lr=0.001,betas=(0.5, 0.999))        \n",
        "max_steps=5000\n",
        "episodes = 5000\n",
        "steps = 3500 \n",
        "ctr = 0\n",
        "render =False\n",
        "train_iter = 1000  # 1000, batch of 8/16 works best for now\n",
        "\n",
        "for ep in range(episodes):\n",
        "\ts = env.reset()\n",
        "\tdone=False\n",
        "\trews=0\n",
        "\tif ep>1500:\n",
        "\t\trender=1\n",
        "\tfor step in range(steps):\n",
        "\t\tif done:\n",
        "\t\t\ts = env.reset()\n",
        "\t\tif render:\n",
        "\t\t\tenv.render()\t\n",
        "   \n",
        "\t\tnew_mu,new_sig = policy.predict(torch.tensor([s]).float().to(device))\n",
        "\t\t\n",
        "\t\tmu,sig=new_mu.cpu().numpy(),new_sig.cpu().numpy()\n",
        "\t\tE =np.random.multivariate_normal(np.zeros_like(mu[0]),np.diag(np.ones_like(sig[0])))#np.random.normal(0,1)\n",
        "\n",
        "\t\tAction  =np.tanh(mu[0] + sig[0]*E)\n",
        "\t\tif ep < 10 :\n",
        "\t\t\t# for some additional exploration not necesserily needed\n",
        "\t\t\tif E > 0.5:\n",
        "\t\t\t\tAction = np.clip(a_bound*E,-a_bound,a_bound)\t\t\n",
        "\t\ts_,r,done,_=env.step(Action*a_bound)\n",
        "\t\tif done :\n",
        "\t\t\tr =r+10000\t# to encourage reaching target more\n",
        "\t\t\tprint(\"reached\")\n",
        "\t\tremember(s,Action,r,s_,done)\n",
        "\t\trews+=r\t\n",
        "\t\tctr+=1\n",
        "\t\ts=s_\n",
        "\tprint(\"episode: \"+str(ep)+ \" rews: \"+str(rews))\t\t\n",
        "\tprint(\"training\")\n",
        "\tfor i in  range(train_iter):\n",
        "\t\treplay_and_train(policy,value,t_value,q_1,q_2,popt,vopt,qopt,size=16)\n",
        "\t\tif i % (train_iter//10)==0:\n",
        "\t\t\tprint('.',end='')\n",
        "\tprint('|')\t"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "1\n",
            "1.0\n",
            "episode: 0 rews: -125.45088658864003\n",
            "training\n",
            "..........|\n",
            "episode: 1 rews: -142.58479914934682\n",
            "training\n",
            "..........|\n",
            "episode: 2 rews: -148.28171052453268\n",
            "training\n",
            "..........|\n",
            "episode: 3 rews: -139.23786982325126\n",
            "training\n",
            "..........|\n",
            "episode: 4 rews: -148.41020043911286\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 5 rews: 9952.47542697799\n",
            "training\n",
            "..........|\n",
            "episode: 6 rews: -153.64919026695753\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 7 rews: 9952.220636184984\n",
            "training\n",
            "..........|\n",
            "episode: 8 rews: -152.1769298895317\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 9 rews: 9957.274953742517\n",
            "training\n",
            "..........|\n",
            "episode: 10 rews: -116.83564227277432\n",
            "training\n",
            "..........|\n",
            "episode: 11 rews: -121.55723536837017\n",
            "training\n",
            "..........|\n",
            "episode: 12 rews: -115.01780376123126\n",
            "training\n",
            "..........|\n",
            "episode: 13 rews: -118.45500642149209\n",
            "training\n",
            "..........|\n",
            "episode: 14 rews: -46.191101952046225\n",
            "training\n",
            "..........|\n",
            "episode: 15 rews: -129.63319782408317\n",
            "training\n",
            "..........|\n",
            "episode: 16 rews: -120.70486565544185\n",
            "training\n",
            "..........|\n",
            "episode: 17 rews: -116.80786868624308\n",
            "training\n",
            "..........|\n",
            "episode: 18 rews: -120.68500419506823\n",
            "training\n",
            "..........|\n",
            "episode: 19 rews: -106.50566711059487\n",
            "training\n",
            "..........|\n",
            "episode: 20 rews: -110.12477493121202\n",
            "training\n",
            "..........|\n",
            "episode: 21 rews: -112.44963705260399\n",
            "training\n",
            "..........|\n",
            "episode: 22 rews: -114.74697969783828\n",
            "training\n",
            "..........|\n",
            "episode: 23 rews: -115.2569212434196\n",
            "training\n",
            "..........|\n",
            "episode: 24 rews: -121.02718734133717\n",
            "training\n",
            "..........|\n",
            "episode: 25 rews: -115.4405487054764\n",
            "training\n",
            "..........|\n",
            "episode: 26 rews: -115.54313638712604\n",
            "training\n",
            "..........|\n",
            "episode: 27 rews: -112.56633746875595\n",
            "training\n",
            "..........|\n",
            "episode: 28 rews: -119.72953390883897\n",
            "training\n",
            "..........|\n",
            "episode: 29 rews: -121.51342527633891\n",
            "training\n",
            "..........|\n",
            "episode: 30 rews: -129.15219364076438\n",
            "training\n",
            "..........|\n",
            "episode: 31 rews: -126.66552040848586\n",
            "training\n",
            "..........|\n",
            "episode: 32 rews: -110.35474369370951\n",
            "training\n",
            "..........|\n",
            "episode: 33 rews: -122.66093130633458\n",
            "training\n",
            "..........|\n",
            "episode: 34 rews: -123.56671669875738\n",
            "training\n",
            "..........|\n",
            "episode: 35 rews: -64.87436358797628\n",
            "training\n",
            "..........|\n",
            "episode: 36 rews: -175.74940507828344\n",
            "training\n",
            "..........|\n",
            "episode: 37 rews: -121.76808189234627\n",
            "training\n",
            "..........|\n",
            "episode: 38 rews: -116.20182680848507\n",
            "training\n",
            "..........|\n",
            "episode: 39 rews: -121.87028949223763\n",
            "training\n",
            "..........|\n",
            "episode: 40 rews: -121.69660460476004\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 41 rews: 9937.297678426004\n",
            "training\n",
            "..........|\n",
            "episode: 42 rews: -126.25529347907646\n",
            "training\n",
            "..........|\n",
            "episode: 43 rews: -120.72785333493259\n",
            "training\n",
            "..........|\n",
            "episode: 44 rews: -113.98034101816633\n",
            "training\n",
            "..........|\n",
            "episode: 45 rews: -117.56504295788893\n",
            "training\n",
            "..........|\n",
            "episode: 46 rews: -118.51285954318824\n",
            "training\n",
            "..........|\n",
            "episode: 47 rews: -122.70587805436757\n",
            "training\n",
            "..........|\n",
            "episode: 48 rews: -120.50248653242312\n",
            "training\n",
            "..........|\n",
            "episode: 49 rews: -124.62791640321213\n",
            "training\n",
            "..........|\n",
            "episode: 50 rews: -122.7912089207492\n",
            "training\n",
            "..........|\n",
            "episode: 51 rews: -115.47694328373895\n",
            "training\n",
            "..........|\n",
            "episode: 52 rews: -116.55692355178473\n",
            "training\n",
            "..........|\n",
            "episode: 53 rews: -116.64949998055032\n",
            "training\n",
            "..........|\n",
            "episode: 54 rews: -137.84738711511562\n",
            "training\n",
            "..........|\n",
            "episode: 55 rews: -120.29226792180341\n",
            "training\n",
            "..........|\n",
            "episode: 56 rews: -118.28640365508778\n",
            "training\n",
            "..........|\n",
            "episode: 57 rews: -121.62116482253494\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 58 rews: 30158.499849052634\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 59 rews: 9975.146509958135\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 60 rews: 40232.77939092004\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 61 rews: 40246.96282796135\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 62 rews: 30171.850147199246\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "episode: 63 rews: 20077.141014936136\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "episode: 64 rews: 20068.03981677782\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 65 rews: 30163.89620349273\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 66 rews: 60430.638058776705\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 67 rews: 30170.408156690195\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 68 rews: 40268.49673963689\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 69 rews: 50363.04411662688\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 70 rews: 50367.92737922362\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 71 rews: 50390.449776300804\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 72 rews: 50385.041109613456\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 73 rews: 50390.20470604549\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 74 rews: 40276.1618643464\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 75 rews: 40258.07490684732\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 76 rews: 50365.621102834084\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 77 rews: 40228.56321564824\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 78 rews: 30173.411472840122\n",
            "training\n",
            "..........|\n",
            "episode: 79 rews: -153.155458144959\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 80 rews: 30161.495954581485\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "episode: 81 rews: 20051.899382682866\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 82 rews: 9954.524305375928\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 83 rews: 9944.075873903488\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 84 rews: 9938.709720699413\n",
            "training\n",
            "..........|\n",
            "episode: 85 rews: -150.31998707966906\n",
            "training\n",
            "..........|\n",
            "episode: 86 rews: -133.5917318355377\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 87 rews: 9949.866032555872\n",
            "training\n",
            "..........|\n",
            "episode: 88 rews: -128.9230810930699\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 89 rews: 9926.23101835843\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 90 rews: 9921.891649451192\n",
            "training\n",
            "..........|\n",
            "episode: 91 rews: -168.88283921642335\n",
            "training\n",
            "..........|\n",
            "episode: 92 rews: -159.9543882562349\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 93 rews: 9929.136228743811\n",
            "training\n",
            "..........|\n",
            "episode: 94 rews: -157.78198984157672\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "episode: 95 rews: 20038.49246964443\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 96 rews: 40231.62379914601\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "episode: 97 rews: 20017.725802610377\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 98 rews: 30119.762234285772\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 99 rews: 50353.23526956005\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 100 rews: 50352.71337906875\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 101 rews: 9995.313133247426\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 102 rews: 30154.335568385806\n",
            "training\n",
            "..........|\n",
            "episode: 103 rews: -155.7045351585349\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 104 rews: 9917.741843328417\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 105 rews: 40259.42530395445\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 106 rews: 30127.886728345784\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 107 rews: 30125.536879965697\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 108 rews: 9911.077135331523\n",
            "training\n",
            "..........|\n",
            "episode: 109 rews: -187.11915924067858\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 110 rews: 110925.23315653345\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 111 rews: 60429.56188341766\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 112 rews: 70534.9881246887\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 113 rews: 70532.13360867792\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 114 rews: 30124.48835627598\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "episode: 115 rews: 20017.975500803226\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "episode: 116 rews: 20013.759298759163\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "episode: 117 rews: 20016.98338541342\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "episode: 118 rews: 9911.900937732433\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 119 rews: 50312.838420186825\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 120 rews: 40211.03726286306\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "reached\n",
            "episode: 121 rews: 30112.63583482824\n",
            "training\n",
            "..........|\n",
            "reached\n",
            "reached\n",
            "episode: 122 rews: 20014.093039298787\n",
            "training\n",
            "...."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QI4onnTUyPAn"
      },
      "source": [
        "import gym\n",
        "\n",
        "memory=deque(maxlen=5000000)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "#env,networks ad optimisers\n",
        "#env=gym.make('Pendulum-v0')\n",
        "env=gym.make('MountainCarContinuous-v0')\n",
        "\n",
        "env=env.unwrapped\n",
        "\n",
        "s_dim = env.observation_space.shape[0]\n",
        "print(s_dim)\n",
        "a_dim = env.action_space.shape[0]\n",
        "print(a_dim)\n",
        "a_bound = env.action_space.high[0]\n",
        "print(a_bound)\n",
        "\n",
        "policy = Policy(s_dim,a_dim,100).to(device)\n",
        "value = Value(s_dim,100).to(device)\n",
        "t_value = Value(s_dim,100).to(device)\n",
        "soft_update(target=t_value, source=value, tau=1)\n",
        "q_1 = Q_net(s_dim,a_dim,100).to(device)\n",
        "q_2 = Q_net(s_dim,a_dim,100).to(device)\n",
        "qopt = optim.Adam(list(q_1.parameters()) + list(q_2.parameters()),lr=0.001,betas=(0.5, 0.999))\n",
        "popt = optim.Adam(policy.parameters(),lr=0.001,betas=(0.5, 0.999))\n",
        "vopt = optim.Adam(value.parameters(),lr=0.001,betas=(0.5, 0.999))        \n",
        "max_steps=5000\n",
        "episodes = 500\n",
        "train_iter = 1000  # 1000, batch of 8/16 works best for now\n",
        "for e in range(episodes):\n",
        "  done = False\n",
        "  s = env.reset()\n",
        "  rew = 0 \n",
        "  stp=0\n",
        "  while not done:\n",
        "    new_mu,new_sig = policy.predict(torch.tensor([s]).float().to(device))\n",
        "    new_mu,new_sig=new_mu.cpu().numpy(),new_sig.cpu().numpy()\n",
        "    #print(new_mu,new_sig,new_mu[0],new_sig[0])\n",
        "    E =np.random.multivariate_normal(np.zeros_like(new_mu[0]),np.diag(np.ones_like(new_mu[0])))#np.random.normal(mu=0,std_dev=1) diag cov of 1 is same as std dev of 1\n",
        "    Act_guassian = new_mu[0] + new_sig[0]*E # guassian action using reparametrisation\n",
        "    act  =np.tanh(Act_guassian)\n",
        "    #if e<10:\n",
        "    #  if np.random.normal(0,1)>0.5:\n",
        "    #    act=E\t#for agressive exploration in mountain car\n",
        "    #  print(E) \n",
        "    s_,r,done,_=env.step(act*a_bound)\n",
        "    #r = s_[0]*10 # for mountain car reward hacking 0 pos ,1 vel\n",
        "    if done:\n",
        "      print(\"reached\")\n",
        "      #r=10000 # for mountain car\n",
        "    if stp>max_steps:\n",
        "      done = True  \n",
        "    remember(s,act,r,s_,done)\n",
        "    s=s_\n",
        "    rew+=r\n",
        "    stp+=1\n",
        "  print(e,rew)\n",
        "  if e>20:\n",
        "    print(\"training\")\n",
        "    for i in  tqdm(range(train_iter)):\n",
        "      replay_and_train(policy,value,t_value,q_1,q_2,popt,vopt,qopt,size=8)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ti4Q4M9bP5NH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}